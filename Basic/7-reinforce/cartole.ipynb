{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(32, input_dim = 4, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(2, activation = \"softmax\"))\n",
    "model.build()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "compute_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma = 0.8):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "episodes = 2000\n",
    "scores = []\n",
    "update_every = 5\n",
    "\n",
    "gradBuffer = model.trainable_variables\n",
    "for ix,grad in enumerate(gradBuffer):\n",
    "    gradBuffer[ix] = grad * 0\n",
    "for e in range(episodes):\n",
    "  \n",
    "    s = env.reset()\n",
    "  \n",
    "    ep_memory = []\n",
    "    ep_score = 0\n",
    "    done = False \n",
    "    while not done: \n",
    "        s = s.reshape([1,4])\n",
    "        #env.render()\n",
    "        with tf.GradientTape() as tape:\n",
    "      #forward pass\n",
    "            #logits = model.predict(s)\n",
    "            \n",
    "            logits = model(s)\n",
    "            #print(logits)\n",
    "            a_dist = logits.numpy()\n",
    "      # Choose random action with p = action dist\n",
    "            #a = np.random.choice(logits[0],p=logits[0])\n",
    "            #a = np.argmax(logits == a)\n",
    "            a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "            #print('a',a)\n",
    "            a = np.argmax(a_dist == a)\n",
    "            #print(a)\n",
    "            loss = compute_loss([a], logits)\n",
    "            #print(loss)\n",
    "    # make the choosen action \n",
    "        s, r, done, _ = env.step(a)\n",
    "        ep_score +=r\n",
    "        if done: r-=10\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        #print(grads)\n",
    "        ep_memory.append([grads,r])\n",
    "    scores.append(ep_score)\n",
    "  # Discound the rewards \n",
    "    ep_memory = np.array(ep_memory)\n",
    "    ep_memory[:,1] = discount_rewards(ep_memory[:,1])\n",
    "  \n",
    "    for grads, r in ep_memory:\n",
    "        for ix,grad in enumerate(grads):\n",
    "            gradBuffer[ix] += grad * r\n",
    "  \n",
    "    if e % update_every == 0:\n",
    "        optimizer.apply_gradients(zip(gradBuffer, model.trainable_variables))\n",
    "        for ix,grad in enumerate(gradBuffer):\n",
    "            gradBuffer[ix] = grad * 0\n",
    "      \n",
    "    if e % 100 == 0:\n",
    "        print(\"Episode  {}  Score  {}\".format(e, np.mean(scores[-10:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
