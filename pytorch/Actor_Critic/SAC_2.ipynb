{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym  \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Memory:\n",
    "    '''\n",
    "    Description:\n",
    "    The Memory class allows to store and sample events\n",
    "    Attributes:  \n",
    "    capacity -- max amount of events stored\n",
    "    data -- list with events memorized\n",
    "    pointer -- position of the list in which an event will be registered\n",
    "    Methods:\n",
    "    store -- save one event in \"data\" in the position indicated by \"pointer\"\n",
    "    sample -- returns a uniformly sampled batch of stored events\n",
    "    retrieve -- returns the whole information memorized\n",
    "    forget -- elliminates all data stored\n",
    "    '''\n",
    "\n",
    "    def __init__(self, capacity = 50000):\n",
    "        '''\n",
    "        Description:\n",
    "        Initializes an empty data list and a pointer located at 0. \n",
    "        Also determines the capacity of the data list.\n",
    "        Arguments:\n",
    "        capacity -- positive int number\n",
    "        '''\n",
    "        self.capacity = capacity\n",
    "        self.data = []        \n",
    "        self.pointer = 0\n",
    "\n",
    "    \n",
    "    def store(self, event):\n",
    "        '''\n",
    "        Description:\n",
    "        Stores the input event in the location designated by the pointer.\n",
    "        The pointer is increased by one modulo the capacity. \n",
    "        Arguments:\n",
    "        event -- tuple to be stored\n",
    "        '''\n",
    "        if len(self.data) < self.capacity:\n",
    "            self.data.append(None)\n",
    "        self.data[self.pointer] = event\n",
    "        self.pointer = (self.pointer + 1) % self.capacity\n",
    "\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        Description:\n",
    "        Samples a specified number of events \n",
    "        Arguments:\n",
    "        batch_size -- int number that determines the amount of events to be sampled\n",
    "        Outputs:\n",
    "        random list with stored events\n",
    "        '''\n",
    "        return random.sample(self.data, batch_size) \n",
    "\n",
    "    def retrieve(self):\n",
    "        '''\n",
    "        Description:\n",
    "        Returns the whole stored data  \n",
    "        Outputs:\n",
    "        data\n",
    "        '''\n",
    "        return(self.data)\n",
    "    \n",
    "    def forget(self):\n",
    "        '''\n",
    "        Description:\n",
    "        Restarts the stored data and the pointer\n",
    "        '''\n",
    "        self.data = []\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class v_valueNet(nn.Module):\n",
    "    '''\n",
    "    Description:\n",
    "    The valueNet is a standard fully connected NN with ReLU activation functions\n",
    "    and 3 linear layers that approximates the value function\n",
    "    Attributes:  \n",
    "    l1,l2,l3 -- linear layers\n",
    "    \n",
    "    Methods:\n",
    "    forward -- calculates otput of network\n",
    "    '''\n",
    "    def __init__(self, input_dim):\n",
    "        '''\n",
    "        Description:\n",
    "        Creates the three linear layers of the net\n",
    "        Arguments:  \n",
    "        input_dim -- int that specifies the size of input        \n",
    "        '''\n",
    "        super().__init__()        \n",
    "        self.l1 = nn.Linear(input_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 1)  \n",
    "\n",
    "        self.l3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        self.l3.bias.data.uniform_(-3e-3, 3e-3)       \n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = 3e-4)    \n",
    "    \n",
    "    def forward(self, s):\n",
    "        '''\n",
    "        Descrption:\n",
    "        Calculates output for the given input\n",
    "        Arguments:  \n",
    "        x -- input to be propagated through the net\n",
    "        Outputs:\n",
    "        x -- number that represents the approximate value of the input        \n",
    "        '''\n",
    "        x = F.relu(self.l1(s))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class q_valueNet(nn.Module):\n",
    "    '''\n",
    "    Description:\n",
    "    The valueNet is a standard fully connected NN with ReLU activation functions\n",
    "    and 3 linear layers that approximates the value function\n",
    "    Attributes:  \n",
    "    l1,l2,l3 -- linear layers\n",
    "    \n",
    "    Methods:\n",
    "    forward -- calculates otput of network\n",
    "    '''\n",
    "    def __init__(self, s_dim, a_dim):\n",
    "        '''\n",
    "        Descrption:\n",
    "        Creates the three linear layers of the net\n",
    "        Arguments:  \n",
    "        input_dim -- int that specifies the size of input        \n",
    "        '''\n",
    "        super().__init__()        \n",
    "        self.l1 = nn.Linear(s_dim+a_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 1)  \n",
    "\n",
    "        self.l3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        self.l3.bias.data.uniform_(-3e-3, 3e-3) \n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = 3e-4)    \n",
    "    \n",
    "    def forward(self, s,a):\n",
    "        '''\n",
    "        Descrption:\n",
    "        Calculates output for the given input\n",
    "        Arguments:  \n",
    "        x -- input to be propagated through the net\n",
    "        Outputs:\n",
    "        x -- number that represents the approximate value of the input        \n",
    "        '''\n",
    "        x = torch.cat([s, a], 1)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class policyNet(nn.Module):\n",
    "    '''\n",
    "    Description:\n",
    "    The policyNet is a standard fully connected NN with ReLU and sigmoid activation \n",
    "    functions and 3 linear layers. This net determines the action for a given state. \n",
    "    Attributes:  \n",
    "    l1,l2,l3 -- linear layers\n",
    "    \n",
    "    Methods:\n",
    "    forward -- calculates otput of network\n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim, min_log_stdev=-30, max_log_stdev=30):\n",
    "        '''\n",
    "        Descrption:\n",
    "        Creates the three linear layers of the net\n",
    "        Arguments:  \n",
    "        input_dim -- int that specifies the size of input        \n",
    "        '''\n",
    "        super().__init__()     \n",
    "        self.min_log_stdev = min_log_stdev\n",
    "        self.max_log_stdev = max_log_stdev\n",
    "\n",
    "        self.l1 = nn.Linear(input_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l31 = nn.Linear(256, output_dim)\n",
    "        self.l32 = nn.Linear(256, output_dim)\n",
    "\n",
    "        self.l31.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        self.l32.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        self.l31.bias.data.uniform_(-3e-3, 3e-3)\n",
    "        self.l32.bias.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = 3e-4)    \n",
    "    \n",
    "    def forward(self, s):\n",
    "        x = F.relu(self.l1(s))\n",
    "        x = F.relu(self.l2(x))\n",
    "        m = self.l31(x)\n",
    "        log_stdev = self.l32(x)\n",
    "        log_stdev = torch.clamp(log_stdev, self.min_log_stdev, self.max_log_stdev)\n",
    "        return m, log_stdev\n",
    "    \n",
    "    def sample_action(self, s):\n",
    "        '''\n",
    "        Description:\n",
    "        Calculates output for the given input\n",
    "        Arguments:  \n",
    "        x -- input to be propagated through the net\n",
    "        Outputs:\n",
    "        a --         \n",
    "        '''\n",
    "        m, log_stdev = self(s)\n",
    "        u = m + log_stdev.exp()*torch.randn_like(m)\n",
    "        a = torch.tanh(u).cpu()        \n",
    "        return a\n",
    "\n",
    "    def sample_action_and_llhood(self, s):\n",
    "        m, log_stdev = self(s)\n",
    "        stdev = log_stdev.exp()\n",
    "        u = m + stdev*torch.randn_like(m)\n",
    "        a = torch.tanh(u)\n",
    "        llhood = (Normal(m, stdev).log_prob(u) - torch.log(torch.clamp(1 - a.pow(2), 1e-6, 1.0))).sum(dim=1, keepdim=True)\n",
    "        return a, llhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "def updateNet(target, source, tau):    \n",
    "    for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - tau) + source_param.data * tau\n",
    "        )\n",
    "\n",
    "def normalize_angle(x):\n",
    "    return (((x+np.pi) % (2*np.pi)) - np.pi)\n",
    "\n",
    "def scale_action(a, min, max):\n",
    "    return (0.5*(a+1.0)*(max-min) + min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''\n",
    "    Attributes:    \n",
    "    Methods:\n",
    "    fit --  \n",
    "    s_score --  \n",
    "    sample_a -- \n",
    "    sample_m_state -- \n",
    "    act --    \n",
    "    learn --\n",
    "    '''\n",
    "\n",
    "    def __init__(self, s_dim=2, a_dim=1, memory_capacity=50000, batch_size=64, discount_factor=0.99, temperature=1.0,\n",
    "        soft_lr=5e-3, reward_scale=1.0):\n",
    "        '''\n",
    "        Initializes the agent.\n",
    "        Arguments:\n",
    "        Returns:\n",
    "        none\n",
    "        '''\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.sa_dim = self.s_dim + self.a_dim          \n",
    "        self.batch_size = batch_size \n",
    "        self.gamma = discount_factor\n",
    "        self.soft_lr = soft_lr        \n",
    "        self.alpha = temperature\n",
    "        self.reward_scale = reward_scale\n",
    "         \n",
    "        self.memory = Memory(memory_capacity)\n",
    "        self.actor = policyNet(s_dim, a_dim).to(device)        \n",
    "        self.critic1 = q_valueNet(self.s_dim, self.a_dim).to(device)\n",
    "        self.critic2 = q_valueNet(self.s_dim, self.a_dim).to(device)\n",
    "        self.baseline = v_valueNet(s_dim).to(device) \n",
    "        self.baseline_target = v_valueNet(s_dim).to(device) \n",
    "    \n",
    "        updateNet(self.baseline_target, self.baseline, 1.0) \n",
    "\n",
    "    def act(self, state, explore=True):\n",
    "        with torch.no_grad():\n",
    "            action = self.actor.sample_action(state)            \n",
    "            return action\n",
    "    \n",
    "    def memorize(self, event):\n",
    "        self.memory.store(event[np.newaxis,:])\n",
    "    \n",
    "    def learn(self):        \n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        batch = np.concatenate(batch, axis=0)\n",
    "       \n",
    "        s_batch = torch.FloatTensor(batch[:,:self.s_dim]).to(device)\n",
    "        a_batch = torch.FloatTensor(batch[:,self.s_dim:self.sa_dim]).to(device)\n",
    "        r_batch = torch.FloatTensor(batch[:,self.sa_dim]).unsqueeze(1).to(device)\n",
    "        ns_batch = torch.FloatTensor(batch[:,self.sa_dim+1:self.sa_dim+1+self.s_dim]).to(device)\n",
    "\n",
    "        # Optimize q networks\n",
    "        q1 = self.critic1(s_batch, a_batch)\n",
    "        q2 = self.critic2(s_batch, a_batch)     \n",
    "        next_v = self.baseline_target(ns_batch)\n",
    "        q_approx = self.reward_scale * r_batch + self.gamma * next_v\n",
    "\n",
    "        q1_loss = self.critic1.loss_func(q1, q_approx.detach())\n",
    "        self.critic1.optimizer.zero_grad()\n",
    "        q1_loss.backward()\n",
    "        self.critic1.optimizer.step()\n",
    "        \n",
    "        q2_loss = self.critic2.loss_func(q2, q_approx.detach())\n",
    "        self.critic2.optimizer.zero_grad()\n",
    "        q2_loss.backward()\n",
    "        self.critic2.optimizer.step()\n",
    "\n",
    "        # Optimize v network\n",
    "        v = self.baseline(s_batch)\n",
    "        a_batch_off, llhood = self.actor.sample_action_and_llhood(s_batch)                \n",
    "        q1_off = self.critic1(s_batch, a_batch_off)\n",
    "        q2_off = self.critic2(s_batch, a_batch_off)\n",
    "        q_off = torch.min(q1_off, q2_off)          \n",
    "        v_approx = q_off - self.alpha*llhood\n",
    "\n",
    "        v_loss = self.baseline.loss_func(v, v_approx.detach())\n",
    "        self.baseline.optimizer.zero_grad()\n",
    "        v_loss.backward()\n",
    "        self.baseline.optimizer.step()\n",
    "        \n",
    "        # Optimize policy network\n",
    "        pi_loss = (llhood - q_off).mean()\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        pi_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        # Update v target network\n",
    "        updateNet(self.baseline_target, self.baseline, self.soft_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class System:\n",
    "    def __init__(self, memory_capacity = 200000, env_steps=1, grad_steps=1, init_steps=256, reward_scale = 25,\n",
    "        temperature=1.0, soft_lr=5e-3, batch_size=256, hard_start = False, original_state=True, system='Hopper-v2'): # 'Pendulum-v0', 'Hopper-v2', 'HalfCheetah-v2', 'Swimmer-v2'\n",
    "        self.env = gym.make(system).unwrapped\n",
    "        self.env.reset()\n",
    "        self.type = system\n",
    "       \n",
    "        self.s_dim = self.env.observation_space.shape[0]               \n",
    "        if not original_state and system == 'Pendulum-v0':\n",
    "            self.s_dim -= 1\n",
    "        self.a_dim = self.env.action_space.shape[0] \n",
    "        self.sa_dim = self.s_dim + self.a_dim\n",
    "        self.e_dim = self.s_dim*2 + self.a_dim + 1\n",
    "\n",
    "        self.env_steps = env_steps\n",
    "        self.grad_steps = grad_steps\n",
    "        self.init_steps = init_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.hard_start = hard_start\n",
    "        self.original_state = original_state\n",
    "\n",
    "        self.min_action = self.env.action_space.low[0]\n",
    "        self.max_action = self.env.action_space.high[0]\n",
    "        self.temperature = temperature\n",
    "        self.reward_scale = reward_scale\n",
    "\n",
    "        self.agent = Agent(s_dim=self.s_dim, a_dim=self.a_dim, memory_capacity=memory_capacity, batch_size=batch_size, reward_scale=reward_scale, \n",
    "            temperature=temperature, soft_lr=soft_lr)    \n",
    "    \n",
    "    def initialization(self):\n",
    "        event = np.empty(self.e_dim)\n",
    "        if self.hard_start:\n",
    "            initial_state = np.array([-np.pi,0.0])\n",
    "            self.env.state = initial_state\n",
    "        else:\n",
    "            self.env.reset()\n",
    "        if self.original_state:\n",
    "            state = self.env._get_obs()\n",
    "        else:\n",
    "            state = self.env.state \n",
    "        \n",
    "        for init_step in range(0, self.init_steps):            \n",
    "            action = np.random.rand(self.a_dim)*2 - 1\n",
    "            reward = self.env.step(scale_action(action, self.min_action, self.max_action))[1]\n",
    "            if self.original_state:\n",
    "                next_state = self.env._get_obs()\n",
    "            else:\n",
    "                next_state = self.env.state\n",
    "                next_state[0] = normalize_angle(next_state[0])\n",
    "\n",
    "            event[:self.s_dim] = state\n",
    "            event[self.s_dim:self.sa_dim] = action\n",
    "            event[self.sa_dim] = reward\n",
    "            event[self.sa_dim+1:self.e_dim] = next_state\n",
    "\n",
    "            self.agent.memorize(event)\n",
    "            state = np.copy(next_state)\n",
    "    \n",
    "    def interaction(self, learn=True, remember=True):   \n",
    "        event = np.empty(self.e_dim)\n",
    "        if self.original_state:\n",
    "            state = self.env._get_obs()\n",
    "        else:            \n",
    "            state = self.env.state \n",
    "            state[0] = normalize_angle(state[0])\n",
    "\n",
    "        for env_step in range(0, self.env_steps):\n",
    "              \n",
    "            cuda_state = torch.FloatTensor(state).unsqueeze(0).to(device)         \n",
    "            action = self.agent.act(cuda_state, explore=learn)\n",
    "            \n",
    "            reward = self.env.step(scale_action(action, self.min_action, self.max_action))[1]\n",
    "            done= self.env.step(scale_action(action, self.min_action, self.max_action))[2]\n",
    "            if self.original_state:\n",
    "                next_state = self.env._get_obs()\n",
    "            else:\n",
    "                next_state = self.env.state\n",
    "                next_state[0] = normalize_angle(next_state[0])\n",
    "\n",
    "            event[:self.s_dim] = state\n",
    "            event[self.s_dim:self.sa_dim] = action\n",
    "            event[self.sa_dim] = reward\n",
    "            event[self.sa_dim+1:self.e_dim] = next_state\n",
    "\n",
    "            if remember:\n",
    "                self.agent.memorize(event)   \n",
    "            \n",
    "            state = np.copy(next_state)\n",
    "        \n",
    "        if learn:\n",
    "            for grad_step in range(0, self.grad_steps):\n",
    "                self.agent.learn()\n",
    "        \n",
    "        return(event),done\n",
    "    \n",
    "    def train_agent(self, tr_epsds, epsd_steps, initialization=True):\n",
    "        if initialization:\n",
    "            self.initialization()\n",
    "        \n",
    "        min_reward = 1e10\n",
    "        max_reward = -1e10\n",
    "        mean_reward = 0.0   \n",
    "        min_mean_reward = 1e10\n",
    "        max_mean_reward = -1e10   \n",
    "\n",
    "        mean_rewards = []           \n",
    "        \n",
    "        for epsd in range(0, tr_epsds):\n",
    "            epsd_min_reward = 1e10\n",
    "            epsd_max_reward = -1e10                \n",
    "            epsd_mean_reward = 0.0\n",
    "            ep_reward=0.0\n",
    "            print('???')\n",
    "            if self.hard_start:\n",
    "                initial_state = np.array([-np.pi,0.0])\n",
    "                self.env.state = initial_state\n",
    "            else:\n",
    "                self.env.reset()\n",
    "            \n",
    "            for epsd_step in range(0, epsd_steps):\n",
    "                    if len(self.agent.memory.data) < self.batch_size:\n",
    "                        event,done = self.interaction(learn=False)\n",
    "                    else:\n",
    "                        event,done = self.interaction()\n",
    "                    \n",
    "                    r = event[self.sa_dim]\n",
    "                    #print(done)\n",
    "                    min_reward = np.min([r, min_reward])\n",
    "                    max_reward = np.max([r, max_reward])\n",
    "                    epsd_min_reward = np.min([r, epsd_min_reward])                        \n",
    "                    epsd_max_reward = np.max([r, epsd_max_reward])                        \n",
    "                    epsd_mean_reward += r \n",
    "                    ep_reward+=r\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "            epsd_mean_reward /=epsd_steps            \n",
    "            mean_rewards.append(epsd_mean_reward)\n",
    "\n",
    "            min_mean_reward = np.min([epsd_mean_reward, min_mean_reward])\n",
    "            max_mean_reward = np.max([epsd_mean_reward, max_mean_reward])            \n",
    "            mean_reward += (epsd_mean_reward - mean_reward)/(epsd+1)\n",
    "            \n",
    "            print(epsd,ep_reward)\n",
    "            \n",
    "            plot(epsd,mean_rewards)\n",
    "        return mean_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "???\n"
     ]
    }
   ],
   "source": [
    "n_test = 6\n",
    "system_type = 'Pendulum-v0'\n",
    "system = System(system=system_type, reward_scale=100)\n",
    "tr_epsds = 2000\n",
    "epsd_steps = 500\n",
    "mean_rewards = []\n",
    "for i in range(1,6):\n",
    "    if i == 1:\n",
    "        mean_rewards.append(system.train_agent(tr_epsds, epsd_steps))        \n",
    "    else:\n",
    "        mean_rewards.append(system.train_agent(tr_epsds, epsd_steps, initialization=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
