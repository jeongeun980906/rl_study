{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as torch_utils\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Net(nn.Module):\n",
    "    def __init__(self,n_in,n_out,action_bound):\n",
    "        super(Actor_Net,self).__init__()\n",
    "        self.fc1=nn.Linear(n_in,64)\n",
    "        self.fc2=nn.Linear(64,32)\n",
    "        self.fc3=nn.Linear(32,16)\n",
    "        \n",
    "        self.fc_mu=nn.Linear(16,n_out)\n",
    "        self.fc_std=nn.Linear(16,n_out)\n",
    "        self.action_bound=action_bound\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h1=F.relu(self.fc1(x))\n",
    "        h2=F.relu(self.fc2(h1))\n",
    "        h3=F.relu(self.fc3(h2))\n",
    "        \n",
    "        mu=F.tanh(self.fc_mu(h3))\n",
    "        std=F.softplus(self.fc_std(h3))\n",
    "        \n",
    "        return mu*self.action_bound,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_Net(nn.Module):\n",
    "    def __init__(self,n_in,n_out):\n",
    "        super(Critic_Net,self).__init__()\n",
    "        self.fc1=nn.Linear(n_in,64)\n",
    "        self.fc2=nn.Linear(64,32)\n",
    "        self.fc3=nn.Linear(32,16)\n",
    "        self.fc_value=nn.Linear(16,n_out)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h1=F.relu(self.fc1(x))\n",
    "        h2=F.relu(self.fc2(h1))\n",
    "        h3=F.relu(self.fc3(h2))\n",
    "        out=self.fc_value(h3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker_Actor(nn.Module):\n",
    "    def __init__(self,state_dim,action_dim,action_bound):\n",
    "        super(Worker_Actor,self).__init__()\n",
    "        \n",
    "        self.state_dim=state_dim\n",
    "        self.std_bound=[1e-2,1.]\n",
    "        self.network=Actor_Net(state_dim,action_dim,action_bound)\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            mu_a,std_a=self.network(state.view(1,self.state_dim))\n",
    "            mu_a,std_a=mu_a[0],std_a[0]\n",
    "            sta_a=torch.clamp(std_a,self.std_bound[0],self.std_bound[1])\n",
    "            action=torch.normal(mu_a,std_a)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Global_Actor(nn.Module):\n",
    "    def __init__(self,state_dim,action_dim,action_bound,lr_rate,entropy_beta):\n",
    "        super(Global_Actor,self).__init__()\n",
    "        \n",
    "        self.state_dim=state_dim\n",
    "        self.std_bound=[1e-2,1.]\n",
    "        self.entropy_beta = entropy_beta\n",
    "        self.network=Actor_Net(state_dim,action_dim,action_bound)\n",
    "        self.optimizer=optim.Adam(self.network.parameters(),lr=lr_rate)\n",
    "        \n",
    "    def log_pdf(self,mu,std,action):\n",
    "        std=torch.clamp(std,min=self.std_bound[0],max=self.std_bound[1])\n",
    "        var=std**2\n",
    "        log_policy_pdf=-0.5*(action-mu)**2/var-0.5*torch.log(var*2*np.pi)\n",
    "        return torch.sum(log_policy_pdf,dim=1,keepdim=True)\n",
    "    \n",
    "    def update(self,states,actions,advantages):\n",
    "        self.network.train()\n",
    "        mu_a,std_a=self.actor_network(states)\n",
    "        log_policy_pdf=self.log_pdf(mu_a,std_a,actions)\n",
    "        loss=torch.sum(-log_policy_pdf*advantages.detach())\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch_utils.clip_grad_norm_(self.network.parameters(), 40.0)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker_Critic(nn.Module):\n",
    "    def __init__(self,state_dim):\n",
    "        super(Worker_Critic,self).__init__()\n",
    "        \n",
    "        self.network=Critic_Net(state_dim,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Global_Critic(nn.Module):\n",
    "    def __init__(self,state_dim,lr_rate):\n",
    "        super(Global_Critic,self).__init__()\n",
    "        \n",
    "        self.network=Critic_Net(state_dim,1)\n",
    "        self.optimizer=optim.Adam(self.network.parameters(),lr=lr_rate)\n",
    "        \n",
    "    def get_value(self,states):\n",
    "        self.network.eval()\n",
    "        value=self.network(states)\n",
    "        return value\n",
    "    \n",
    "    def update(self,states,targets):\n",
    "        self.network.train()\n",
    "        values=self.network(states)\n",
    "        loss=F.mse_loss(values,targets.detach())\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch_utils.clip_grad_norm_(self.network.parameters(), 40.0)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_episode_reward = []\n",
    "class A3Cagent():\n",
    "    def __init__(self,env_name,n_workers):\n",
    "        self.env_name=env_name\n",
    "        self.n_workers=n_workers\n",
    "        self.actor_learning_rate=0.0001\n",
    "        self.critic_learning_rate=0.001\n",
    "        self.entropy_beta=0.01\n",
    "        \n",
    "        env=gym.make(env_name)\n",
    "        state_dim=env.observation_space.shape[0]\n",
    "        action_dim=env.action_space.shape[0]\n",
    "        action_bound=env.action_space.high[0]\n",
    "        \n",
    "        self.global_actor=Global_Actor(state_dim,action_dim,action_bound,self.actor_learning_rate,self.entropy_beta)\n",
    "        self.global_critic=Global_Critic(state_dim,self.critic_learning_rate)\n",
    "        \n",
    "        self.global_actor.share_memory()\n",
    "        self.global_critic.share_memory()\n",
    "        \n",
    "        self.global_episode_count=mp.Value('i',0)\n",
    "        self.global_step= mp.Value('i', 0)\n",
    "    \n",
    "    def train(self,max_episode_num):\n",
    "        workers=[]\n",
    "        for i in range(self.n_workers):\n",
    "            worker_name='worker%i' %i\n",
    "            workers.append(Worker(worker_name,self.env_name,self.global_actor,self.global_critic,max_episode_num,self.global_episode_count,self.global_step))\n",
    "        for worker in workers:\n",
    "            worker.start()\n",
    "        for worker in workers:\n",
    "            worker.join()\n",
    "    def plot_result(self):\n",
    "        plt.plot(self.global_episode_count)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(mp.Process):\n",
    "    def __init__(self,worker_name,env_name,global_actor,global_critic,max_episode_num,global_episode_count,global_step):\n",
    "        super(Worker,self).__init__()\n",
    "        self.worker_name=worker_name\n",
    "        self.env=gym.make(env_name)\n",
    "        self.state_dim=self.env.observation_space.shape[0]\n",
    "        self.action_dim=self.env.action_space.shape[0]\n",
    "        self.action_bound=self.env.action_space.high[0]\n",
    "        \n",
    "        self.gamma=0.95\n",
    "        self.t_max=4\n",
    "        self.max_episode=max_episode_num\n",
    "        \n",
    "        self.global_actor=global_actor\n",
    "        self.global_critic=global_critic\n",
    "        \n",
    "        self.worker_actor=Worker_Actor(self.state_dim,self.action_dim,self.action_bound)\n",
    "        self.worker_critic=Worker_Critic(self.state_dim)\n",
    "        \n",
    "        #initial transfer global network parameters\n",
    "        \n",
    "        self.worker_actor.network.load_state_dict(self.global_actor.network.state_dict())\n",
    "        self.worker_critic.network.load_state_dict(self.global_critic.network.state_dict())\n",
    "        \n",
    "        self.global_episode_count= global_episode_count\n",
    "        self.global_step= global_step\n",
    "    def n_step_td_target(self,rewards,next_v_value,done):\n",
    "        td_targets=torch.zeros(rewards.size())\n",
    "        cumulative=0\n",
    "        if not done:\n",
    "            cumulative=next_v_value\n",
    "        for k in reversed(range(0,len(rewards))):\n",
    "            cumulative=self.gamma*cumulative+rewards[k]\n",
    "            td_targets[k]=cumulative\n",
    "        return td_targets\n",
    "    \n",
    "    def unpack_batch(self,batch):\n",
    "        unpack=[]\n",
    "        for idx in range(len(batch)):\n",
    "            unpack.append(batch[idx])\n",
    "        unpack=torch.cat(unpack,axis=0)\n",
    "        return unpack\n",
    "    \n",
    "    def run(self):\n",
    "        global global_episode_reward\n",
    "        print(self.worker_name, \"starts ---\")\n",
    "        while self.global_episode_count.value<=int(self.max_episode):\n",
    "            batch_state,batch_action,batch_reward=[],[],[]\n",
    "            time, episode_reward, done = 0, 0, False\n",
    "            state=self.env.reset()\n",
    "            state=torch.from_numpy(state).type(torch.FloatTensor)\n",
    "            \n",
    "            while not done:\n",
    "                action=self.worker_actor.get_action(state)\n",
    "                action=np.array([action.item()])\n",
    "                action=np.clip(action,-self.action_bound,self.action_bound)\n",
    "                \n",
    "                next_state,reward,done,_=self.env.step(action)\n",
    "                \n",
    "                next_state=torch.from_numpy(next_state).type(torch.FloatTensor)\n",
    "                action=torch.from_numpy(action).type(torch.FloatTensor)\n",
    "                reward=torch.FloatTensor([reward])\n",
    "                \n",
    "                state= state.view(1, self.state_dim)\n",
    "                next_state= next_state.view(1, self.state_dim)\n",
    "                action= action.view(1, self.action_dim)\n",
    "                reward= reward.view(1, 1)\n",
    "                \n",
    "                train_reward=(reward+8)/8\n",
    "                \n",
    "                batch_state.append(state)\n",
    "                batch_action.append(action)\n",
    "                batch_reward.append(train_reward)\n",
    "                \n",
    "                state=next_state[0]\n",
    "                episode_reward+=reward[0]\n",
    "                time+=1\n",
    "                if len(batch_state)==self.t_max or done:\n",
    "                    states=self.unpack_batch(batch_state)\n",
    "                    actions=self.unpack_batch(batch_action)\n",
    "                    rewards=self.unpack_batch(batch_rewad)\n",
    "                \n",
    "                    batch_state, batch_action, batch_reward = [],[],[]\n",
    "                    \n",
    "                    v_values=self.global_critic.get_value(states)\n",
    "                    next_v_value=self.global_critic.get_value(next_state)\n",
    "                    n_step_td_targets=self.n_step_td_target(rewards,next_v_value,done)\n",
    "                    advantages=n_step_td_targets-v_values\n",
    "                    \n",
    "                    self.global_critic.update(states,n_step_td_targets)\n",
    "                    self.global_actor.update(states,actions,advantages)\n",
    "                    \n",
    "                    self.worker_actor.network.load_state_dict(self.global_actor.network.state_dict())\n",
    "                    self.worker_critic.network.load_state_dict(self.global_critic.network.state_dict())\n",
    "                    \n",
    "                    self.global_step.value+=1\n",
    "            if done:\n",
    "                self.global_episode_count.value+=1\n",
    "                global_episode_reward.append(episode_reward.item())\n",
    "                print('Worker:', self.worker_name,\n",
    "                            ', Episode:',self.global_episode_count,\n",
    "                            ', Step:', time, ', Reward:', np.mean(global_episode_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "aa\n",
      "aa\n",
      "aa\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-b72093ebdda5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mA3Cagent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_workers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_episode_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-d431fb1916bd>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, max_episode_num)\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWorker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworker_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_actor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_critic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_episode_num\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_episode_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mworker\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mworker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mworker\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mworker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "max_episode_num=1000\n",
    "env_name='Pendulum-v0'\n",
    "n_workers=4\n",
    "\n",
    "agent=A3Cagent(env_name,n_workers)\n",
    "agent.train(max_episode_num)\n",
    "agent.plot_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
