{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers, losses\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\박정은\\gym\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "num_action = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorModel(Model):\n",
    "    def __init__(self):\n",
    "        super(ActorModel, self).__init__()\n",
    "        self.layer_a1 = Dense(64, activation='relu')\n",
    "        self.layer_a2 = Dense(64, activation='relu')\n",
    "        self.logits = Dense(num_action, activation='softmax')\n",
    "\n",
    "    def call(self, state):\n",
    "        layer_a1 = self.layer_a1(state)\n",
    "        layer_a2 = self.layer_a2(layer_a1)\n",
    "        logits = self.logits(layer_a2)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticModel(Model):\n",
    "    def __init__(self):\n",
    "        super(CriticModel, self).__init__()\n",
    "        self.layer_c1 = Dense(64, activation='relu')\n",
    "        self.layer_c2 = Dense(64, activation='relu')\n",
    "        self.value = Dense(1)\n",
    "\n",
    "    def call(self, state):\n",
    "        layer_c1 = self.layer_c1(state)\n",
    "        layer_c2 = self.layer_c2(layer_c1)\n",
    "        value = self.value(layer_c2)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTrain:\n",
    "    def __init__(self):\n",
    "        # hyper parameters\n",
    "        self.lr =0.001\n",
    "        self.lr2 = 0.001\n",
    "        self.df = 0.99\n",
    "        self.en = 0.001\n",
    "\n",
    "        self.actor_model = ActorModel()\n",
    "        self.actor_opt = optimizers.Adam(lr=self.lr, )\n",
    "\n",
    "        self.critic_model = CriticModel()\n",
    "        self.critic_opt = optimizers.Adam(lr=self.lr2, )\n",
    "\n",
    "        # tensorboard\n",
    "        self.log_dir = 'logs/'\n",
    "        self.train_summary_writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        self.reward_board = tf.keras.metrics.Mean('reward_board', dtype=tf.float32)\n",
    "        #self.train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "        #self.train_loss_c = tf.keras.metrics.Mean('train_loss_c', dtype=tf.float32)\n",
    "\n",
    "    def actor_loss(self, states, actions, advantages):\n",
    "        policy = self.actor_model(tf.convert_to_tensor(np.vstack(states), dtype=tf.float32))\n",
    "\n",
    "        # SparseCategoricalCrossentropy = ce loss with not one-hot encoded output\n",
    "        # from_logits = True  =>  cross_entropy with soft_max\n",
    "        entropy = losses.categorical_crossentropy(policy, policy, from_logits=False)\n",
    "        ce_loss = losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "        # policy_loss = ce_loss(actions, policy, sample_weight=np.array(advantages))  # same way\n",
    "        log_pi = ce_loss(actions, policy)\n",
    "        policy_loss = log_pi * np.array(advantages)\n",
    "        policy_loss = tf.reduce_mean(policy_loss)\n",
    "\n",
    "        return policy_loss - self.en * entropy\n",
    "\n",
    "    def critic_loss(self, states, rewards, dones):\n",
    "        last_state = states[-1]\n",
    "        if dones[-1] == True :\n",
    "            reward_sum = 0\n",
    "        else :\n",
    "            reward_sum = self.critic_model(tf.convert_to_tensor(last_state[None, :], dtype=tf.float32))\n",
    "        discounted_rewards = []\n",
    "        for reward in rewards[::-1]:\n",
    "            reward_sum = reward + self.df * reward_sum\n",
    "            discounted_rewards.append(reward_sum)\n",
    "        discounted_rewards.reverse()\n",
    "        discounted_rewards = tf.convert_to_tensor(np.array(discounted_rewards)[:, None], dtype=tf.float32)\n",
    "        values = self.critic_model(tf.convert_to_tensor(np.vstack(states), dtype=tf.float32))\n",
    "        error = tf.square(values - discounted_rewards)*0.5\n",
    "        error = tf.reduce_mean(error)\n",
    "        return error\n",
    "\n",
    "    def train(self, states, actions, rewards, next_states, dones):\n",
    "\n",
    "        critic_variable = self.critic_model.trainable_variables\n",
    "        with tf.GradientTape() as tape_critic:\n",
    "            tape_critic.watch(critic_variable)\n",
    "            critic_loss = self.critic_loss(states, rewards, dones)\n",
    "\n",
    "        # gradient descent will be applied automatically\n",
    "        critic_grads = tape_critic.gradient(critic_loss, critic_variable)\n",
    "        self.critic_opt.apply_gradients(zip(critic_grads, critic_variable))\n",
    "\n",
    "\n",
    "        advantages = self.compute_advantages(states, rewards, dones)\n",
    "        actor_variable = self.actor_model.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(actor_variable)\n",
    "            actor_loss = self.actor_loss(states, actions, advantages)\n",
    "\n",
    "        actor_grads = tape.gradient(actor_loss, actor_variable)\n",
    "        self.actor_opt.apply_gradients(zip(actor_grads, actor_variable))\n",
    "\n",
    "        # self.train_loss(tf.reduce_mean(actor_loss))\n",
    "        # self.train_loss_c(tf.reduce_mean(critic_loss))\n",
    "        self.train_loss = tf.reduce_mean(actor_loss)\n",
    "        self.train_loss_c = tf.reduce_mean(critic_loss)\n",
    "\n",
    "    def compute_advantages(self, states, rewards, dones):\n",
    "        last_state = states[-1]\n",
    "        if dones[-1] == True:\n",
    "            reward_sum = 0\n",
    "        else:\n",
    "            reward_sum = self.critic_model(tf.convert_to_tensor(last_state[None, :], dtype=tf.float32))\n",
    "        discounted_rewards = []\n",
    "        for reward in rewards[::-1]:\n",
    "            reward_sum = reward + self.df * reward_sum\n",
    "            discounted_rewards.append(reward_sum)\n",
    "        discounted_rewards.reverse()\n",
    "        values = self.critic_model(tf.convert_to_tensor(np.vstack(states), dtype=tf.float32))\n",
    "        advantages = discounted_rewards - values\n",
    "        return advantages\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        t_end = 500\n",
    "        epi = 100000\n",
    "        train_size = 20\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "\n",
    "        state = env.reset()\n",
    "        for e in range(epi):\n",
    "            total_reward = 0\n",
    "            env.render()\n",
    "            for t in range(t_end):\n",
    "                policy = self.actor_model(tf.convert_to_tensor(state[None, :], dtype=tf.float32))\n",
    "                action = tf.squeeze(tf.random.categorical(policy, 1), axis=-1)\n",
    "                action = np.array(action)[0]\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                #env.render()\n",
    "                if t == t_end :\n",
    "                    done = True\n",
    "                    reward += 10\n",
    "                if t < t_end and done :\n",
    "                    reward = -1\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(done)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "\n",
    "                if len(states) == train_size or done :\n",
    "                    self.train(states, actions, rewards, next_states, dones)\n",
    "                    states = []\n",
    "                    actions = []\n",
    "                    rewards = []\n",
    "                    next_states = []\n",
    "                    dones = []\n",
    "\n",
    "                if done:\n",
    "                    self.reward_board(total_reward)\n",
    "                    print(\"e : \", e, \" reward : \", total_reward, \" step : \", t)\n",
    "                    env.reset()\n",
    "                    with self.train_summary_writer.as_default():\n",
    "                        # tf.summary.scalar('reward', self.reward_board.result(), step=e)\n",
    "                        tf.summary.scalar('actor_loss', self.train_loss, step=e)\n",
    "                        tf.summary.scalar('critic_loss', self.train_loss_c, step=e)\n",
    "                        tf.summary.scalar('reward', total_reward, step=e)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e :  0  reward :  16.0  step :  17\n",
      "e :  1  reward :  58.0  step :  59\n",
      "e :  2  reward :  16.0  step :  17\n",
      "e :  3  reward :  20.0  step :  21\n",
      "e :  4  reward :  48.0  step :  49\n",
      "e :  5  reward :  20.0  step :  21\n",
      "e :  6  reward :  11.0  step :  12\n",
      "e :  7  reward :  30.0  step :  31\n",
      "e :  8  reward :  8.0  step :  9\n",
      "e :  9  reward :  16.0  step :  17\n",
      "e :  10  reward :  30.0  step :  31\n",
      "e :  11  reward :  11.0  step :  12\n",
      "e :  12  reward :  18.0  step :  19\n",
      "e :  13  reward :  12.0  step :  13\n",
      "e :  14  reward :  12.0  step :  13\n",
      "e :  15  reward :  30.0  step :  31\n",
      "e :  16  reward :  14.0  step :  15\n",
      "e :  17  reward :  9.0  step :  10\n",
      "e :  18  reward :  10.0  step :  11\n",
      "e :  19  reward :  35.0  step :  36\n",
      "e :  20  reward :  14.0  step :  15\n",
      "e :  21  reward :  7.0  step :  8\n",
      "e :  22  reward :  8.0  step :  9\n",
      "e :  23  reward :  21.0  step :  22\n",
      "e :  24  reward :  21.0  step :  22\n",
      "e :  25  reward :  10.0  step :  11\n",
      "e :  26  reward :  54.0  step :  55\n",
      "e :  27  reward :  19.0  step :  20\n",
      "e :  28  reward :  16.0  step :  17\n",
      "e :  29  reward :  26.0  step :  27\n",
      "e :  30  reward :  21.0  step :  22\n",
      "e :  31  reward :  11.0  step :  12\n",
      "e :  32  reward :  21.0  step :  22\n",
      "e :  33  reward :  76.0  step :  77\n",
      "e :  34  reward :  36.0  step :  37\n",
      "e :  35  reward :  15.0  step :  16\n",
      "e :  36  reward :  24.0  step :  25\n",
      "e :  37  reward :  16.0  step :  17\n",
      "e :  38  reward :  37.0  step :  38\n",
      "e :  39  reward :  28.0  step :  29\n",
      "e :  40  reward :  26.0  step :  27\n",
      "e :  41  reward :  14.0  step :  15\n",
      "e :  42  reward :  11.0  step :  12\n",
      "e :  43  reward :  26.0  step :  27\n",
      "e :  44  reward :  53.0  step :  54\n",
      "e :  45  reward :  25.0  step :  26\n",
      "e :  46  reward :  24.0  step :  25\n",
      "e :  47  reward :  20.0  step :  21\n",
      "e :  48  reward :  65.0  step :  66\n",
      "e :  49  reward :  28.0  step :  29\n",
      "e :  50  reward :  33.0  step :  34\n",
      "e :  51  reward :  28.0  step :  29\n",
      "e :  52  reward :  10.0  step :  11\n",
      "e :  53  reward :  11.0  step :  12\n",
      "e :  54  reward :  31.0  step :  32\n",
      "e :  55  reward :  26.0  step :  27\n",
      "e :  56  reward :  29.0  step :  30\n",
      "e :  57  reward :  13.0  step :  14\n",
      "e :  58  reward :  10.0  step :  11\n",
      "e :  59  reward :  12.0  step :  13\n",
      "e :  60  reward :  35.0  step :  36\n",
      "e :  61  reward :  17.0  step :  18\n",
      "e :  62  reward :  32.0  step :  33\n",
      "e :  63  reward :  22.0  step :  23\n",
      "e :  64  reward :  44.0  step :  45\n",
      "e :  65  reward :  23.0  step :  24\n",
      "e :  66  reward :  21.0  step :  22\n",
      "e :  67  reward :  29.0  step :  30\n",
      "e :  68  reward :  35.0  step :  36\n",
      "e :  69  reward :  23.0  step :  24\n",
      "e :  70  reward :  42.0  step :  43\n",
      "e :  71  reward :  13.0  step :  14\n",
      "e :  72  reward :  51.0  step :  52\n",
      "e :  73  reward :  22.0  step :  23\n",
      "e :  74  reward :  63.0  step :  64\n",
      "e :  75  reward :  9.0  step :  10\n",
      "e :  76  reward :  87.0  step :  88\n",
      "e :  77  reward :  71.0  step :  72\n",
      "e :  78  reward :  12.0  step :  13\n",
      "e :  79  reward :  33.0  step :  34\n",
      "e :  80  reward :  18.0  step :  19\n",
      "e :  81  reward :  43.0  step :  44\n",
      "e :  82  reward :  20.0  step :  21\n",
      "e :  83  reward :  33.0  step :  34\n",
      "e :  84  reward :  47.0  step :  48\n",
      "e :  85  reward :  30.0  step :  31\n",
      "e :  86  reward :  17.0  step :  18\n",
      "e :  87  reward :  44.0  step :  45\n",
      "e :  88  reward :  7.0  step :  8\n",
      "e :  89  reward :  17.0  step :  18\n",
      "e :  90  reward :  8.0  step :  9\n",
      "e :  91  reward :  190.0  step :  191\n",
      "e :  92  reward :  23.0  step :  24\n",
      "e :  93  reward :  133.0  step :  134\n",
      "e :  94  reward :  64.0  step :  65\n",
      "e :  95  reward :  18.0  step :  19\n",
      "e :  96  reward :  21.0  step :  22\n",
      "e :  97  reward :  41.0  step :  42\n",
      "e :  98  reward :  52.0  step :  53\n",
      "e :  99  reward :  35.0  step :  36\n",
      "e :  100  reward :  18.0  step :  19\n",
      "e :  101  reward :  68.0  step :  69\n",
      "e :  102  reward :  11.0  step :  12\n",
      "e :  103  reward :  46.0  step :  47\n",
      "e :  104  reward :  19.0  step :  20\n",
      "e :  105  reward :  45.0  step :  46\n",
      "e :  106  reward :  26.0  step :  27\n",
      "e :  107  reward :  32.0  step :  33\n",
      "e :  108  reward :  71.0  step :  72\n",
      "e :  109  reward :  24.0  step :  25\n",
      "e :  110  reward :  62.0  step :  63\n",
      "e :  111  reward :  30.0  step :  31\n",
      "e :  112  reward :  21.0  step :  22\n",
      "e :  113  reward :  22.0  step :  23\n",
      "e :  114  reward :  25.0  step :  26\n",
      "e :  115  reward :  13.0  step :  14\n",
      "e :  116  reward :  35.0  step :  36\n",
      "e :  117  reward :  13.0  step :  14\n",
      "e :  118  reward :  27.0  step :  28\n",
      "e :  119  reward :  8.0  step :  9\n",
      "e :  120  reward :  29.0  step :  30\n",
      "e :  121  reward :  26.0  step :  27\n",
      "e :  122  reward :  50.0  step :  51\n",
      "e :  123  reward :  78.0  step :  79\n",
      "e :  124  reward :  59.0  step :  60\n",
      "e :  125  reward :  24.0  step :  25\n",
      "e :  126  reward :  13.0  step :  14\n",
      "e :  127  reward :  11.0  step :  12\n",
      "e :  128  reward :  14.0  step :  15\n",
      "e :  129  reward :  39.0  step :  40\n",
      "e :  130  reward :  122.0  step :  123\n",
      "e :  131  reward :  21.0  step :  22\n",
      "e :  132  reward :  31.0  step :  32\n",
      "e :  133  reward :  30.0  step :  31\n",
      "e :  134  reward :  32.0  step :  33\n",
      "e :  135  reward :  25.0  step :  26\n",
      "e :  136  reward :  20.0  step :  21\n",
      "e :  137  reward :  29.0  step :  30\n",
      "e :  138  reward :  47.0  step :  48\n",
      "e :  139  reward :  171.0  step :  172\n",
      "e :  140  reward :  60.0  step :  61\n",
      "e :  141  reward :  52.0  step :  53\n",
      "e :  142  reward :  52.0  step :  53\n",
      "e :  143  reward :  52.0  step :  53\n",
      "e :  144  reward :  33.0  step :  34\n"
     ]
    }
   ],
   "source": [
    "ActorCritic = ActorCriticTrain()\n",
    "ActorCritic.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
