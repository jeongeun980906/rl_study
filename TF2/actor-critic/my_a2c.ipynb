{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers, losses\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\박정은\\gym\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('CartPole-v0')\n",
    "num_action=env.action_space.n\n",
    "print(num_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorModel(Model):\n",
    "    def __init__(self):\n",
    "        super(ActorModel,self).__init__()\n",
    "        self.layer_a1=Dense(64,activation='relu')\n",
    "        self.layer_a2=Dense(64,activation='relu')\n",
    "        self.logits=Dense(num_action,activation='softmax')\n",
    "    def call(self,state):\n",
    "        layer_a1=self.layer_a1(state)\n",
    "        layer_a2=self.layer_a2(layer_a1)\n",
    "        logits=self.logits(layer_a2)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticModel(Model):\n",
    "    def __init__(self):\n",
    "        super(CriticModel,self).__init__()\n",
    "        self.layer_c1=Dense(64,activation='relu')\n",
    "        self.layer_c2=Dense(64,activation='relu')\n",
    "        self.value=Dense(1)\n",
    "        \n",
    "    def call(self,state):\n",
    "        layer_c1=self.layer_c1(state)\n",
    "        layer_c2=self.layer_c2(layer_c1)\n",
    "        value=self.value(layer_c2)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTrain:\n",
    "    def __init__(self):\n",
    "        self.lr=0.001\n",
    "        self.lr2=0.001\n",
    "        self.df=0.99\n",
    "        self.en=0.001\n",
    "        \n",
    "        self.actor_model=ActorModel()\n",
    "        self.actor_opt=optimizers.Adam(lr=self.lr, )\n",
    "        self.critic_model=CriticModel()\n",
    "        self.critic_opt=optimizers.Adam(lr=self.lr2, )\n",
    "        \n",
    "    def actor_loss(self,states,actions,advantages):\n",
    "        policy=self.actor_model(tf.convert_to_tensor(np.vstack(states),\n",
    "                                                     dtype=tf.float32))\n",
    "        entropy=losses.categorical_crossentropy(policy,policy,\n",
    "                                               from_logits=False)\n",
    "        ce_loss=losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "        log_pi=ce_loss(actions,policy)\n",
    "        policy_loss=log_pi*np.array(advantages)\n",
    "        policy_loss=tf.reduce_mean(policy_loss)\n",
    "        \n",
    "        return policy_loss-self.en*entropy\n",
    "    \n",
    "    def critic_loss(self,states,rewards,dones):\n",
    "        last_state=states[-1]\n",
    "        if dones[-1]==True:\n",
    "            reward_sum=0\n",
    "        else:\n",
    "            reward_sum=self.critic_model(tf.convert_to_tensor(last_state[None,:],dtype=tf.float32))\n",
    "        discounted_rewards=[]\n",
    "        for reward in rewards[::-1]:\n",
    "            reward_sum=reward+self.df*reward_sum\n",
    "            discounted_rewards.append(reward_sum)\n",
    "        discounted_rewards.reverse()\n",
    "        discounted_rewards=tf.convert_to_tensor(np.array(discounted_rewards)[:,None],dtype=tf.float32)\n",
    "        values=self.critic_model(tf.convert_to_tensor(np.vstack(states),dtype=tf.float32))\n",
    "        error=tf.square(values-discounted_rewards)*0.5\n",
    "        error=tf.reduce_mean(error)\n",
    "        return error\n",
    "    \n",
    "    def train(self,states,actions,rewards,next_states,dones):\n",
    "        critic_variable=self.critic_model.trainable_variables\n",
    "        with tf.GradientTape() as tape_critic:\n",
    "            tape_critic.watch(critic_variable)\n",
    "            critic_loss=self.critic_loss(states,rewards,dones)\n",
    "        critic_grads=tape_critic.gradient(critic_loss,critic_variable)\n",
    "        self.critic_opt.apply_gradients(zip(critic_grads,critic_variable))\n",
    "        \n",
    "        advantages=self.compute_advantages(states,rewards,dones)\n",
    "        actor_variable=self.actor_model.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(actor_variable)\n",
    "            actor_loss=self.actor_loss(states,actions,advantages)\n",
    "        actor_grads=tape.gradient(actor_loss,actor_variable)\n",
    "        self.actor_opt.apply_gradients(zip(actor_grads,actor_variable))\n",
    "        \n",
    "    def compute_advantages(self,states,rewards,dones):\n",
    "        last_state=states[-1]\n",
    "        if dones[-1]==True:\n",
    "            reward_sum=0\n",
    "        else:\n",
    "            reward_sum=self.critic_model(tf.convert_to_tensor(last_state[None, :],dtype=tf.float32))\n",
    "        discounted_rewards=[]\n",
    "        for reward in rewards[::-1]:\n",
    "            reward_sum=reward+self.df*reward_sum\n",
    "            discounted_rewards.append(reward_sum)\n",
    "        discounted_rewards.reverse()\n",
    "        values=self.critic_model(tf.convert_to_tensor(np.vstack(states),dtype=tf.float32))\n",
    "        advantages=discounted_rewards-values\n",
    "        return advantages\n",
    "    def run(self):\n",
    "        t_end=500\n",
    "        epi=1000\n",
    "        train_size=20\n",
    "        states=[]\n",
    "        actions=[]\n",
    "        rewards=[]\n",
    "        next_states=[]\n",
    "        dones=[]\n",
    "        state=env.reset()\n",
    "        for e in range(epi):\n",
    "            total_reward=0\n",
    "            for t in range(t_end):\n",
    "                policy = self.actor_model(tf.convert_to_tensor(state[None, :], dtype=tf.float32))\n",
    "                action = tf.squeeze(tf.random.categorical(policy, 1), axis=-1)\n",
    "                action = np.array(action)[0]\n",
    "                next_state, reward, done, _ = env.step(action) \n",
    "                env.render()\n",
    "                if t==t_end:\n",
    "                    done=True\n",
    "                    reward+=10\n",
    "                if t<t_end and done:\n",
    "                    reward=-1\n",
    "                total_reward+=reward\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(done)\n",
    "            \n",
    "                state=next_state\n",
    "            \n",
    "                if len(states)==train_size or done:\n",
    "                    self.train(states,actions,rewards,next_states,dones)\n",
    "                    states = []\n",
    "                    actions = []\n",
    "                    rewards = []\n",
    "                    next_states = []\n",
    "                    dones = []\n",
    "                if done:\n",
    "                    print(\"e : \", e, \" reward : \", total_reward, \" step : \", t)\n",
    "                    env.reset()\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e :  0  reward :  13.0  step :  14\n",
      "e :  1  reward :  17.0  step :  18\n",
      "e :  2  reward :  40.0  step :  41\n",
      "e :  3  reward :  18.0  step :  19\n",
      "e :  4  reward :  18.0  step :  19\n",
      "e :  5  reward :  60.0  step :  61\n",
      "e :  6  reward :  11.0  step :  12\n",
      "e :  7  reward :  14.0  step :  15\n",
      "e :  8  reward :  14.0  step :  15\n",
      "e :  9  reward :  14.0  step :  15\n",
      "e :  10  reward :  73.0  step :  74\n",
      "e :  11  reward :  16.0  step :  17\n",
      "e :  12  reward :  30.0  step :  31\n",
      "e :  13  reward :  14.0  step :  15\n",
      "e :  14  reward :  12.0  step :  13\n",
      "e :  15  reward :  18.0  step :  19\n",
      "e :  16  reward :  14.0  step :  15\n",
      "e :  17  reward :  15.0  step :  16\n",
      "e :  18  reward :  17.0  step :  18\n",
      "e :  19  reward :  24.0  step :  25\n",
      "e :  20  reward :  14.0  step :  15\n",
      "e :  21  reward :  17.0  step :  18\n",
      "e :  22  reward :  15.0  step :  16\n",
      "e :  23  reward :  15.0  step :  16\n",
      "e :  24  reward :  19.0  step :  20\n",
      "e :  25  reward :  24.0  step :  25\n",
      "e :  26  reward :  22.0  step :  23\n",
      "e :  27  reward :  31.0  step :  32\n",
      "e :  28  reward :  36.0  step :  37\n",
      "e :  29  reward :  12.0  step :  13\n",
      "e :  30  reward :  17.0  step :  18\n",
      "e :  31  reward :  14.0  step :  15\n",
      "e :  32  reward :  6.0  step :  7\n",
      "e :  33  reward :  46.0  step :  47\n",
      "e :  34  reward :  13.0  step :  14\n",
      "e :  35  reward :  10.0  step :  11\n",
      "e :  36  reward :  19.0  step :  20\n",
      "e :  37  reward :  13.0  step :  14\n",
      "e :  38  reward :  11.0  step :  12\n",
      "e :  39  reward :  10.0  step :  11\n",
      "e :  40  reward :  12.0  step :  13\n",
      "e :  41  reward :  10.0  step :  11\n",
      "e :  42  reward :  16.0  step :  17\n",
      "e :  43  reward :  16.0  step :  17\n",
      "e :  44  reward :  16.0  step :  17\n",
      "e :  45  reward :  15.0  step :  16\n",
      "e :  46  reward :  17.0  step :  18\n",
      "e :  47  reward :  11.0  step :  12\n",
      "e :  48  reward :  18.0  step :  19\n",
      "e :  49  reward :  27.0  step :  28\n",
      "e :  50  reward :  40.0  step :  41\n",
      "e :  51  reward :  40.0  step :  41\n",
      "e :  52  reward :  23.0  step :  24\n",
      "e :  53  reward :  8.0  step :  9\n",
      "e :  54  reward :  24.0  step :  25\n",
      "e :  55  reward :  16.0  step :  17\n",
      "e :  56  reward :  15.0  step :  16\n",
      "e :  57  reward :  13.0  step :  14\n",
      "e :  58  reward :  20.0  step :  21\n",
      "e :  59  reward :  19.0  step :  20\n",
      "e :  60  reward :  24.0  step :  25\n",
      "e :  61  reward :  13.0  step :  14\n",
      "e :  62  reward :  30.0  step :  31\n",
      "e :  63  reward :  29.0  step :  30\n",
      "e :  64  reward :  18.0  step :  19\n",
      "e :  65  reward :  18.0  step :  19\n",
      "e :  66  reward :  12.0  step :  13\n",
      "e :  67  reward :  6.0  step :  7\n",
      "e :  68  reward :  16.0  step :  17\n",
      "e :  69  reward :  36.0  step :  37\n",
      "e :  70  reward :  16.0  step :  17\n",
      "e :  71  reward :  24.0  step :  25\n",
      "e :  72  reward :  17.0  step :  18\n",
      "e :  73  reward :  38.0  step :  39\n",
      "e :  74  reward :  15.0  step :  16\n",
      "e :  75  reward :  47.0  step :  48\n",
      "e :  76  reward :  25.0  step :  26\n",
      "e :  77  reward :  26.0  step :  27\n",
      "e :  78  reward :  26.0  step :  27\n",
      "e :  79  reward :  17.0  step :  18\n",
      "e :  80  reward :  51.0  step :  52\n",
      "e :  81  reward :  24.0  step :  25\n",
      "e :  82  reward :  14.0  step :  15\n",
      "e :  83  reward :  8.0  step :  9\n",
      "e :  84  reward :  25.0  step :  26\n",
      "e :  85  reward :  34.0  step :  35\n",
      "e :  86  reward :  11.0  step :  12\n",
      "e :  87  reward :  10.0  step :  11\n",
      "e :  88  reward :  11.0  step :  12\n",
      "e :  89  reward :  24.0  step :  25\n",
      "e :  90  reward :  13.0  step :  14\n",
      "e :  91  reward :  20.0  step :  21\n",
      "e :  92  reward :  12.0  step :  13\n",
      "e :  93  reward :  16.0  step :  17\n",
      "e :  94  reward :  18.0  step :  19\n",
      "e :  95  reward :  18.0  step :  19\n",
      "e :  96  reward :  36.0  step :  37\n",
      "e :  97  reward :  11.0  step :  12\n",
      "e :  98  reward :  27.0  step :  28\n",
      "e :  99  reward :  25.0  step :  26\n",
      "e :  100  reward :  34.0  step :  35\n",
      "e :  101  reward :  22.0  step :  23\n",
      "e :  102  reward :  30.0  step :  31\n",
      "e :  103  reward :  41.0  step :  42\n",
      "e :  104  reward :  20.0  step :  21\n",
      "e :  105  reward :  11.0  step :  12\n",
      "e :  106  reward :  62.0  step :  63\n",
      "e :  107  reward :  22.0  step :  23\n",
      "e :  108  reward :  42.0  step :  43\n",
      "e :  109  reward :  33.0  step :  34\n",
      "e :  110  reward :  42.0  step :  43\n",
      "e :  111  reward :  49.0  step :  50\n",
      "e :  112  reward :  22.0  step :  23\n",
      "e :  113  reward :  36.0  step :  37\n",
      "e :  114  reward :  19.0  step :  20\n",
      "e :  115  reward :  19.0  step :  20\n",
      "e :  116  reward :  20.0  step :  21\n",
      "e :  117  reward :  26.0  step :  27\n",
      "e :  118  reward :  10.0  step :  11\n",
      "e :  119  reward :  23.0  step :  24\n",
      "e :  120  reward :  12.0  step :  13\n",
      "e :  121  reward :  50.0  step :  51\n",
      "e :  122  reward :  28.0  step :  29\n",
      "e :  123  reward :  17.0  step :  18\n",
      "e :  124  reward :  10.0  step :  11\n",
      "e :  125  reward :  31.0  step :  32\n",
      "e :  126  reward :  48.0  step :  49\n",
      "e :  127  reward :  41.0  step :  42\n",
      "e :  128  reward :  12.0  step :  13\n",
      "e :  129  reward :  28.0  step :  29\n",
      "e :  130  reward :  49.0  step :  50\n",
      "e :  131  reward :  28.0  step :  29\n",
      "e :  132  reward :  41.0  step :  42\n",
      "e :  133  reward :  50.0  step :  51\n",
      "e :  134  reward :  34.0  step :  35\n",
      "e :  135  reward :  23.0  step :  24\n",
      "e :  136  reward :  11.0  step :  12\n",
      "e :  137  reward :  18.0  step :  19\n",
      "e :  138  reward :  92.0  step :  93\n",
      "e :  139  reward :  26.0  step :  27\n",
      "e :  140  reward :  70.0  step :  71\n",
      "e :  141  reward :  22.0  step :  23\n",
      "e :  142  reward :  29.0  step :  30\n",
      "e :  143  reward :  33.0  step :  34\n",
      "e :  144  reward :  11.0  step :  12\n",
      "e :  145  reward :  18.0  step :  19\n",
      "e :  146  reward :  51.0  step :  52\n",
      "e :  147  reward :  34.0  step :  35\n",
      "e :  148  reward :  12.0  step :  13\n",
      "e :  149  reward :  18.0  step :  19\n",
      "e :  150  reward :  23.0  step :  24\n",
      "e :  151  reward :  78.0  step :  79\n",
      "e :  152  reward :  44.0  step :  45\n",
      "e :  153  reward :  23.0  step :  24\n",
      "e :  154  reward :  38.0  step :  39\n",
      "e :  155  reward :  25.0  step :  26\n",
      "e :  156  reward :  17.0  step :  18\n",
      "e :  157  reward :  78.0  step :  79\n",
      "e :  158  reward :  29.0  step :  30\n",
      "e :  159  reward :  15.0  step :  16\n",
      "e :  160  reward :  66.0  step :  67\n",
      "e :  161  reward :  13.0  step :  14\n",
      "e :  162  reward :  47.0  step :  48\n",
      "e :  163  reward :  91.0  step :  92\n",
      "e :  164  reward :  21.0  step :  22\n",
      "e :  165  reward :  40.0  step :  41\n",
      "e :  166  reward :  33.0  step :  34\n",
      "e :  167  reward :  111.0  step :  112\n",
      "e :  168  reward :  26.0  step :  27\n",
      "e :  169  reward :  13.0  step :  14\n",
      "e :  170  reward :  68.0  step :  69\n",
      "e :  171  reward :  51.0  step :  52\n",
      "e :  172  reward :  54.0  step :  55\n",
      "e :  173  reward :  30.0  step :  31\n",
      "e :  174  reward :  56.0  step :  57\n",
      "e :  175  reward :  46.0  step :  47\n",
      "e :  176  reward :  32.0  step :  33\n",
      "e :  177  reward :  74.0  step :  75\n",
      "e :  178  reward :  37.0  step :  38\n",
      "e :  179  reward :  90.0  step :  91\n",
      "e :  180  reward :  22.0  step :  23\n",
      "e :  181  reward :  147.0  step :  148\n",
      "e :  182  reward :  98.0  step :  99\n",
      "e :  183  reward :  31.0  step :  32\n",
      "e :  184  reward :  39.0  step :  40\n",
      "e :  185  reward :  110.0  step :  111\n",
      "e :  186  reward :  44.0  step :  45\n",
      "e :  187  reward :  134.0  step :  135\n",
      "e :  188  reward :  85.0  step :  86\n",
      "e :  189  reward :  31.0  step :  32\n",
      "e :  190  reward :  63.0  step :  64\n",
      "e :  191  reward :  25.0  step :  26\n",
      "e :  192  reward :  130.0  step :  131\n",
      "e :  193  reward :  58.0  step :  59\n",
      "e :  194  reward :  63.0  step :  64\n",
      "e :  195  reward :  100.0  step :  101\n",
      "e :  196  reward :  40.0  step :  41\n",
      "e :  197  reward :  133.0  step :  134\n",
      "e :  198  reward :  22.0  step :  23\n",
      "e :  199  reward :  39.0  step :  40\n",
      "e :  200  reward :  107.0  step :  108\n",
      "e :  201  reward :  70.0  step :  71\n",
      "e :  202  reward :  12.0  step :  13\n",
      "e :  203  reward :  46.0  step :  47\n",
      "e :  204  reward :  194.0  step :  195\n",
      "e :  205  reward :  31.0  step :  32\n",
      "e :  206  reward :  66.0  step :  67\n",
      "e :  207  reward :  197.0  step :  198\n",
      "e :  208  reward :  19.0  step :  20\n",
      "e :  209  reward :  91.0  step :  92\n",
      "e :  210  reward :  98.0  step :  99\n",
      "e :  211  reward :  141.0  step :  142\n",
      "e :  212  reward :  116.0  step :  117\n",
      "e :  213  reward :  43.0  step :  44\n",
      "e :  214  reward :  38.0  step :  39\n",
      "e :  215  reward :  12.0  step :  13\n",
      "e :  216  reward :  49.0  step :  50\n",
      "e :  217  reward :  48.0  step :  49\n",
      "e :  218  reward :  194.0  step :  195\n",
      "e :  219  reward :  144.0  step :  145\n",
      "e :  220  reward :  198.0  step :  199\n",
      "e :  221  reward :  173.0  step :  174\n",
      "e :  222  reward :  62.0  step :  63\n",
      "e :  223  reward :  100.0  step :  101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e :  224  reward :  42.0  step :  43\n",
      "e :  225  reward :  25.0  step :  26\n",
      "e :  226  reward :  85.0  step :  86\n",
      "e :  227  reward :  26.0  step :  27\n",
      "e :  228  reward :  9.0  step :  10\n",
      "e :  229  reward :  32.0  step :  33\n",
      "e :  230  reward :  94.0  step :  95\n",
      "e :  231  reward :  139.0  step :  140\n",
      "e :  232  reward :  158.0  step :  159\n",
      "e :  233  reward :  35.0  step :  36\n",
      "e :  234  reward :  69.0  step :  70\n",
      "e :  235  reward :  93.0  step :  94\n",
      "e :  236  reward :  111.0  step :  112\n",
      "e :  237  reward :  62.0  step :  63\n",
      "e :  238  reward :  26.0  step :  27\n",
      "e :  239  reward :  54.0  step :  55\n",
      "e :  240  reward :  63.0  step :  64\n",
      "e :  241  reward :  146.0  step :  147\n",
      "e :  242  reward :  123.0  step :  124\n",
      "e :  243  reward :  62.0  step :  63\n",
      "e :  244  reward :  64.0  step :  65\n",
      "e :  245  reward :  49.0  step :  50\n",
      "e :  246  reward :  17.0  step :  18\n",
      "e :  247  reward :  108.0  step :  109\n",
      "e :  248  reward :  198.0  step :  199\n",
      "e :  249  reward :  17.0  step :  18\n",
      "e :  250  reward :  198.0  step :  199\n",
      "e :  251  reward :  29.0  step :  30\n",
      "e :  252  reward :  20.0  step :  21\n",
      "e :  253  reward :  71.0  step :  72\n",
      "e :  254  reward :  49.0  step :  50\n",
      "e :  255  reward :  87.0  step :  88\n",
      "e :  256  reward :  76.0  step :  77\n",
      "e :  257  reward :  107.0  step :  108\n",
      "e :  258  reward :  79.0  step :  80\n",
      "e :  259  reward :  114.0  step :  115\n",
      "e :  260  reward :  52.0  step :  53\n",
      "e :  261  reward :  198.0  step :  199\n",
      "e :  262  reward :  31.0  step :  32\n",
      "e :  263  reward :  30.0  step :  31\n",
      "e :  264  reward :  14.0  step :  15\n",
      "e :  265  reward :  36.0  step :  37\n",
      "e :  266  reward :  61.0  step :  62\n",
      "e :  267  reward :  141.0  step :  142\n",
      "e :  268  reward :  117.0  step :  118\n",
      "e :  269  reward :  49.0  step :  50\n",
      "e :  270  reward :  18.0  step :  19\n",
      "e :  271  reward :  11.0  step :  12\n",
      "e :  272  reward :  165.0  step :  166\n",
      "e :  273  reward :  97.0  step :  98\n",
      "e :  274  reward :  15.0  step :  16\n",
      "e :  275  reward :  46.0  step :  47\n",
      "e :  276  reward :  60.0  step :  61\n",
      "e :  277  reward :  148.0  step :  149\n",
      "e :  278  reward :  187.0  step :  188\n",
      "e :  279  reward :  14.0  step :  15\n",
      "e :  280  reward :  19.0  step :  20\n",
      "e :  281  reward :  58.0  step :  59\n",
      "e :  282  reward :  47.0  step :  48\n",
      "e :  283  reward :  112.0  step :  113\n",
      "e :  284  reward :  198.0  step :  199\n",
      "e :  285  reward :  133.0  step :  134\n",
      "e :  286  reward :  149.0  step :  150\n",
      "e :  287  reward :  110.0  step :  111\n",
      "e :  288  reward :  99.0  step :  100\n",
      "e :  289  reward :  83.0  step :  84\n",
      "e :  290  reward :  27.0  step :  28\n",
      "e :  291  reward :  24.0  step :  25\n",
      "e :  292  reward :  58.0  step :  59\n",
      "e :  293  reward :  91.0  step :  92\n",
      "e :  294  reward :  85.0  step :  86\n",
      "e :  295  reward :  94.0  step :  95\n",
      "e :  296  reward :  38.0  step :  39\n",
      "e :  297  reward :  48.0  step :  49\n",
      "e :  298  reward :  95.0  step :  96\n",
      "e :  299  reward :  120.0  step :  121\n",
      "e :  300  reward :  198.0  step :  199\n",
      "e :  301  reward :  138.0  step :  139\n",
      "e :  302  reward :  198.0  step :  199\n",
      "e :  303  reward :  84.0  step :  85\n",
      "e :  304  reward :  56.0  step :  57\n",
      "e :  305  reward :  198.0  step :  199\n",
      "e :  306  reward :  74.0  step :  75\n",
      "e :  307  reward :  97.0  step :  98\n",
      "e :  308  reward :  198.0  step :  199\n",
      "e :  309  reward :  165.0  step :  166\n",
      "e :  310  reward :  177.0  step :  178\n",
      "e :  311  reward :  198.0  step :  199\n",
      "e :  312  reward :  97.0  step :  98\n",
      "e :  313  reward :  198.0  step :  199\n",
      "e :  314  reward :  43.0  step :  44\n",
      "e :  315  reward :  17.0  step :  18\n",
      "e :  316  reward :  23.0  step :  24\n",
      "e :  317  reward :  182.0  step :  183\n",
      "e :  318  reward :  15.0  step :  16\n",
      "e :  319  reward :  141.0  step :  142\n",
      "e :  320  reward :  172.0  step :  173\n",
      "e :  321  reward :  198.0  step :  199\n",
      "e :  322  reward :  118.0  step :  119\n",
      "e :  323  reward :  198.0  step :  199\n",
      "e :  324  reward :  93.0  step :  94\n",
      "e :  325  reward :  17.0  step :  18\n",
      "e :  326  reward :  16.0  step :  17\n",
      "e :  327  reward :  10.0  step :  11\n",
      "e :  328  reward :  25.0  step :  26\n",
      "e :  329  reward :  70.0  step :  71\n",
      "e :  330  reward :  48.0  step :  49\n",
      "e :  331  reward :  82.0  step :  83\n",
      "e :  332  reward :  47.0  step :  48\n",
      "e :  333  reward :  147.0  step :  148\n",
      "e :  334  reward :  19.0  step :  20\n",
      "e :  335  reward :  74.0  step :  75\n",
      "e :  336  reward :  37.0  step :  38\n",
      "e :  337  reward :  40.0  step :  41\n",
      "e :  338  reward :  198.0  step :  199\n",
      "e :  339  reward :  81.0  step :  82\n",
      "e :  340  reward :  53.0  step :  54\n",
      "e :  341  reward :  72.0  step :  73\n",
      "e :  342  reward :  150.0  step :  151\n",
      "e :  343  reward :  191.0  step :  192\n",
      "e :  344  reward :  198.0  step :  199\n",
      "e :  345  reward :  107.0  step :  108\n",
      "e :  346  reward :  172.0  step :  173\n",
      "e :  347  reward :  198.0  step :  199\n",
      "e :  348  reward :  14.0  step :  15\n",
      "e :  349  reward :  110.0  step :  111\n",
      "e :  350  reward :  82.0  step :  83\n",
      "e :  351  reward :  167.0  step :  168\n",
      "e :  352  reward :  107.0  step :  108\n",
      "e :  353  reward :  65.0  step :  66\n",
      "e :  354  reward :  57.0  step :  58\n",
      "e :  355  reward :  172.0  step :  173\n",
      "e :  356  reward :  16.0  step :  17\n",
      "e :  357  reward :  98.0  step :  99\n",
      "e :  358  reward :  144.0  step :  145\n",
      "e :  359  reward :  35.0  step :  36\n",
      "e :  360  reward :  151.0  step :  152\n",
      "e :  361  reward :  84.0  step :  85\n",
      "e :  362  reward :  20.0  step :  21\n",
      "e :  363  reward :  198.0  step :  199\n",
      "e :  364  reward :  119.0  step :  120\n",
      "e :  365  reward :  55.0  step :  56\n",
      "e :  366  reward :  138.0  step :  139\n",
      "e :  367  reward :  71.0  step :  72\n",
      "e :  368  reward :  122.0  step :  123\n",
      "e :  369  reward :  38.0  step :  39\n",
      "e :  370  reward :  72.0  step :  73\n",
      "e :  371  reward :  167.0  step :  168\n",
      "e :  372  reward :  47.0  step :  48\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d45810bfa0d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mActorCritic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mActorCriticTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mActorCritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-f907030a6afb>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mt_end\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                     \u001b[0mdone\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\박정은\\gym\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\박정은\\gym\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\박정은\\gym\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_always_dwm\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dwm_composition_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m                     \u001b[0m_dwmapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDwmFlush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ActorCritic = ActorCriticTrain()\n",
    "ActorCritic.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
