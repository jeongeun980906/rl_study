{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, state_dim, action_dim, action_bound, learning_rate):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.std_bound = [1e-2, 1.0] # std bound\n",
    "\n",
    "        self.model = self.build_network()\n",
    "\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "\n",
    "    ## actor network\n",
    "    def build_network(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        h1 = Dense(64, activation='relu')(state_input)\n",
    "        h2 = Dense(32, activation='relu')(h1)\n",
    "        h3 = Dense(16, activation='relu')(h2)\n",
    "        out_mu = Dense(self.action_dim, activation='tanh')(h3)\n",
    "        std_output = Dense(self.action_dim, activation='softplus')(h3)\n",
    "\n",
    "        # Scale output to [-action_bound, action_bound]\n",
    "        mu_output = Lambda(lambda x: x*self.action_bound)(out_mu)\n",
    "        model = Model(state_input, [mu_output, std_output])\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    ## log policy pdf\n",
    "    def log_pdf(self, mu, std, action):\n",
    "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
    "        var = std**2\n",
    "        log_policy_pdf = -0.5 * (action - mu) ** 2 / var - 0.5 * tf.math.log(var * 2 * np.pi)\n",
    "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
    "\n",
    "\n",
    "    ## actor policy\n",
    "    def get_action(self, state):\n",
    "        # type of action in env is numpy array\n",
    "        # np.reshape(state, [1, self.state_dim]) : shape (state_dim,) -> shape (1, state_dim)\n",
    "        # why [0]?  shape (1, action_dim) -> (action_dim,)\n",
    "        mu_a, std_a = self.model.predict(np.reshape(state, [1, self.state_dim]))\n",
    "        mu_a = mu_a[0]\n",
    "        std_a = std_a[0]\n",
    "        std_a = np.clip(std_a, self.std_bound[0], self.std_bound[1])\n",
    "        action = np.random.normal(mu_a, std_a, size=self.action_dim)\n",
    "        return action\n",
    "\n",
    "    def train(self, states, actions, advantages):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # policy pdf\n",
    "            mu_a, std_a = self.model(states)\n",
    "            log_policy_pdf = self.log_pdf(mu_a, std_a, actions)\n",
    "\n",
    "            # loss functions and its gradient\n",
    "            loss_policy = log_policy_pdf * advantages\n",
    "            loss = tf.reduce_sum(-loss_policy)\n",
    "        dj_dtheta = tape.gradient(loss, self.model.trainable_variables)\n",
    "        grads = zip(dj_dtheta, self.model.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(grads)\n",
    "\n",
    "    ## actor prediction\n",
    "    def predict(self, state):\n",
    "        mu_a, _= self.model.predict(np.reshape(state, [1, self.state_dim]))\n",
    "        return mu_a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    \"\"\"\n",
    "        Critic Network for A2C: V function approximator\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, learning_rate):\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # create and compile critic network\n",
    "        self.model, self.states = self.build_network()\n",
    "\n",
    "        self.model.compile(optimizer=Adam(self.learning_rate), loss='mse')\n",
    "\n",
    "    ## critic network\n",
    "    def build_network(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        h1 = Dense(64, activation='relu')(state_input)\n",
    "        h2 = Dense(32, activation='relu')(h1)\n",
    "        h3 = Dense(16, activation='relu')(h2)\n",
    "        v_output = Dense(1, activation='linear')(h3)\n",
    "        model = Model(state_input, v_output)\n",
    "        model.summary()\n",
    "        return model, state_input\n",
    "\n",
    "\n",
    "    ## single gradient update on a single batch data\n",
    "    def train_on_batch(self, states, td_targets):\n",
    "        return self.model.train_on_batch(states, td_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2Cagent(object):\n",
    "\n",
    "    def __init__(self, env):\n",
    "\n",
    "        # hyperparameters\n",
    "        self.GAMMA = 0.95\n",
    "        self.BATCH_SIZE = 32\n",
    "        self.ACTOR_LEARNING_RATE = 0.0001\n",
    "        self.CRITIC_LEARNING_RATE = 0.001\n",
    "\n",
    "        self.env = env\n",
    "        # get state dimension\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        # get action dimension\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        # get action bound\n",
    "        self.action_bound = env.action_space.high[0]\n",
    "\n",
    "        # create actor and critic networks\n",
    "        self.actor = Actor(self.state_dim, self.action_dim, self.action_bound, self.ACTOR_LEARNING_RATE)\n",
    "        self.critic = Critic(self.state_dim, self.action_dim, self.CRITIC_LEARNING_RATE)\n",
    "\n",
    "        # save the results\n",
    "        self.save_epi_reward = []\n",
    "\n",
    "\n",
    "    ## computing Advantages and targets: y_k = r_k + gamma*V(s_k+1), A(s_k, a_k)= y_k - V(s_k)\n",
    "    def advantage_td_target(self, reward, v_value, next_v_value, done):\n",
    "        if done:\n",
    "                y_k = reward\n",
    "                advantage = y_k - v_value\n",
    "        else:\n",
    "                y_k = reward + self.GAMMA * next_v_value\n",
    "                advantage = y_k - v_value\n",
    "        return advantage, y_k\n",
    "\n",
    "\n",
    "    ## convert (list of np.array) to np.array\n",
    "    def unpack_batch(self, batch):\n",
    "        unpack = batch[0]\n",
    "        for idx in range(len(batch)-1):\n",
    "            unpack = np.append(unpack, batch[idx+1], axis=0)\n",
    "\n",
    "        return unpack\n",
    "\n",
    "\n",
    "    ## train the agent\n",
    "    def train(self, max_episode_num):\n",
    "\n",
    "        for ep in range(int(max_episode_num)):\n",
    "\n",
    "            # initialize batch\n",
    "            batch_state, batch_action, batch_td_target, batch_advantage = [], [], [], []\n",
    "            # reset episode\n",
    "            time, episode_reward, done = 0, 0, False\n",
    "            # reset the environment and observe the first state\n",
    "            state = self.env.reset() # shape of state from gym (3,)\n",
    "\n",
    "            while not done:\n",
    "\n",
    "                # visualize the environment\n",
    "                self.env.render()\n",
    "                # pick an action (shape of gym action = (action_dim,) )\n",
    "                action = self.actor.get_action(state)\n",
    "                # clip continuous action to be within action_bound\n",
    "                action = np.clip(action, -self.action_bound, self.action_bound)\n",
    "                # observe reward, new_state, shape of output of gym (state_dim,)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                # change shape (state_dim,) -> (1, state_dim), same to action, next_state\n",
    "                state = np.reshape(state, [1, self.state_dim])\n",
    "                next_state = np.reshape(next_state, [1, self.state_dim])\n",
    "                action = np.reshape(action, [1, self.action_dim])\n",
    "                reward = np.reshape(reward, [1, 1])\n",
    "                # compute next v_value\n",
    "                v_value = self.critic.model.predict(state)\n",
    "                next_v_value = self.critic.model.predict(next_state)\n",
    "                # compute advantage and TD target\n",
    "                train_reward = (reward+8)/8  # <-- normalization\n",
    "                advantage, y_i = self.advantage_td_target(train_reward, v_value, next_v_value, done)\n",
    "\n",
    "                # append to the batch\n",
    "                batch_state.append(state)\n",
    "                batch_action.append(action)\n",
    "                batch_td_target.append(y_i)\n",
    "                batch_advantage.append(advantage)\n",
    "\n",
    "                # continue until batch becomes full\n",
    "                if len(batch_state) < self.BATCH_SIZE:\n",
    "                    # update current state\n",
    "                    state = next_state[0]\n",
    "                    episode_reward += reward[0]\n",
    "                    time += 1\n",
    "                    continue\n",
    "\n",
    "                # if batch is full, start to train networks on batch\n",
    "                # extract batched states, actions, td_targets, advantages\n",
    "                states = self.unpack_batch(batch_state)\n",
    "                actions = self.unpack_batch(batch_action)\n",
    "                td_targets = self.unpack_batch(batch_td_target)\n",
    "                advantages = self.unpack_batch(batch_advantage)\n",
    "                # clear the batch\n",
    "                batch_state, batch_action, batch_td_target, batch_advantage = [], [], [], []\n",
    "                # train critic\n",
    "                self.critic.train_on_batch(states, td_targets)\n",
    "                # train actor\n",
    "                self.actor.train(states, actions, advantages)\n",
    "\n",
    "                # update current state\n",
    "                state = next_state[0]\n",
    "                episode_reward += reward[0]\n",
    "                time += 1\n",
    "\n",
    "\n",
    "            ## display rewards every episode\n",
    "            print('Episode: ', ep+1, 'Time: ', time, 'Reward: ', episode_reward)\n",
    "\n",
    "            self.save_epi_reward.append(episode_reward)\n",
    "\n",
    "        np.savetxt('./save_weights/pendulum_epi_reward.txt', self.save_epi_reward)\n",
    "        print(self.save_epi_reward)\n",
    "\n",
    "\n",
    "    ## save them to file if done\n",
    "    def plot_result(self):\n",
    "        plt.plot(self.save_epi_reward)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\박정은\\gym\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           256         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           2080        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           528         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            17          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            17          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,898\n",
      "Trainable params: 2,898\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 2,881\n",
      "Trainable params: 2,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Episode:  1 Time:  200 Reward:  [-1363.92476914]\n",
      "Episode:  2 Time:  200 Reward:  [-1537.8662081]\n",
      "Episode:  3 Time:  200 Reward:  [-1807.36709073]\n",
      "Episode:  4 Time:  200 Reward:  [-1357.67158741]\n",
      "Episode:  5 Time:  200 Reward:  [-1547.93751394]\n",
      "Episode:  6 Time:  200 Reward:  [-1572.3318145]\n",
      "Episode:  7 Time:  200 Reward:  [-1597.99234614]\n",
      "Episode:  8 Time:  200 Reward:  [-1370.04491836]\n",
      "Episode:  9 Time:  200 Reward:  [-1532.51211761]\n",
      "Episode:  10 Time:  200 Reward:  [-1617.24422866]\n",
      "Episode:  11 Time:  200 Reward:  [-1605.07563577]\n",
      "Episode:  12 Time:  200 Reward:  [-1373.08475058]\n"
     ]
    }
   ],
   "source": [
    "max_episode_num = 1000\n",
    "env_name = 'Pendulum-v0'\n",
    "env = gym.make(env_name)\n",
    "agent = A2Cagent(env)\n",
    "\n",
    "agent.train(max_episode_num)\n",
    "\n",
    "agent.plot_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
