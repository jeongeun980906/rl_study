{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers, losses\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "num_action = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorModel(Model):\n",
    "    def __init__(self):\n",
    "        super(ActorModel, self).__init__()\n",
    "        self.layer_a1 = Dense(64, activation='relu')\n",
    "        self.layer_a2 = Dense(64, activation='relu')\n",
    "        self.logits = Dense(num_action, activation='softmax')\n",
    "\n",
    "    def call(self, state):\n",
    "        layer_a1 = self.layer_a1(state)\n",
    "        layer_a2 = self.layer_a2(layer_a1)\n",
    "        logits = self.logits(layer_a2)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticModel(Model):\n",
    "    def __init__(self):\n",
    "        super(CriticModel, self).__init__()\n",
    "        self.layer_c1 = Dense(64, activation='relu')\n",
    "        self.layer_c2 = Dense(64, activation='relu')\n",
    "        self.value = Dense(1)\n",
    "\n",
    "    def call(self, state):\n",
    "        layer_c1 = self.layer_c1(state)\n",
    "        layer_c2 = self.layer_c2(layer_c1)\n",
    "        value = self.value(layer_c2)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTrain:\n",
    "    def __init__(self):\n",
    "        # hyper parameters\n",
    "        self.lr =0.001\n",
    "        self.lr2 = 0.001\n",
    "        self.df = 0.99\n",
    "        self.en = 0.001\n",
    "\n",
    "        self.actor_model = ActorModel()\n",
    "        self.actor_opt = optimizers.Adam(lr=self.lr, )\n",
    "\n",
    "        self.critic_model = CriticModel()\n",
    "        self.critic_opt = optimizers.Adam(lr=self.lr2, )\n",
    "\n",
    "        # tensorboard\n",
    "        self.log_dir = 'logs/'\n",
    "        self.train_summary_writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        self.reward_board = tf.keras.metrics.Mean('reward_board', dtype=tf.float32)\n",
    "        #self.train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "        #self.train_loss_c = tf.keras.metrics.Mean('train_loss_c', dtype=tf.float32)\n",
    "\n",
    "    def actor_loss(self, states, actions, advantages):\n",
    "        policy = self.actor_model(tf.convert_to_tensor(np.vstack(states), dtype=tf.float32))\n",
    "\n",
    "        # SparseCategoricalCrossentropy = ce loss with not one-hot encoded output\n",
    "        # from_logits = True  =>  cross_entropy with soft_max\n",
    "        entropy = losses.categorical_crossentropy(policy, policy, from_logits=False)\n",
    "        ce_loss = losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "        # policy_loss = ce_loss(actions, policy, sample_weight=np.array(advantages))  # same way\n",
    "        log_pi = ce_loss(actions, policy)\n",
    "        policy_loss = log_pi * np.array(advantages)\n",
    "        policy_loss = tf.reduce_mean(policy_loss)\n",
    "\n",
    "        return policy_loss - self.en * entropy\n",
    "\n",
    "    def critic_loss(self, states, rewards, dones):\n",
    "        last_state = states[-1]\n",
    "        if dones[-1] == True :\n",
    "            reward_sum = 0\n",
    "        else :\n",
    "            reward_sum = self.critic_model(tf.convert_to_tensor(last_state[None, :], dtype=tf.float32))\n",
    "        discounted_rewards = []\n",
    "        for reward in rewards[::-1]:\n",
    "            reward_sum = reward + self.df * reward_sum\n",
    "            discounted_rewards.append(reward_sum)\n",
    "        discounted_rewards.reverse()\n",
    "        discounted_rewards = tf.convert_to_tensor(np.array(discounted_rewards)[:, None], dtype=tf.float32)\n",
    "        values = self.critic_model(tf.convert_to_tensor(np.vstack(states), dtype=tf.float32))\n",
    "        error = tf.square(values - discounted_rewards)*0.5\n",
    "        error = tf.reduce_mean(error)\n",
    "        return error\n",
    "\n",
    "    def train(self, states, actions, rewards, next_states, dones):\n",
    "\n",
    "        critic_variable = self.critic_model.trainable_variables\n",
    "        with tf.GradientTape() as tape_critic:\n",
    "            tape_critic.watch(critic_variable)\n",
    "            critic_loss = self.critic_loss(states, rewards, dones)\n",
    "\n",
    "        # gradient descent will be applied automatically\n",
    "        critic_grads = tape_critic.gradient(critic_loss, critic_variable)\n",
    "        self.critic_opt.apply_gradients(zip(critic_grads, critic_variable))\n",
    "\n",
    "\n",
    "        advantages = self.compute_advantages(states, rewards, dones)\n",
    "        actor_variable = self.actor_model.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(actor_variable)\n",
    "            actor_loss = self.actor_loss(states, actions, advantages)\n",
    "\n",
    "        actor_grads = tape.gradient(actor_loss, actor_variable)\n",
    "        self.actor_opt.apply_gradients(zip(actor_grads, actor_variable))\n",
    "\n",
    "        # self.train_loss(tf.reduce_mean(actor_loss))\n",
    "        # self.train_loss_c(tf.reduce_mean(critic_loss))\n",
    "        self.train_loss = tf.reduce_mean(actor_loss)\n",
    "        self.train_loss_c = tf.reduce_mean(critic_loss)\n",
    "\n",
    "    def compute_advantages(self, states, rewards, dones):\n",
    "        last_state = states[-1]\n",
    "        if dones[-1] == True:\n",
    "            reward_sum = 0\n",
    "        else:\n",
    "            reward_sum = self.critic_model(tf.convert_to_tensor(last_state[None, :], dtype=tf.float32))\n",
    "        discounted_rewards = []\n",
    "        for reward in rewards[::-1]:\n",
    "            reward_sum = reward + self.df * reward_sum\n",
    "            discounted_rewards.append(reward_sum)\n",
    "        discounted_rewards.reverse()\n",
    "        values = self.critic_model(tf.convert_to_tensor(np.vstack(states), dtype=tf.float32))\n",
    "        advantages = discounted_rewards - values\n",
    "        return advantages\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        t_end = 500\n",
    "        epi = 100000\n",
    "        train_size = 20\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "\n",
    "        state = env.reset()\n",
    "        for e in range(epi):\n",
    "            total_reward = 0\n",
    "            #env.render()\n",
    "            for t in range(t_end):\n",
    "                policy = self.actor_model(tf.convert_to_tensor(state[None, :], dtype=tf.float32))\n",
    "                action = tf.squeeze(tf.random.categorical(policy, 1), axis=-1)\n",
    "                action = np.array(action)[0]\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                env.render()\n",
    "                if t == t_end :\n",
    "                    done = True\n",
    "                    reward += 10\n",
    "                if t < t_end and done :\n",
    "                    reward = -1\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(done)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "\n",
    "                if len(states) == train_size or done :\n",
    "                    self.train(states, actions, rewards, next_states, dones)\n",
    "                    states = []\n",
    "                    actions = []\n",
    "                    rewards = []\n",
    "                    next_states = []\n",
    "                    dones = []\n",
    "\n",
    "                if done:\n",
    "                    self.reward_board(total_reward)\n",
    "                    print(\"e : \", e, \" reward : \", total_reward, \" step : \", t)\n",
    "                    env.reset()\n",
    "                    with self.train_summary_writer.as_default():\n",
    "                        # tf.summary.scalar('reward', self.reward_board.result(), step=e)\n",
    "                        tf.summary.scalar('actor_loss', self.train_loss, step=e)\n",
    "                        tf.summary.scalar('critic_loss', self.train_loss_c, step=e)\n",
    "                        tf.summary.scalar('reward', total_reward, step=e)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActorCritic = ActorCriticTrain()\n",
    "ActorCritic.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
