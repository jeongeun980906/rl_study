{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda,concatenate\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    \"\"\"\n",
    "        Actor Network for DDPG\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, action_bound, tau, learning_rate):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.model = self.build_network()\n",
    "        self.target_model = self.build_network()\n",
    "\n",
    "\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
    "\n",
    "    ## actor network\n",
    "    def build_network(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        h1 = Dense(64, activation='relu')(state_input)\n",
    "        h2 = Dense(32, activation='relu')(h1)\n",
    "        h3 = Dense(16, activation='relu')(h2)\n",
    "        out = Dense(self.action_dim, activation='tanh')(h3)\n",
    "\n",
    "        # Scale output to [-action_bound, action_bound]\n",
    "        action_output = Lambda(lambda x: x*self.action_bound)(out)\n",
    "        model = Model(state_input, action_output)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "\n",
    "    ## actor prediction\n",
    "    def predict(self, state):\n",
    "\t\t# type of action in env is numpy array\n",
    "        return self.model.predict(np.reshape(state, [1, self.state_dim]))[0]\n",
    "\n",
    "\n",
    "    ## target actor prediction\n",
    "    def target_predict(self, state):\n",
    "        return self.target_model.predict(state)\n",
    "\n",
    "\n",
    "    ## transfer actor weights to target actor with a aau\n",
    "    def update_target_network(self):\n",
    "        theta, target_theta = self.model.get_weights(), self.target_model.get_weights()\n",
    "        for i in range(len(theta)):\n",
    "            target_theta[i] = self.tau * theta[i] + (1 - self.tau) * target_theta[i]\n",
    "        self.target_model.set_weights(target_theta)\n",
    "\n",
    "\n",
    "    ## train the actor network\n",
    "    def train(self, states, dq_das):\n",
    "        with tf.GradientTape() as tape:\n",
    "            self.dj_dtheta = tape.gradient(self.model(states), self.model.trainable_variables, -dq_das)\n",
    "        grads = zip(self.dj_dtheta, self.model.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    \"\"\"\n",
    "        Critic Network for DDPG: Q function approximator\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, tau, learning_rate):\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # create critic and target critic network\n",
    "        self.model = self.build_network()\n",
    "        self.target_model = self.build_network()\n",
    "\n",
    "        self.model.compile(optimizer=Adam(self.learning_rate), loss='mse')\n",
    "        self.target_model.compile(optimizer=Adam(self.learning_rate), loss='mse')\n",
    "\n",
    "\n",
    "    ## critic network\n",
    "    def build_network(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        action_input = Input((self.action_dim,))\n",
    "        x1 = Dense(64, activation='relu')(state_input)\n",
    "        x2 = Dense(32, activation='linear')(x1)\n",
    "        #a1 = Dense(1, activation='linear')(action_input)\n",
    "        a1 = Dense(32, activation='linear')(action_input)\n",
    "        h2 = concatenate([x2, a1], axis=-1)\n",
    "        #h2 = Add()([x2, a1])\n",
    "        h3 = Dense(16, activation='relu')(h2)\n",
    "        q_output = Dense(1, activation='linear')(h3)\n",
    "        model = Model([state_input, action_input], q_output)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "\n",
    "    ## q-value prediction of target critic\n",
    "    def target_predict(self, inp):\n",
    "        return self.target_model.predict(inp)\n",
    "\n",
    "\n",
    "    ## transfer critic weights to target critic with a aau\n",
    "    def update_target_network(self):\n",
    "        phi = self.model.get_weights()\n",
    "        target_phi = self.target_model.get_weights()\n",
    "        for i in range(len(phi)):\n",
    "            target_phi[i] = self.tau * phi[i] + (1 - self.tau) * target_phi[i]\n",
    "        self.target_model.set_weights(target_phi)\n",
    "\n",
    "\n",
    "    ## gradient of q-values wrt actions\n",
    "    def dq_da(self, states, actions):\n",
    "        a = tf.convert_to_tensor(actions)\n",
    "        with tf.GradientTape() as tape:\n",
    "            # compute dq_da to feed to the actor\n",
    "            tape.watch(a)\n",
    "            q = self.model([states, a])\n",
    "            q = tf.squeeze(q)\n",
    "        q_grads = tape.gradient(q, a)\n",
    "        return q_grads\n",
    "\n",
    "    ## single gradient update on a single batch data\n",
    "    def train_on_batch(self, states, actions, td_targets):\n",
    "        return self.model.train_on_batch([states, actions], td_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"\n",
    "    Reply Buffer\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = deque()\n",
    "        self.count = 0\n",
    "\n",
    "    ## save to buffer\n",
    "    def add_buffer(self, state, action, reward, next_state, done):\n",
    "        transition = (state, action, reward, next_state, done)\n",
    "\n",
    "        # check if buffer is full\n",
    "        if self.count < self.buffer_size:\n",
    "            self.buffer.append(transition)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(transition)\n",
    "\n",
    "    ## sample a batch\n",
    "    def sample_batch(self, batch_size):\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "        # return a batch of transitions\n",
    "        states = np.asarray([i[0] for i in batch])\n",
    "        actions = np.asarray([i[1] for i in batch])\n",
    "        rewards = np.asarray([i[2] for i in batch])\n",
    "        next_states = np.asarray([i[3] for i in batch])\n",
    "        dones = np.asarray([i[4] for i in batch])\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "\n",
    "    ## Current buffer occupation\n",
    "    def buffer_size(self):\n",
    "        return self.count\n",
    "\n",
    "    ## Clear buffer\n",
    "    def clear_buffer(self):\n",
    "        self.buffer = deque()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGagent(object):\n",
    "\n",
    "    def __init__(self, env):\n",
    "\n",
    "        ## hyperparameters\n",
    "        self.GAMMA = 0.95\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.BUFFER_SIZE = 20000\n",
    "        self.ACTOR_LEARNING_RATE = 0.0001\n",
    "        self.CRITIC_LEARNING_RATE = 0.001\n",
    "        self.TAU = 0.001\n",
    "\n",
    "        self.env = env\n",
    "        # get state dimension\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        # get action dimension\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        # get action bound\n",
    "        self.action_bound = env.action_space.high[0]\n",
    "\n",
    "        ## create actor and critic networks\n",
    "        self.actor = Actor(self.state_dim,\n",
    "                           self.action_dim, self.action_bound, self.TAU, self.ACTOR_LEARNING_RATE)\n",
    "        self.critic = Critic(self.state_dim, self.action_dim, self.TAU, self.CRITIC_LEARNING_RATE)\n",
    "\n",
    "        ## initialize replay buffer\n",
    "        self.buffer = ReplayBuffer(self.BUFFER_SIZE)\n",
    "\n",
    "        # save the results\n",
    "        self.save_epi_reward = []\n",
    "\n",
    "    ## Ornstein Uhlenbeck Noise\n",
    "    def ou_noise(self, x, rho=0.15, mu=0, dt=1e-1, sigma=0.2, dim=1):\n",
    "        return x + rho*(mu - x)*dt + sigma*np.sqrt(dt)*np.random.normal(size=dim)\n",
    "\n",
    "    ## computing TD target: y_k = r_k + gamma*Q(s_k+1, a_k+1)\n",
    "    def td_target(self, rewards, q_values, dones):\n",
    "        y_k = np.asarray(q_values)\n",
    "        for i in range(q_values.shape[0]): # number of batch\n",
    "            if dones[i]:\n",
    "                y_k[i] = rewards[i]\n",
    "            else:\n",
    "                y_k[i] = rewards[i] + self.GAMMA * q_values[i]\n",
    "        return y_k\n",
    "\n",
    "\n",
    "    ## train the agent\n",
    "    def train(self, max_episode_num):\n",
    "\n",
    "        # initial transfer model weights to target model network\n",
    "        self.actor.update_target_network()\n",
    "        self.critic.update_target_network()\n",
    "\n",
    "        for ep in range(int(max_episode_num)):\n",
    "            # reset OU noise\n",
    "            pre_noise = np.zeros(self.action_dim)\n",
    "            # reset episode\n",
    "            time, episode_reward, done = 0, 0, False\n",
    "            # reset the environment and observe the first state\n",
    "            state = self.env.reset()\n",
    "            while not done:\n",
    "                # visualize the environment\n",
    "                #self.env.render()\n",
    "                # pick an action: shape = (1,)\n",
    "                action = self.actor.predict(state)\n",
    "                noise = self.ou_noise(pre_noise, dim=self.action_dim)\n",
    "                # clip continuous action to be within action_bound\n",
    "                action = np.clip(action + noise, -self.action_bound, self.action_bound)\n",
    "                # observe reward, new_state\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                # add transition to replay buffer\n",
    "                train_reward = (reward + 8) / 8\n",
    "                self.buffer.add_buffer(state, action, train_reward, next_state, done)\n",
    "\n",
    "                if self.buffer.buffer_size > 1000:  # start train after buffer has some amounts\n",
    "\n",
    "                    # sample transitions from replay buffer\n",
    "                    states, actions, rewards, next_states, dones = self.buffer.sample_batch(self.BATCH_SIZE)\n",
    "                    # predict target Q-values\n",
    "                    target_qs = self.critic.target_predict([next_states, self.actor.target_predict(next_states)])\n",
    "                    # compute TD targets\n",
    "                    y_i = self.td_target(rewards, target_qs, dones)\n",
    "                    # train critic using sampled batch\n",
    "                    self.critic.train_on_batch(states, actions, y_i)\n",
    "                    # Q gradient wrt current policy\n",
    "                    s_actions = self.actor.model.predict(states) # shape=(batch, 1),\n",
    "                    # caution: NOT self.actor.predict !\n",
    "                    # self.actor.model.predict(state) -> shape=(1,1)\n",
    "                    # self.actor.predict(state) -> shape=(1,) -> type of gym action\n",
    "                    s_grads = self.critic.dq_da(states, s_actions)\n",
    "                    dq_das = np.array(s_grads).reshape((-1, self.action_dim))\n",
    "                    # train actor\n",
    "                    self.actor.train(states, dq_das)\n",
    "                    # update both target network\n",
    "                    self.actor.update_target_network()\n",
    "                    self.critic.update_target_network()\n",
    "\n",
    "                # update current state\n",
    "                pre_noise = noise\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                time += 1\n",
    "\n",
    "            ## display rewards every episode\n",
    "            print('Episode: ', ep+1, 'Time: ', time, 'Reward: ', episode_reward)\n",
    "\n",
    "            self.save_epi_reward.append(episode_reward)\n",
    "        print(self.save_epi_reward)\n",
    "\n",
    "\n",
    "    ## save them to file if done\n",
    "    def plot_result(self):\n",
    "        plt.plot(self.save_epi_reward)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\박정은\\gym\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 2,881\n",
      "Trainable params: 2,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 2,881\n",
      "Trainable params: 2,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           256         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 32)           2080        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 32)           64          input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64)           0           dense_9[0][0]                    \n",
      "                                                                 dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           1040        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            17          dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,457\n",
      "Trainable params: 3,457\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 64)           256         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 32)           2080        dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 32)           64          input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64)           0           dense_14[0][0]                   \n",
      "                                                                 dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 16)           1040        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1)            17          dense_16[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,457\n",
      "Trainable params: 3,457\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Layer dense_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Episode:  1 Time:  200 Reward:  -1856.8822567150767\n",
      "Episode:  2 Time:  200 Reward:  -1375.2171243693315\n",
      "Episode:  3 Time:  200 Reward:  -1707.7289477894415\n",
      "Episode:  4 Time:  200 Reward:  -1677.7345805715345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  5 Time:  200 Reward:  -1587.1932377853261\n",
      "Episode:  6 Time:  200 Reward:  -1626.4720190967291\n",
      "Episode:  7 Time:  200 Reward:  -1584.0178315900923\n",
      "Episode:  8 Time:  200 Reward:  -1582.8789971801123\n",
      "Episode:  9 Time:  200 Reward:  -1539.4173548007973\n",
      "Episode:  10 Time:  200 Reward:  -1521.884476283167\n",
      "Episode:  11 Time:  200 Reward:  -1502.2266541680017\n",
      "Episode:  12 Time:  200 Reward:  -1576.1319397421587\n",
      "Episode:  13 Time:  200 Reward:  -1522.585600914639\n",
      "Episode:  14 Time:  200 Reward:  -1431.1348188891204\n",
      "Episode:  15 Time:  200 Reward:  -1564.1871729411055\n",
      "Episode:  16 Time:  200 Reward:  -1493.0111305240432\n",
      "Episode:  17 Time:  200 Reward:  -1519.554739423586\n",
      "Episode:  18 Time:  200 Reward:  -1556.2142488555114\n",
      "Episode:  19 Time:  200 Reward:  -1498.102078869386\n",
      "Episode:  20 Time:  200 Reward:  -1453.945119481014\n",
      "Episode:  21 Time:  200 Reward:  -1533.1597805871293\n",
      "Episode:  22 Time:  200 Reward:  -1436.5016790465243\n",
      "Episode:  23 Time:  200 Reward:  -1539.0100602240902\n",
      "Episode:  24 Time:  200 Reward:  -1504.8357650411624\n",
      "Episode:  25 Time:  200 Reward:  -1412.1382183342364\n",
      "Episode:  26 Time:  200 Reward:  -1590.4432575191736\n",
      "Episode:  27 Time:  200 Reward:  -1534.9580883037042\n",
      "Episode:  28 Time:  200 Reward:  -1499.2561368467839\n",
      "Episode:  29 Time:  200 Reward:  -1527.1654094602914\n",
      "Episode:  30 Time:  200 Reward:  -1404.4199117656233\n",
      "Episode:  31 Time:  200 Reward:  -1473.3001890427681\n",
      "Episode:  32 Time:  200 Reward:  -1467.6909447179553\n",
      "Episode:  33 Time:  200 Reward:  -1300.751549066172\n",
      "Episode:  34 Time:  200 Reward:  -1131.0600838374035\n",
      "Episode:  35 Time:  200 Reward:  -1140.7415545715703\n",
      "Episode:  36 Time:  200 Reward:  -1193.7711587942497\n",
      "Episode:  37 Time:  200 Reward:  -807.1043797432408\n",
      "Episode:  38 Time:  200 Reward:  -701.7909901127553\n",
      "Episode:  39 Time:  200 Reward:  -846.1604851127203\n",
      "Episode:  40 Time:  200 Reward:  -798.3513878143767\n",
      "Episode:  41 Time:  200 Reward:  -801.0065296716799\n",
      "Episode:  42 Time:  200 Reward:  -673.56490926665\n",
      "Episode:  43 Time:  200 Reward:  -953.3200198397197\n",
      "Episode:  44 Time:  200 Reward:  -909.7723887870604\n",
      "Episode:  45 Time:  200 Reward:  -1157.354135367544\n",
      "Episode:  46 Time:  200 Reward:  -1145.0133493905957\n"
     ]
    }
   ],
   "source": [
    "max_episode_num = 200\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "agent = DDPGagent(env)\n",
    "\n",
    "agent.train(max_episode_num)\n",
    "\n",
    "agent.plot_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
