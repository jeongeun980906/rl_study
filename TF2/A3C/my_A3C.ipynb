{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import threading\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network1(state_dim,action_dim,action_bound):\n",
    "    state_input=Input((state_dim,))\n",
    "    h1=Dense(64,activation='relu')(state_input)\n",
    "    h2=Dense(32,activation='relu')(h1)\n",
    "    h3=Dense(16,activation='relu')(h2)\n",
    "    out_mu=Dense(action_dim,activation='tanh')(h3)\n",
    "    std_output=Dense(action_dim,activation='softplus')(h3)\n",
    "    mu_output=Lambda(lambda x: x*action_bound)(out_mu)\n",
    "    \n",
    "    model=Model(state_input,[mu_output,std_output])\n",
    "    #model.summary()\n",
    "    model._make_predict_function()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalActor(object):\n",
    "    def __init__(self,state_dim,action_dim,action_bound,learning_rate,entropy_beta):\n",
    "        self.state_dim=state_dim\n",
    "        self.action_dim=action_dim\n",
    "        self.action_bound=action_bound\n",
    "        self.learning_rate=learning_rate\n",
    "        self.entropy_beta=entropy_beta\n",
    "        self.std_bound=[1e-2,1]\n",
    "        self.model=build_network1(self.state_dim,self.action_dim,self.action_bound)\n",
    "        self.actor_optimizer=tf.keras.optimizers.Adam(self.learning_rate)\n",
    "    #log policy pdf    \n",
    "    def log_pdf(self,mu,std,action):\n",
    "        std=tf.clip_by_value(std,self.std_bound[0],self.std_bound[1])\n",
    "        var=std**2\n",
    "        log_policy_pdf=-0.5*(action-mu)**2/var-0.5*tf.math.log(var*2*np.pi)\n",
    "        entropy=0.5*(tf.math.log(2*np.pi*std**2)+1.0)\n",
    "        return tf.reduce_sum(log_policy_pdf,1,keepdims=True),tf.reduce_sum(entropy,1,keepdims=True)\n",
    "    \n",
    "    def train(self,states,actions,advantages):\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu_a,std_a=self.model(states)\n",
    "            log_policy_pdf,entropy=self.log_pdf(mu_a,std_a,actions)\n",
    "            loss_policy=log_policy_pdf*advantages\n",
    "            loss=tf.reduce_sum(-loss_policy-self.entropy_beta*entropy)\n",
    "        dj_dtheta=tape.gradient(loss,self.model.trainable_variables)\n",
    "        dj_dtheta,_=tf.clip_by_global_norm(dj_dtheta,40)\n",
    "        \n",
    "        grads=zip(dj_dtheta,self.model.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(grads)\n",
    "        \n",
    "    def prdict(self,state):\n",
    "        mu_a,_=self.model.predict(np.reshape(state,[1,self.state_dim]))\n",
    "        return mu_a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkerActor(object):\n",
    "    def __init__(self,state_dim,action_dim,action_bound):\n",
    "        self.state_dim=state_dim\n",
    "        self.action_dim=action_dim\n",
    "        self.action_bound=action_bound\n",
    "        self.std_bound=[1e-2,1]\n",
    "        self.model=build_network1(self.state_dim,self.action_dim,self.action_bound)\n",
    "        \n",
    "    def get_action(self,state):\n",
    "        mu_a,std_a=self.model.predict(np.reshape(state,[1,self.state_dim]))\n",
    "        mu_a=mu_a[0]\n",
    "        std_a=std_a[0]\n",
    "        std_a=np.clip(std_a,self.std_bound[0],self.std_bound[1])\n",
    "        action=np.random.normal(mu_a,std_a,size=self.action_dim)\n",
    "        return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network2(state_dim):\n",
    "    state_input=Input((state_dim,))\n",
    "    h1=Dense(64,activation='relu')(state_input)\n",
    "    h2=Dense(32,activation='relu')(h1)\n",
    "    h3=Dense(16,activation='relu')(h2)\n",
    "    v_output=Dense(1,activation='linear')(h3)\n",
    "    model=Model(state_input,v_output)\n",
    "    #model.summary()\n",
    "    model._make_predict_function()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalCritic(object):\n",
    "    \n",
    "    def __init__(self,state_dim,action_dim,learning_rate):\n",
    "        self.state_dim=state_dim\n",
    "        self.action_dim=action_dim\n",
    "        self.learning_rate=learning_rate\n",
    "        self.model=build_network2(state_dim)\n",
    "        self.critic_optimizer=tf.keras.optimizers.Adam(self.learning_rate)\n",
    "        \n",
    "    def train(self,states,td_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_values=self.model(states)\n",
    "            loss=tf.reduce_sum(tf.square(td_targets-v_values))\n",
    "        dj_dphi=tape.gradient(loss,self.model.trainable_variables)\n",
    "        dj_dphi,_=tf.clip_by_global_norm(dj_dphi,40)\n",
    "        \n",
    "        grads=zip(dj_dphi,self.model.trainable_variables)\n",
    "        \n",
    "        self.critic_optimizer.apply_gradients(grads)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkerCritic(object):\n",
    "    def __init__(self,state_dim):\n",
    "        self.model=build_network2(state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_episode_count = 0\n",
    "global_step = 0\n",
    "global_episode_reward = []  # save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3Cagent(object):\n",
    "    def __init__(self,env_name):\n",
    "        self.env_name=env_name\n",
    "        self.WORKERS_NUM=multiprocessing.cpu_count() #4\n",
    "        self.ACTOR_LEARNING_RATE = 0.0001\n",
    "        self.CRITIC_LEARNING_RATE = 0.001\n",
    "        self.ENTROPY_BETA = 0.01\n",
    "        \n",
    "        env = gym.make(self.env_name)\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        # get action dimension\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        # get action bound\n",
    "        action_bound = env.action_space.high[0]\n",
    "        \n",
    "        self.global_actor = GlobalActor(state_dim, action_dim, action_bound, self.ACTOR_LEARNING_RATE,\n",
    "                                         self.ENTROPY_BETA)\n",
    "        self.global_critic = GlobalCritic(state_dim, action_dim, self.CRITIC_LEARNING_RATE)\n",
    "\n",
    "    def train(self,max_episode_num):\n",
    "        workers=[]\n",
    "        for i in range(self.WORKERS_NUM):\n",
    "            worker_name='worker%i' %i\n",
    "            workers.append(A3Cworker(worker_name,self.env_name,self.global_actor,\n",
    "                                    self.global_critic,max_episode_num))\n",
    "        for worker in workers:\n",
    "            worker.start()\n",
    "        for worker in workers:\n",
    "            worker.join()\n",
    "        print(global_episode_reward)\n",
    "    def plot_result(self):\n",
    "        plt.plot(global_episode_reward)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3Cworker(threading.Thread):\n",
    "    def __init__(self,worker_name,env_name,global_actor,global_critic,max_episode_num):\n",
    "        threading.Thread.__init__(self)\n",
    "        \n",
    "        self.GAMMA=0.95\n",
    "        self.t_MAX=4\n",
    "        self.max_episode_num=max_episode_num\n",
    "        \n",
    "        self.env=gym.make(env_name)\n",
    "        self.worker_name=worker_name\n",
    "        \n",
    "        self.global_actor = global_actor\n",
    "        self.global_critic = global_critic\n",
    "        # get state dimension\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        # get action dimension\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        # get action bound\n",
    "        self.action_bound = self.env.action_space.high[0]\n",
    "\n",
    "        # create local actor and critic networks\n",
    "        self.worker_actor = WorkerActor(self.state_dim, self.action_dim, self.action_bound)\n",
    "        self.worker_critic = WorkerCritic(self.state_dim)\n",
    "        \n",
    "        self.worker_actor.model.set_weights(self.global_actor.model.get_weights())\n",
    "        self.worker_critic.model.set_weights(self.global_critic.model.get_weights())\n",
    "        \n",
    "    def n_step_td_target(self,rewards,next_v_value,done):\n",
    "        td_targets=np.zeros_like(rewards)\n",
    "        cumulative=0\n",
    "        if not done:\n",
    "            cumulative=next_v_value\n",
    "        for k in reversed(range(0,len(rewards))):\n",
    "            cumulative=self.GAMMA*cumulative+rewards[k]\n",
    "            td_targets[k]=cumulative\n",
    "        return td_targets\n",
    "    \n",
    "    def unpack_batch(self,batch):\n",
    "        unpack=batch[0]\n",
    "        for idx in range(len(batch)-1):\n",
    "            unpack=np.append(unpack,batch[idx+1],axis=0)\n",
    "        return unpack\n",
    "    \n",
    "    def run(self):\n",
    "        global global_episode_count, global_step\n",
    "        global global_episode_reward  # total episode across all workers\n",
    "\n",
    "        print(self.worker_name, \"starts ---\")\n",
    "        \n",
    "        while global_episode_count<=int(self.max_episode_num):\n",
    "            batch_state,batch_action,batch_reward=[],[],[]\n",
    "            step,episode_reward,done=0,0,False\n",
    "            state=self.env.reset()\n",
    "            \n",
    "            while not done:\n",
    "                #self.env.render()\n",
    "                action=self.worker_actor.get_action(state)\n",
    "                action=np.clip(action,-self.action_bound,self.action_bound)\n",
    "                next_state,reward,done,_=self.env.step(action)\n",
    "                state=np.reshape(state,[1,self.state_dim])\n",
    "                reward=np.reshape(reward,[1,1])\n",
    "                action=np.reshape(action,[1,self.action_dim])\n",
    "                \n",
    "                batch_state.append(state)\n",
    "                batch_action.append(action)\n",
    "                batch_reward.append((reward+8)/8)\n",
    "                \n",
    "                state=next_state\n",
    "                step+=1\n",
    "                episode_reward+=reward[0]\n",
    "                \n",
    "                if len(batch_state)==self.t_MAX or done:\n",
    "                    states=self.unpack_batch(batch_state)\n",
    "                    actions=self.unpack_batch(batch_action)\n",
    "                    rewards=self.unpack_batch(batch_reward)\n",
    "                    \n",
    "                    batch_state, batch_action, batch_reward = [], [], []\n",
    "                    \n",
    "                    next_state=np.reshape(next_state,[1,self.state_dim])\n",
    "                    next_v_value=self.global_critic.model.predict(next_state)\n",
    "                    n_step_td_targets=self.n_step_td_target(rewards,next_v_value,done)\n",
    "                    v_values=self.global_critic.model.predict(states)\n",
    "                    advantages=n_step_td_targets-v_values\n",
    "                    \n",
    "                    self.global_critic.train(states,n_step_td_targets)\n",
    "                    self.global_actor.train(states,actions,advantages)\n",
    "                    \n",
    "                    self.worker_actor.model.set_weights(self.global_actor.model.get_weights())\n",
    "                    self.worker_critic.model.set_weights(self.global_critic.model.get_weights())\n",
    "                    \n",
    "                    global_step+=1\n",
    "                    \n",
    "                if done:\n",
    "                    global_episode_count+=1\n",
    "                    print('Worker name:', self.worker_name, ', Episode: ', global_episode_count,\n",
    "                          ', Step: ', step, ', Reward: ', episode_reward)\n",
    "\n",
    "                    global_episode_reward.append(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\박정은\\gym\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker0worker1worker2   worker3 starts ---starts ---starts ---\n",
      "\n",
      "\n",
      "starts ---\n",
      "WARNING:tensorflow:Layer dense_150 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_150 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_150 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_150 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_145 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_145 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_145 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_145 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Worker name: worker0 , Episode:  1 , Step:  200 , Reward:  [-1362.88976008]\n",
      "Worker name: worker3 , Episode:  2 , Step:  200 , Reward:  [-1469.26175527]\n",
      "Worker name: worker2 , Episode:  3 , Step:  200 , Reward:  [-1470.19464235]\n",
      "Worker name: worker1 , Episode:  4 , Step:  200 , Reward:  [-1338.86362985]\n",
      "Worker name: worker0 , Episode:  5 , Step:  200 , Reward:  [-1436.20587272]\n",
      "Worker name: worker3 , Episode:  6 , Step:  200 , Reward:  [-1392.1931577]\n",
      "Worker name: worker2 , Episode:  7 , Step:  200 , Reward:  [-1316.70224114]\n",
      "Worker name: worker1 , Episode:  8 , Step:  200 , Reward:  [-1230.29845311]\n",
      "Worker name: worker0 , Episode:  9 , Step:  200 , Reward:  [-1305.70176936]\n",
      "Worker name: worker2 , Episode:  10 , Step:  200 , Reward:  [-1241.72477885]\n",
      "Worker name: worker3 , Episode:  11 , Step:  200 , Reward:  [-1204.03935528]\n",
      "Worker name: worker1 , Episode:  12 , Step:  200 , Reward:  [-1311.08595462]\n",
      "Worker name: worker0 , Episode:  13 , Step:  200 , Reward:  [-1321.27359143]\n",
      "Worker name: worker2 , Episode:  14 , Step:  200 , Reward:  [-1291.93909678]\n",
      "Worker name: worker3 , Episode:  15 , Step:  200 , Reward:  [-1234.08689028]\n",
      "Worker name: worker1 , Episode:  16 , Step:  200 , Reward:  [-1327.3133367]\n",
      "Worker name: worker0 , Episode:  17 , Step:  200 , Reward:  [-1359.71535497]\n",
      "Worker name: worker2 , Episode:  18 , Step:  200 , Reward:  [-1254.73047876]\n",
      "Worker name: worker3 , Episode:  19 , Step:  200 , Reward:  [-1362.20322077]\n",
      "Worker name: worker1 , Episode:  20 , Step:  200 , Reward:  [-1140.27951701]\n",
      "Worker name: worker0 , Episode:  21 , Step:  200 , Reward:  [-1309.85664756]\n",
      "Worker name: worker2 , Episode:  22 , Step:  200 , Reward:  [-1133.88342628]\n",
      "Worker name: worker3 , Episode:  23 , Step:  200 , Reward:  [-1309.38953003]\n",
      "Worker name: worker1 , Episode:  24 , Step:  200 , Reward:  [-1284.50595338]\n",
      "Worker name: worker0 , Episode:  25 , Step:  200 , Reward:  [-903.97187945]\n",
      "Worker name: worker2 , Episode:  26 , Step:  200 , Reward:  [-1163.23465239]\n",
      "Worker name: worker3 , Episode:  27 , Step:  200 , Reward:  [-1323.62504242]\n",
      "Worker name: worker1 , Episode:  28 , Step:  200 , Reward:  [-932.23694513]\n",
      "Worker name: worker0 , Episode:  29 , Step:  200 , Reward:  [-1010.93079102]\n",
      "Worker name: worker2 , Episode:  30 , Step:  200 , Reward:  [-1038.4839418]\n",
      "Worker name: worker3 , Episode:  31 , Step:  200 , Reward:  [-1044.18559059]\n",
      "Worker name: worker1 , Episode:  32 , Step:  200 , Reward:  [-947.64871512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker name: worker0 , Episode:  33 , Step:  200 , Reward:  [-906.2568021]\n",
      "Worker name: worker2 , Episode:  34 , Step:  200 , Reward:  [-1409.22323095]\n",
      "Worker name: worker3 , Episode:  35 , Step:  200 , Reward:  [-1698.46008145]\n",
      "Worker name: worker1 , Episode:  36 , Step:  200 , Reward:  [-1030.67479638]\n",
      "Worker name: worker0 , Episode:  37 , Step:  200 , Reward:  [-1004.57159363]\n",
      "Worker name: worker2 , Episode:  38 , Step:  200 , Reward:  [-1050.71747723]\n",
      "Worker name: worker3 , Episode:  39 , Step:  200 , Reward:  [-1036.47069963]\n",
      "Worker name: worker1 , Episode:  40 , Step:  200 , Reward:  [-1016.49180005]\n",
      "Worker name: worker0 , Episode:  41 , Step:  200 , Reward:  [-1348.6196252]\n",
      "Worker name: worker2 , Episode:  42 , Step:  200 , Reward:  [-967.73524847]\n",
      "Worker name: worker3 , Episode:  43 , Step:  200 , Reward:  [-967.02725715]\n",
      "Worker name: worker1 , Episode:  44 , Step:  200 , Reward:  [-1048.38728281]\n",
      "Worker name: worker0 , Episode:  45 , Step:  200 , Reward:  [-1313.64106649]\n",
      "Worker name: worker2 , Episode:  46 , Step:  200 , Reward:  [-1005.29657199]\n",
      "Worker name: worker3 , Episode:  47 , Step:  200 , Reward:  [-986.46951409]\n",
      "Worker name: worker1 , Episode:  48 , Step:  200 , Reward:  [-1197.63842668]\n",
      "Worker name: worker0 , Episode:  49 , Step:  200 , Reward:  [-1118.69590548]\n",
      "Worker name: worker2 , Episode:  50 , Step:  200 , Reward:  [-1132.70234821]\n",
      "Worker name: worker3 , Episode:  51 , Step:  200 , Reward:  [-1250.31926388]\n",
      "Worker name: worker1 , Episode:  52 , Step:  200 , Reward:  [-1011.95987395]\n",
      "Worker name: worker0 , Episode:  53 , Step:  200 , Reward:  [-1284.74868861]\n",
      "Worker name: worker2 , Episode:  54 , Step:  200 , Reward:  [-1145.49666416]\n",
      "Worker name: worker3 , Episode:  55 , Step:  200 , Reward:  [-1373.40983644]\n",
      "Worker name: worker1 , Episode:  56 , Step:  200 , Reward:  [-1381.80353509]\n",
      "Worker name: worker0 , Episode:  57 , Step:  200 , Reward:  [-927.41810465]\n",
      "Worker name: worker2 , Episode:  58 , Step:  200 , Reward:  [-1573.50572798]\n",
      "Worker name: worker3 , Episode:  59 , Step:  200 , Reward:  [-1262.69044345]\n",
      "Worker name: worker1 , Episode:  60 , Step:  200 , Reward:  [-1313.25811933]\n",
      "Worker name: worker0 , Episode:  61 , Step:  200 , Reward:  [-1058.56644452]\n",
      "Worker name: worker2 , Episode:  62 , Step:  200 , Reward:  [-1289.61154672]\n",
      "Worker name: worker3 , Episode:  63 , Step:  200 , Reward:  [-1458.45504912]\n",
      "Worker name: worker1 , Episode:  64 , Step:  200 , Reward:  [-1055.83540278]\n",
      "Worker name: worker0 , Episode:  65 , Step:  200 , Reward:  [-1040.76602432]\n",
      "Worker name: worker2 , Episode:  66 , Step:  200 , Reward:  [-1130.58071159]\n",
      "Worker name: worker3 , Episode:  67 , Step:  200 , Reward:  [-1197.51385482]\n",
      "Worker name: worker1 , Episode:  68 , Step:  200 , Reward:  [-1105.5339918]\n",
      "Worker name: worker0 , Episode:  69 , Step:  200 , Reward:  [-892.22792463]\n",
      "Worker name: worker2 , Episode:  70 , Step:  200 , Reward:  [-893.26727844]\n",
      "Worker name: worker3 , Episode:  71 , Step:  200 , Reward:  [-965.21855864]\n",
      "Worker name: worker1 , Episode:  72 , Step:  200 , Reward:  [-943.53492387]\n",
      "Worker name: worker0 , Episode:  73 , Step:  200 , Reward:  [-1340.02418866]\n",
      "Worker name: worker2 , Episode:  74 , Step:  200 , Reward:  [-1111.19310961]\n",
      "Worker name: worker3 , Episode:  75 , Step:  200 , Reward:  [-1015.34346162]\n",
      "Worker name: worker1 , Episode:  76 , Step:  200 , Reward:  [-1262.45439026]\n",
      "Worker name: worker0 , Episode:  77 , Step:  200 , Reward:  [-1021.36634055]\n",
      "Worker name: worker2 , Episode:  78 , Step:  200 , Reward:  [-1001.51192536]\n",
      "Worker name: worker3 , Episode:  79 , Step:  200 , Reward:  [-1262.75170677]\n",
      "Worker name: worker1 , Episode:  80 , Step:  200 , Reward:  [-1271.96328248]\n",
      "Worker name: worker0 , Episode:  81 , Step:  200 , Reward:  [-1185.0185692]\n",
      "Worker name: worker2 , Episode:  82 , Step:  200 , Reward:  [-1359.69274932]\n",
      "Worker name: worker3 , Episode:  83 , Step:  200 , Reward:  [-1059.06313458]\n",
      "Worker name: worker1 , Episode:  84 , Step:  200 , Reward:  [-1008.906126]\n",
      "Worker name: worker0 , Episode:  85 , Step:  200 , Reward:  [-1035.17575987]\n",
      "Worker name: worker2 , Episode:  86 , Step:  200 , Reward:  [-1374.58312228]\n",
      "Worker name: worker3 , Episode:  87 , Step:  200 , Reward:  [-887.9943678]\n",
      "Worker name: worker1 , Episode:  88 , Step:  200 , Reward:  [-1030.23468615]\n",
      "Worker name: worker0 , Episode:  89 , Step:  200 , Reward:  [-1085.44776811]\n",
      "Worker name: worker2 , Episode:  90 , Step:  200 , Reward:  [-773.66236932]\n",
      "Worker name: worker3 , Episode:  91 , Step:  200 , Reward:  [-1328.38229762]\n",
      "Worker name: worker1 , Episode:  92 , Step:  200 , Reward:  [-911.34553998]\n",
      "Worker name: worker0 , Episode:  93 , Step:  200 , Reward:  [-1109.93395539]\n",
      "Worker name: worker2 , Episode:  94 , Step:  200 , Reward:  [-886.48128434]\n",
      "Worker name: worker3 , Episode:  95 , Step:  200 , Reward:  [-1347.31234495]\n",
      "Worker name: worker1 , Episode:  96 , Step:  200 , Reward:  [-1137.21277187]\n",
      "Worker name: worker0 , Episode:  97 , Step:  200 , Reward:  [-1144.62984159]\n",
      "Worker name: worker2 , Episode:  98 , Step:  200 , Reward:  [-993.55209229]\n",
      "Worker name: worker3 , Episode:  99 , Step:  200 , Reward:  [-777.61349649]\n",
      "Worker name: worker1 , Episode:  100 , Step:  200 , Reward:  [-1025.96929839]\n",
      "Worker name: worker0 , Episode:  101 , Step:  200 , Reward:  [-1039.50066841]\n",
      "Worker name: worker2 , Episode:  102 , Step:  200 , Reward:  [-1035.35790805]\n",
      "Worker name: worker3 , Episode:  103 , Step:  200 , Reward:  [-1047.48792562]\n",
      "Worker name: worker1 , Episode:  104 , Step:  200 , Reward:  [-860.44298517]\n",
      "Worker name: worker0 , Episode:  105 , Step:  200 , Reward:  [-879.78212443]\n",
      "Worker name: worker2 , Episode:  106 , Step:  200 , Reward:  [-994.720015]\n",
      "Worker name: worker3 , Episode:  107 , Step:  200 , Reward:  [-1120.960016]\n",
      "Worker name: worker1 , Episode:  108 , Step:  200 , Reward:  [-1042.20938418]\n",
      "Worker name: worker0 , Episode:  109 , Step:  200 , Reward:  [-1106.17546595]\n",
      "Worker name: worker2 , Episode:  110 , Step:  200 , Reward:  [-999.45643171]\n",
      "Worker name: worker3 , Episode:  111 , Step:  200 , Reward:  [-820.48308995]\n",
      "Worker name: worker1 , Episode:  112 , Step:  200 , Reward:  [-751.02504899]\n",
      "Worker name: worker0 , Episode:  113 , Step:  200 , Reward:  [-936.02815311]\n",
      "Worker name: worker2 , Episode:  114 , Step:  200 , Reward:  [-1018.37659407]\n",
      "Worker name: worker3 , Episode:  115 , Step:  200 , Reward:  [-1130.60330853]\n",
      "Worker name: worker1 , Episode:  116 , Step:  200 , Reward:  [-1167.68127433]\n",
      "Worker name: worker0 , Episode:  117 , Step:  200 , Reward:  [-899.85163148]\n",
      "Worker name: worker2 , Episode:  118 , Step:  200 , Reward:  [-985.74132211]\n",
      "Worker name: worker3 , Episode:  119 , Step:  200 , Reward:  [-1015.50985093]\n",
      "Worker name: worker1 , Episode:  120 , Step:  200 , Reward:  [-895.54809118]\n",
      "Worker name: worker0 , Episode:  121 , Step:  200 , Reward:  [-914.96951822]\n",
      "Worker name: worker2 , Episode:  122 , Step:  200 , Reward:  [-924.73887048]\n",
      "Worker name: worker3 , Episode:  123 , Step:  200 , Reward:  [-991.86195462]\n",
      "Worker name: worker1 , Episode:  124 , Step:  200 , Reward:  [-1015.49591964]\n",
      "Worker name: worker0 , Episode:  125 , Step:  200 , Reward:  [-1161.54486756]\n",
      "Worker name: worker2 , Episode:  126 , Step:  200 , Reward:  [-990.08933995]\n",
      "Worker name: worker3 , Episode:  127 , Step:  200 , Reward:  [-861.66650408]\n",
      "Worker name: worker1 , Episode:  128 , Step:  200 , Reward:  [-778.13294328]\n",
      "Worker name: worker0 , Episode:  129 , Step:  200 , Reward:  [-1007.34947476]\n",
      "Worker name: worker2 , Episode:  130 , Step:  200 , Reward:  [-1013.79022275]\n",
      "Worker name: worker3 , Episode:  131 , Step:  200 , Reward:  [-896.97358213]\n",
      "Worker name: worker1 , Episode:  132 , Step:  200 , Reward:  [-1124.58551066]\n",
      "Worker name: worker0 , Episode:  133 , Step:  200 , Reward:  [-1270.65789401]\n",
      "Worker name: worker2 , Episode:  134 , Step:  200 , Reward:  [-1090.7692271]\n",
      "Worker name: worker3 , Episode:  135 , Step:  200 , Reward:  [-921.45439597]\n",
      "Worker name: worker1 , Episode:  136 , Step:  200 , Reward:  [-655.71067404]\n",
      "Worker name: worker0 , Episode:  137 , Step:  200 , Reward:  [-874.81525315]\n",
      "Worker name: worker2 , Episode:  138 , Step:  200 , Reward:  [-1078.90059739]\n",
      "Worker name: worker3 , Episode:  139 , Step:  200 , Reward:  [-1238.39877382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker name: worker0 , Episode:  140 , Step:  200 , Reward:  [-1137.45334303]\n",
      "Worker name: worker1 , Episode:  141 , Step:  200 , Reward:  [-1050.542744]\n",
      "Worker name: worker2 , Episode:  142 , Step:  200 , Reward:  [-1002.86834087]\n",
      "Worker name: worker3 , Episode:  143 , Step:  200 , Reward:  [-1252.50075003]\n",
      "Worker name: worker0 , Episode:  144 , Step:  200 , Reward:  [-893.00110259]\n",
      "Worker name: worker1 , Episode:  145 , Step:  200 , Reward:  [-1023.83026876]\n",
      "Worker name: worker2 , Episode:  146 , Step:  200 , Reward:  [-906.40322247]\n",
      "Worker name: worker3 , Episode:  147 , Step:  200 , Reward:  [-895.29734904]\n",
      "Worker name: worker0 , Episode:  148 , Step:  200 , Reward:  [-736.6230298]\n",
      "Worker name: worker1 , Episode:  149 , Step:  200 , Reward:  [-880.96390143]\n",
      "Worker name: worker2 , Episode:  150 , Step:  200 , Reward:  [-691.82653835]\n",
      "Worker name: worker0 , Episode:  151 , Step:  200 , Reward:  [-1124.77033188]\n",
      "Worker name: worker3 , Episode:  152 , Step:  200 , Reward:  [-764.97534536]\n",
      "Worker name: worker1 , Episode:  153 , Step:  200 , Reward:  [-900.01951379]\n",
      "Worker name: worker2 , Episode:  154 , Step:  200 , Reward:  [-864.88601821]\n",
      "Worker name: worker0 , Episode:  155 , Step:  200 , Reward:  [-1129.35610319]\n",
      "Worker name: worker3 , Episode:  156 , Step:  200 , Reward:  [-880.42710052]\n",
      "Worker name: worker1 , Episode:  157 , Step:  200 , Reward:  [-1001.41987496]\n",
      "Worker name: worker2 , Episode:  158 , Step:  200 , Reward:  [-732.5451018]\n",
      "Worker name: worker0 , Episode:  159 , Step:  200 , Reward:  [-1164.57626154]\n",
      "Worker name: worker3 , Episode:  160 , Step:  200 , Reward:  [-1014.20293637]\n",
      "Worker name: worker1 , Episode:  161 , Step:  200 , Reward:  [-877.94719397]\n",
      "Worker name: worker2 , Episode:  162 , Step:  200 , Reward:  [-916.01938145]\n",
      "Worker name: worker0 , Episode:  163 , Step:  200 , Reward:  [-762.9027899]\n",
      "Worker name: worker3 , Episode:  164 , Step:  200 , Reward:  [-993.48171972]\n",
      "Worker name: worker1 , Episode:  165 , Step:  200 , Reward:  [-761.49988491]\n",
      "Worker name: worker2 , Episode:  166 , Step:  200 , Reward:  [-819.56204651]\n",
      "Worker name: worker0 , Episode:  167 , Step:  200 , Reward:  [-998.55434313]\n",
      "Worker name: worker3 , Episode:  168 , Step:  200 , Reward:  [-1057.83106763]\n",
      "Worker name: worker1 , Episode:  169 , Step:  200 , Reward:  [-1164.49859498]\n",
      "Worker name: worker2 , Episode:  170 , Step:  200 , Reward:  [-1247.33569934]\n",
      "Worker name: worker0 , Episode:  171 , Step:  200 , Reward:  [-736.73918869]\n",
      "Worker name: worker3 , Episode:  172 , Step:  200 , Reward:  [-798.96826994]\n",
      "Worker name: worker1 , Episode:  173 , Step:  200 , Reward:  [-1174.07650182]\n",
      "Worker name: worker2 , Episode:  174 , Step:  200 , Reward:  [-772.26734031]\n",
      "Worker name: worker0 , Episode:  175 , Step:  200 , Reward:  [-919.47777267]\n",
      "Worker name: worker3 , Episode:  176 , Step:  200 , Reward:  [-899.45389616]\n",
      "Worker name: worker1 , Episode:  177 , Step:  200 , Reward:  [-1144.49276944]\n",
      "Worker name: worker2 , Episode:  178 , Step:  200 , Reward:  [-968.32291214]\n",
      "Worker name: worker0 , Episode:  179 , Step:  200 , Reward:  [-765.19163837]\n",
      "Worker name: worker3 , Episode:  180 , Step:  200 , Reward:  [-1154.84037887]\n",
      "Worker name: worker1 , Episode:  181 , Step:  200 , Reward:  [-909.2511411]\n",
      "Worker name: worker2 , Episode:  182 , Step:  200 , Reward:  [-631.35443346]\n",
      "Worker name: worker0 , Episode:  183 , Step:  200 , Reward:  [-985.09524909]\n",
      "Worker name: worker3 , Episode:  184 , Step:  200 , Reward:  [-862.33659614]\n",
      "Worker name: worker1 , Episode:  185 , Step:  200 , Reward:  [-754.08836813]\n",
      "Worker name: worker2 , Episode:  186 , Step:  200 , Reward:  [-739.98949016]\n",
      "Worker name: worker0 , Episode:  187 , Step:  200 , Reward:  [-878.81926255]\n",
      "Worker name: worker3 , Episode:  188 , Step:  200 , Reward:  [-1042.53120837]\n",
      "Worker name: worker1 , Episode:  189 , Step:  200 , Reward:  [-1002.46000156]\n",
      "Worker name: worker2 , Episode:  190 , Step:  200 , Reward:  [-1211.17021558]\n",
      "Worker name: worker0 , Episode:  191 , Step:  200 , Reward:  [-1012.20149771]\n",
      "Worker name: worker3 , Episode:  192 , Step:  200 , Reward:  [-908.3915352]\n",
      "Worker name: worker2 , Episode:  193 , Step:  200 , Reward:  [-894.26709758]\n",
      "Worker name: worker1 , Episode:  194 , Step:  200 , Reward:  [-972.87507538]\n",
      "Worker name: worker0 , Episode:  195 , Step:  200 , Reward:  [-805.6900083]\n",
      "Worker name: worker3 , Episode:  196 , Step:  200 , Reward:  [-997.41119646]\n",
      "Worker name: worker2 , Episode:  197 , Step:  200 , Reward:  [-690.92602535]\n",
      "Worker name: worker1 , Episode:  198 , Step:  200 , Reward:  [-1002.78028592]\n",
      "Worker name: worker0 , Episode:  199 , Step:  200 , Reward:  [-889.35101522]\n",
      "Worker name: worker3 , Episode:  200 , Step:  200 , Reward:  [-767.51515551]\n",
      "Worker name: worker2 , Episode:  201 , Step:  200 , Reward:  [-769.6520927]\n",
      "Worker name: worker1 , Episode:  202 , Step:  200 , Reward:  [-756.97917353]\n",
      "Worker name: worker0 , Episode:  203 , Step:  200 , Reward:  [-507.14264453]\n",
      "Worker name: worker3 , Episode:  204 , Step:  200 , Reward:  [-647.4455761]\n",
      "Worker name: worker2 , Episode:  205 , Step:  200 , Reward:  [-896.68657532]\n",
      "Worker name: worker1 , Episode:  206 , Step:  200 , Reward:  [-1219.34135179]\n",
      "Worker name: worker0 , Episode:  207 , Step:  200 , Reward:  [-1027.53624286]\n",
      "Worker name: worker3 , Episode:  208 , Step:  200 , Reward:  [-1019.53581847]\n",
      "Worker name: worker2 , Episode:  209 , Step:  200 , Reward:  [-1152.99962747]\n",
      "Worker name: worker1 , Episode:  210 , Step:  200 , Reward:  [-1027.01012965]\n",
      "Worker name: worker0 , Episode:  211 , Step:  200 , Reward:  [-638.79641959]\n",
      "Worker name: worker3 , Episode:  212 , Step:  200 , Reward:  [-888.04453474]\n",
      "Worker name: worker2 , Episode:  213 , Step:  200 , Reward:  [-880.39455668]\n",
      "Worker name: worker1 , Episode:  214 , Step:  200 , Reward:  [-637.4262375]\n",
      "Worker name: worker0 , Episode:  215 , Step:  200 , Reward:  [-642.78344558]\n",
      "Worker name: worker3 , Episode:  216 , Step:  200 , Reward:  [-797.08890565]\n",
      "Worker name: worker2 , Episode:  217 , Step:  200 , Reward:  [-1001.10162499]\n",
      "Worker name: worker1 , Episode:  218 , Step:  200 , Reward:  [-899.85679803]\n",
      "Worker name: worker0 , Episode:  219 , Step:  200 , Reward:  [-759.12348624]\n",
      "Worker name: worker3 , Episode:  220 , Step:  200 , Reward:  [-639.58741667]\n",
      "Worker name: worker2 , Episode:  221 , Step:  200 , Reward:  [-893.1365676]\n",
      "Worker name: worker1 , Episode:  222 , Step:  200 , Reward:  [-895.96198558]\n",
      "Worker name: worker0 , Episode:  223 , Step:  200 , Reward:  [-1068.87410313]\n",
      "Worker name: worker2 , Episode:  224 , Step:  200 , Reward:  [-704.89292341]\n",
      "Worker name: worker3 , Episode:  225 , Step:  200 , Reward:  [-761.15745778]\n",
      "Worker name: worker1 , Episode:  226 , Step:  200 , Reward:  [-982.02217632]\n",
      "Worker name: worker0 , Episode:  227 , Step:  200 , Reward:  [-989.4894099]\n",
      "Worker name: worker2 , Episode:  228 , Step:  200 , Reward:  [-1010.82190459]\n",
      "Worker name: worker3 , Episode:  229 , Step:  200 , Reward:  [-767.23626505]\n",
      "Worker name: worker1 , Episode:  230 , Step:  200 , Reward:  [-1006.98989377]\n",
      "Worker name: worker0 , Episode:  231 , Step:  200 , Reward:  [-1071.40603346]\n",
      "Worker name: worker2 , Episode:  232 , Step:  200 , Reward:  [-945.45359621]\n",
      "Worker name: worker3 , Episode:  233 , Step:  200 , Reward:  [-893.75662704]\n",
      "Worker name: worker1 , Episode:  234 , Step:  200 , Reward:  [-953.72036229]\n",
      "Worker name: worker0 , Episode:  235 , Step:  200 , Reward:  [-893.60240578]\n",
      "Worker name: worker2 , Episode:  236 , Step:  200 , Reward:  [-769.06680588]\n",
      "Worker name: worker3 , Episode:  237 , Step:  200 , Reward:  [-751.49887315]\n",
      "Worker name: worker1 , Episode:  238 , Step:  200 , Reward:  [-641.21881081]\n",
      "Worker name: worker0 , Episode:  239 , Step:  200 , Reward:  [-762.35041945]\n",
      "Worker name: worker2 , Episode:  240 , Step:  200 , Reward:  [-994.22850438]\n",
      "Worker name: worker3 , Episode:  241 , Step:  200 , Reward:  [-685.64325914]\n",
      "Worker name: worker1 , Episode:  242 , Step:  200 , Reward:  [-1038.29236339]\n",
      "Worker name: worker0 , Episode:  243 , Step:  200 , Reward:  [-900.94479721]\n",
      "Worker name: worker2 , Episode:  244 , Step:  200 , Reward:  [-526.36648481]\n",
      "Worker name: worker3 , Episode:  245 , Step:  200 , Reward:  [-998.70525033]\n",
      "Worker name: worker1 , Episode:  246 , Step:  200 , Reward:  [-752.90744568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker name: worker0 , Episode:  247 , Step:  200 , Reward:  [-883.68174886]\n",
      "Worker name: worker2 , Episode:  248 , Step:  200 , Reward:  [-819.66988999]\n",
      "Worker name: worker3 , Episode:  249 , Step:  200 , Reward:  [-640.31910477]\n",
      "Worker name: worker1 , Episode:  250 , Step:  200 , Reward:  [-829.17352427]\n",
      "Worker name: worker0 , Episode:  251 , Step:  200 , Reward:  [-761.78172893]\n",
      "Worker name: worker2 , Episode:  252 , Step:  200 , Reward:  [-1188.95938582]\n",
      "Worker name: worker3 , Episode:  253 , Step:  200 , Reward:  [-1005.28169052]\n",
      "Worker name: worker1 , Episode:  254 , Step:  200 , Reward:  [-762.09917703]\n",
      "Worker name: worker0 , Episode:  255 , Step:  200 , Reward:  [-879.63092583]\n",
      "Worker name: worker2 , Episode:  256 , Step:  200 , Reward:  [-1059.62039461]\n",
      "Worker name: worker3 , Episode:  257 , Step:  200 , Reward:  [-1027.24437982]\n",
      "Worker name: worker1 , Episode:  258 , Step:  200 , Reward:  [-1039.55237964]\n",
      "Worker name: worker0 , Episode:  259 , Step:  200 , Reward:  [-1138.41186116]\n",
      "Worker name: worker2 , Episode:  260 , Step:  200 , Reward:  [-911.23656213]\n",
      "Worker name: worker3 , Episode:  261 , Step:  200 , Reward:  [-1001.9449347]\n",
      "Worker name: worker1 , Episode:  262 , Step:  200 , Reward:  [-779.10572154]\n",
      "Worker name: worker0 , Episode:  263 , Step:  200 , Reward:  [-762.98909505]\n",
      "Worker name: worker2 , Episode:  264 , Step:  200 , Reward:  [-1174.19978183]\n",
      "Worker name: worker3 , Episode:  265 , Step:  200 , Reward:  [-1039.64158031]\n",
      "Worker name: worker1 , Episode:  266 , Step:  200 , Reward:  [-1026.15770027]\n",
      "Worker name: worker0 , Episode:  267 , Step:  200 , Reward:  [-1141.21125954]\n",
      "Worker name: worker2 , Episode:  268 , Step:  200 , Reward:  [-775.22937643]\n",
      "Worker name: worker3 , Episode:  269 , Step:  200 , Reward:  [-421.74012381]\n",
      "Worker name: worker1 , Episode:  270 , Step:  200 , Reward:  [-645.71386985]\n",
      "Worker name: worker0 , Episode:  271 , Step:  200 , Reward:  [-891.17043813]\n",
      "Worker name: worker2 , Episode:  272 , Step:  200 , Reward:  [-615.46964196]\n",
      "Worker name: worker3 , Episode:  273 , Step:  200 , Reward:  [-1030.09414724]\n",
      "Worker name: worker1 , Episode:  274 , Step:  200 , Reward:  [-636.36768939]\n",
      "Worker name: worker0 , Episode:  275 , Step:  200 , Reward:  [-1028.48774254]\n",
      "Worker name: worker2 , Episode:  276 , Step:  200 , Reward:  [-902.49818233]\n",
      "Worker name: worker3 , Episode:  277 , Step:  200 , Reward:  [-642.58331333]\n",
      "Worker name: worker0 , Episode:  278 , Step:  200 , Reward:  [-647.88741815]\n",
      "Worker name: worker1 , Episode:  279 , Step:  200 , Reward:  [-1024.22466424]\n",
      "Worker name: worker2 , Episode:  280 , Step:  200 , Reward:  [-814.49078582]\n",
      "Worker name: worker3 , Episode:  281 , Step:  200 , Reward:  [-771.04333986]\n",
      "Worker name: worker0 , Episode:  282 , Step:  200 , Reward:  [-767.00147228]\n",
      "Worker name: worker1 , Episode:  283 , Step:  200 , Reward:  [-893.17484955]\n",
      "Worker name: worker2 , Episode:  284 , Step:  200 , Reward:  [-915.82020241]\n",
      "Worker name: worker3 , Episode:  285 , Step:  200 , Reward:  [-885.01085597]\n",
      "Worker name: worker0 , Episode:  286 , Step:  200 , Reward:  [-786.10700798]\n",
      "Worker name: worker1 , Episode:  287 , Step:  200 , Reward:  [-869.63378716]\n",
      "Worker name: worker2 , Episode:  288 , Step:  200 , Reward:  [-732.31653881]\n",
      "Worker name: worker3 , Episode:  289 , Step:  200 , Reward:  [-576.22392735]\n",
      "Worker name: worker0 , Episode:  290 , Step:  200 , Reward:  [-381.89392874]\n",
      "Worker name: worker1 , Episode:  291 , Step:  200 , Reward:  [-500.10460667]\n",
      "Worker name: worker2 , Episode:  292 , Step:  200 , Reward:  [-784.04194722]\n",
      "Worker name: worker3 , Episode:  293 , Step:  200 , Reward:  [-908.11486126]\n",
      "Worker name: worker0 , Episode:  294 , Step:  200 , Reward:  [-1071.85011892]\n",
      "Worker name: worker1 , Episode:  295 , Step:  200 , Reward:  [-888.33438665]\n",
      "Worker name: worker2 , Episode:  296 , Step:  200 , Reward:  [-835.90531315]\n",
      "Worker name: worker0 , Episode:  297 , Step:  200 , Reward:  [-516.97327874]\n",
      "Worker name: worker3 , Episode:  298 , Step:  200 , Reward:  [-745.29874383]\n",
      "Worker name: worker1 , Episode:  299 , Step:  200 , Reward:  [-579.32412237]\n",
      "Worker name: worker2 , Episode:  300 , Step:  200 , Reward:  [-764.16014265]\n",
      "Worker name: worker0 , Episode:  301 , Step:  200 , Reward:  [-512.74221573]\n",
      "Worker name: worker3 , Episode:  302 , Step:  200 , Reward:  [-698.33109494]\n",
      "Worker name: worker1 , Episode:  303 , Step:  200 , Reward:  [-759.3693419]\n",
      "Worker name: worker2 , Episode:  304 , Step:  200 , Reward:  [-643.33948714]\n",
      "Worker name: worker0 , Episode:  305 , Step:  200 , Reward:  [-512.20225985]\n",
      "Worker name: worker3 , Episode:  306 , Step:  200 , Reward:  [-534.65123764]\n",
      "Worker name: worker1 , Episode:  307 , Step:  200 , Reward:  [-775.32906292]\n",
      "Worker name: worker2 , Episode:  308 , Step:  200 , Reward:  [-765.13510091]\n",
      "Worker name: worker0 , Episode:  309 , Step:  200 , Reward:  [-702.47376411]\n",
      "Worker name: worker3 , Episode:  310 , Step:  200 , Reward:  [-758.22841984]\n",
      "Worker name: worker1 , Episode:  311 , Step:  200 , Reward:  [-774.87724035]\n",
      "Worker name: worker2 , Episode:  312 , Step:  200 , Reward:  [-759.64781532]\n",
      "Worker name: worker0 , Episode:  313 , Step:  200 , Reward:  [-632.99156195]\n"
     ]
    }
   ],
   "source": [
    "max_episode_num = 1000\n",
    "env_name = 'Pendulum-v0'\n",
    "agent = A3Cagent(env_name)\n",
    "\n",
    "agent.train(max_episode_num)\n",
    "\n",
    "agent.plot_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
