{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network1(state_dim, action_dim, action_bound):\n",
    "    state_input = Input((state_dim,))\n",
    "    h1 = Dense(64, activation='relu')(state_input)\n",
    "    h2 = Dense(32, activation='relu')(h1)\n",
    "    h3 = Dense(16, activation='relu')(h2)\n",
    "    out_mu = Dense(action_dim, activation='tanh')(h3)\n",
    "    std_output = Dense(action_dim, activation='softplus')(h3)\n",
    "\n",
    "    # Scale output to [-action_bound, action_bound]\n",
    "    mu_output = Lambda(lambda x: x * action_bound)(out_mu)\n",
    "    model = Model(state_input, [mu_output, std_output])\n",
    "    # model.summary()\n",
    "    model._make_predict_function()  # class 안에서 def가 정의되면 필요없음\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Global_Actor(object):\n",
    "    \"\"\"\n",
    "        Global Actor Network for A3C\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, action_bound, learning_rate, entropy_beta):\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.learning_rate = learning_rate\n",
    "        self.entropy_beta = entropy_beta\n",
    "\n",
    "        self.std_bound = [1e-2, 1]  # std bound\n",
    "\n",
    "        self.model = build_network1(self.state_dim,\n",
    "                                                            self.action_dim,\n",
    "                                                            self.action_bound)\n",
    "\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "    ## log policy pdf\n",
    "    def log_pdf(self, mu, std, action):\n",
    "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
    "        var = std**2\n",
    "        log_policy_pdf = -0.5 * (action - mu) ** 2 / var - 0.5 * tf.math.log(var * 2 * np.pi)\n",
    "        entropy = 0.5 * (tf.math.log(2 * np.pi * std ** 2) + 1.0)\n",
    "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True), tf.reduce_sum(entropy, 1, keepdims=True)\n",
    "\n",
    "\n",
    "    ## train the actor network run by worker\n",
    "    def train(self, states, actions, advantages):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # policy pdf\n",
    "            mu_a, std_a = self.model(states)\n",
    "            log_policy_pdf, entropy = self.log_pdf(mu_a, std_a, actions)\n",
    "\n",
    "            # loss function and its gradient\n",
    "            loss_policy = log_policy_pdf * advantages\n",
    "            loss = tf.reduce_sum(-loss_policy - self.entropy_beta * entropy)\n",
    "        dj_dtheta = tape.gradient(loss, self.model.trainable_variables)\n",
    "\n",
    "        # gradient clipping\n",
    "        dj_dtheta, _ = tf.clip_by_global_norm(dj_dtheta, 40) #40\n",
    "\n",
    "        # pretend gradient wrt global theta\n",
    "        grads = zip(dj_dtheta, self.model.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(grads)\n",
    "\n",
    "    ## actor prediction\n",
    "    def predict(self, state):\n",
    "        mu_a, _= self.model.predict(np.reshape(state, [1, self.state_dim]))\n",
    "        return mu_a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker_Actor(object):\n",
    "    \"\"\"\n",
    "        Worker Actor Network for A3C\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, action_bound):\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "\n",
    "        self.std_bound = [1e-2, 1]  # std bound\n",
    "\n",
    "        self.model = build_network1(self.state_dim, self.action_dim, self.action_bound)\n",
    "\n",
    "\n",
    "    ## actor policy\n",
    "    def get_action(self, state):\n",
    "        # type of action in env is numpy array\n",
    "        # np.reshape(state, [1, self.state_dim]) : shape (state_dim,) -> shape (1, state_dim)\n",
    "        # why [0]?  shape (1, action_dim) -> (action_dim,)\n",
    "        mu_a, std_a = self.model.predict(np.reshape(state, [1, self.state_dim]))\n",
    "        mu_a = mu_a[0]\n",
    "        std_a = std_a[0]\n",
    "        std_a = np.clip(std_a, self.std_bound[0], self.std_bound[1])\n",
    "        action = np.random.normal(mu_a, std_a, size=self.action_dim)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network2(state_dim):\n",
    "    state_input = Input((state_dim,))\n",
    "    h1 = Dense(64, activation='relu')(state_input)\n",
    "    h2 = Dense(32, activation='relu')(h1)\n",
    "    h3 = Dense(16, activation='relu')(h2)\n",
    "    v_output = Dense(1, activation='linear')(h3)\n",
    "    model = Model(state_input, v_output)\n",
    "    #model.summary()\n",
    "    model._make_predict_function()  # class 안에서 def가 정의되면 필요없음\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Global_Critic(object):\n",
    "    \"\"\"\n",
    "        Global Critic Network for A3C: V function approximator\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, learning_rate):\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.model = build_network2(state_dim)\n",
    "\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
    "\n",
    "\n",
    "    ## train the critic network run by worker\n",
    "    def train(self, states, td_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # loss function and its gradient\n",
    "            v_values = self.model(states)\n",
    "            loss = tf.reduce_sum(tf.square(td_targets-v_values))\n",
    "        dj_dphi = tape.gradient(loss, self.model.trainable_variables)\n",
    "\n",
    "        # gradient clipping\n",
    "        dj_dphi, _ = tf.clip_by_global_norm(dj_dphi, 40) #40\n",
    "\n",
    "        # gradients\n",
    "        grads = zip(dj_dphi, self.model.trainable_variables)\n",
    "\n",
    "        self.critic_optimizer.apply_gradients(grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker_Critic(object):\n",
    "    \"\"\"\n",
    "        Critic Network for A3C: V function approximator\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim):\n",
    "\n",
    "        self.model = build_network2(state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_episode_count = 0\n",
    "global_step = 0\n",
    "global_episode_reward = []  # save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3Cagent(object):\n",
    "\n",
    "    \"\"\"\n",
    "        Global network\n",
    "    \"\"\"\n",
    "    def __init__(self, env_name):\n",
    "\n",
    "        # training environment\n",
    "        self.env_name = env_name\n",
    "        self.WORKERS_NUM = multiprocessing.cpu_count() #4\n",
    "\n",
    "        # hyperparameters\n",
    "        self.ACTOR_LEARNING_RATE = 0.0001\n",
    "        self.CRITIC_LEARNING_RATE = 0.001\n",
    "        self.ENTROPY_BETA = 0.01\n",
    "\n",
    "        # get state dimension\n",
    "        env = gym.make(self.env_name)\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        # get action dimension\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        # get action bound\n",
    "        action_bound = env.action_space.high[0]\n",
    "\n",
    "        # create global actor and critic networks\n",
    "        self.global_actor = Global_Actor(state_dim, action_dim, action_bound, self.ACTOR_LEARNING_RATE,\n",
    "                                         self.ENTROPY_BETA)\n",
    "        self.global_critic = Global_Critic(state_dim, action_dim, self.CRITIC_LEARNING_RATE)\n",
    "\n",
    "\n",
    "    def train(self, max_episode_num):\n",
    "\n",
    "        workers = []\n",
    "\n",
    "        # create worker\n",
    "        for i in range(self.WORKERS_NUM):\n",
    "            worker_name = 'worker%i' % i\n",
    "            workers.append(A3Cworker(worker_name, self.env_name, self.global_actor,\n",
    "                                     self.global_critic, max_episode_num))\n",
    "\n",
    "\n",
    "         # create worker (multi-agents) and do parallel training\n",
    "        for worker in workers:\n",
    "            worker.start()\n",
    "\n",
    "        for worker in workers:\n",
    "            worker.join()\n",
    "            \n",
    "        print(global_episode_reward)\n",
    "\n",
    "\n",
    "    ## save them to file if done\n",
    "    def plot_result(self):\n",
    "        plt.plot(global_episode_reward)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3Cworker(threading.Thread):\n",
    "\n",
    "    \"\"\"\n",
    "        local agent network (worker)\n",
    "    \"\"\"\n",
    "    def __init__(self, worker_name, env_name, global_actor, global_critic, max_episode_num):\n",
    "        threading.Thread.__init__(self)\n",
    "\n",
    "        #self.lock = threading.Lock()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.GAMMA = 0.95\n",
    "        self.t_MAX = 4 # t-step prediction\n",
    "\n",
    "        self.max_episode_num = max_episode_num\n",
    "\n",
    "        # environment\n",
    "        self.env = gym.make(env_name)\n",
    "        self.worker_name = worker_name\n",
    "\n",
    "        # global network sharing\n",
    "        self.global_actor = global_actor\n",
    "        self.global_critic = global_critic\n",
    "\n",
    "\n",
    "        # get state dimension\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        # get action dimension\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        # get action bound\n",
    "        self.action_bound = self.env.action_space.high[0]\n",
    "\n",
    "        # create local actor and critic networks\n",
    "        self.worker_actor = Worker_Actor(self.state_dim, self.action_dim, self.action_bound)\n",
    "        self.worker_critic = Worker_Critic(self.state_dim)\n",
    "\n",
    "        # initial transfer global network parameters to worker network parameters\n",
    "        self.worker_actor.model.set_weights(self.global_actor.model.get_weights())\n",
    "        self.worker_critic.model.set_weights(self.global_critic.model.get_weights())\n",
    "\n",
    "\n",
    "    ## computing Advantages and targets: y_k = r_k + gamma*V(s_k+1), A(s_k, a_k)= y_k - V(s_k)\n",
    "    def n_step_td_target(self, rewards, next_v_value, done):\n",
    "        td_targets = np.zeros_like(rewards)\n",
    "        cumulative = 0\n",
    "        if not done:\n",
    "            cumulative = next_v_value\n",
    "\n",
    "        for k in reversed(range(0, len(rewards))):\n",
    "            cumulative = self.GAMMA * cumulative + rewards[k]\n",
    "            td_targets[k] = cumulative\n",
    "        return td_targets\n",
    "\n",
    "\n",
    "    ## convert (list of np.array) to np.array\n",
    "    def unpack_batch(self, batch):\n",
    "        unpack = batch[0]\n",
    "        for idx in range(len(batch) - 1):\n",
    "            unpack = np.append(unpack, batch[idx + 1], axis=0)\n",
    "\n",
    "        return unpack\n",
    "\n",
    "\n",
    "    # train each worker\n",
    "    def run(self):\n",
    "\n",
    "        global global_episode_count, global_step\n",
    "        global global_episode_reward  # total episode across all workers\n",
    "\n",
    "        print(self.worker_name, \"starts ---\")\n",
    "\n",
    "        while global_episode_count <= int(self.max_episode_num):\n",
    "\n",
    "            # initialize batch\n",
    "            batch_state, batch_action, batch_reward = [], [], []\n",
    "\n",
    "            # reset episode\n",
    "            step, episode_reward, done = 0, 0, False\n",
    "            # reset the environment and observe the first state\n",
    "            state = self.env.reset() # shape of state from gym (3,)\n",
    "\n",
    "            while not done:\n",
    "\n",
    "                # visualize the environment\n",
    "                #self.env.render()\n",
    "                # pick an action (shape of gym action = (action_dim,) )\n",
    "                action = self.worker_actor.get_action(state)\n",
    "                # clip continuous action to be within action_bound\n",
    "                action = np.clip(action, -self.action_bound, self.action_bound)\n",
    "                # observe reward, new_state, shape of output of gym (state_dim,)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                # change shape (state_dim,) -> (1, state_dim), same to action, next_state\n",
    "                state = np.reshape(state, [1, self.state_dim])\n",
    "                reward = np.reshape(reward, [1, 1])\n",
    "                action = np.reshape(action, [1, self.action_dim])\n",
    "\n",
    "                # append to the batch\n",
    "                batch_state.append(state)\n",
    "                batch_action.append(action)\n",
    "                batch_reward.append((reward+8)/8) # <-- normalization\n",
    "                #batch_reward.append(reward)\n",
    "\n",
    "                # update state and step\n",
    "                state = next_state\n",
    "                step += 1\n",
    "                episode_reward += reward[0]\n",
    "\n",
    "                # if batch is full or episode ends, start to train global on batch\n",
    "                if len(batch_state) == self.t_MAX or done:\n",
    "\n",
    "                    # extract states, actions, rewards from batch\n",
    "                    states = self.unpack_batch(batch_state)\n",
    "                    actions = self.unpack_batch(batch_action)\n",
    "                    rewards = self.unpack_batch(batch_reward)\n",
    "\n",
    "                    # clear the batch\n",
    "                    batch_state, batch_action, batch_reward = [], [], []\n",
    "\n",
    "                    # compute n-step TD target and advantage prediction with global network\n",
    "                    next_state = np.reshape(next_state, [1, self.state_dim])\n",
    "                    next_v_value = self.global_critic.model.predict(next_state)\n",
    "                    n_step_td_targets = self.n_step_td_target(rewards, next_v_value, done)\n",
    "                    v_values = self.global_critic.model.predict(states)\n",
    "                    advantages = n_step_td_targets - v_values\n",
    "\n",
    "\n",
    "                    #with self.lock:\n",
    "                    # update global critic\n",
    "                    self.global_critic.train(states, n_step_td_targets)\n",
    "                    # update global actor\n",
    "                    self.global_actor.train(states, actions, advantages)\n",
    "\n",
    "                    # transfer global network parameters to worker network parameters\n",
    "                    self.worker_actor.model.set_weights(self.global_actor.model.get_weights())\n",
    "                    self.worker_critic.model.set_weights(self.global_critic.model.get_weights())\n",
    "\n",
    "                    # update global step\n",
    "                    global_step += 1\n",
    "\n",
    "                if done:\n",
    "                    # update global episode count\n",
    "                    global_episode_count += 1\n",
    "                    ## display rewards every episode\n",
    "                    print('Worker name:', self.worker_name, ', Episode: ', global_episode_count,\n",
    "                          ', Step: ', step, ', Reward: ', episode_reward)\n",
    "\n",
    "                    global_episode_reward.append(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker0worker1 starts ---\n",
      " starts ---\n",
      "worker2 starts ---\n",
      "worker3 starts ---\n",
      "WARNING:tensorflow:Layer dense_14 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_14 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_14 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_14 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Worker name: worker2 , Episode:  1 , Step:  200 , Reward:  [-1361.92305403]\n",
      "Worker name: worker0 , Episode:  2 , Step:  200 , Reward:  [-1329.97313184]\n",
      "Worker name: worker1 , Episode:  3 , Step:  200 , Reward:  [-1399.48572155]\n",
      "Worker name: worker3 , Episode:  4 , Step:  200 , Reward:  [-1380.68371209]\n",
      "Worker name: worker2 , Episode:  5 , Step:  200 , Reward:  [-1501.5452484]\n",
      "Worker name: worker1 , Episode:  6 , Step:  200 , Reward:  [-1402.90469928]\n",
      "Worker name: worker0 , Episode:  7 , Step:  200 , Reward:  [-1426.46681826]\n",
      "Worker name: worker3 , Episode:  8 , Step:  200 , Reward:  [-1313.107962]\n",
      "Worker name: worker2 , Episode:  9 , Step:  200 , Reward:  [-1426.19699671]\n",
      "Worker name: worker1 , Episode:  10 , Step:  200 , Reward:  [-1457.64598411]\n",
      "Worker name: worker0 , Episode:  11 , Step:  200 , Reward:  [-1456.56907414]\n",
      "Worker name: worker3 , Episode:  12 , Step:  200 , Reward:  [-1393.75218632]\n",
      "Worker name: worker2 , Episode:  13 , Step:  200 , Reward:  [-1402.42576514]\n",
      "Worker name: worker1 , Episode:  14 , Step:  200 , Reward:  [-1486.05030327]\n",
      "Worker name: worker0 , Episode:  15 , Step:  200 , Reward:  [-1374.66624991]\n",
      "Worker name: worker3 , Episode:  16 , Step:  200 , Reward:  [-1481.82984532]\n",
      "Worker name: worker2 , Episode:  17 , Step:  200 , Reward:  [-1108.98274394]\n",
      "Worker name: worker0 , Episode:  18 , Step:  200 , Reward:  [-1206.44441272]\n",
      "Worker name: worker1 , Episode:  19 , Step:  200 , Reward:  [-1345.52948821]\n",
      "Worker name: worker3 , Episode:  20 , Step:  200 , Reward:  [-1292.09888565]\n",
      "Worker name: worker2 , Episode:  21 , Step:  200 , Reward:  [-1020.08884726]\n",
      "Worker name: worker0 , Episode:  22 , Step:  200 , Reward:  [-1060.92426319]\n",
      "Worker name: worker1 , Episode:  23 , Step:  200 , Reward:  [-1116.94252162]\n",
      "Worker name: worker3 , Episode:  24 , Step:  200 , Reward:  [-1218.46809103]\n",
      "Worker name: worker2 , Episode:  25 , Step:  200 , Reward:  [-1012.11088368]\n",
      "Worker name: worker0 , Episode:  26 , Step:  200 , Reward:  [-1301.92711295]\n",
      "Worker name: worker1 , Episode:  27 , Step:  200 , Reward:  [-1193.81721787]\n",
      "Worker name: worker3 , Episode:  28 , Step:  200 , Reward:  [-1093.36641489]\n",
      "Worker name: worker2 , Episode:  29 , Step:  200 , Reward:  [-1040.05514709]\n",
      "Worker name: worker0 , Episode:  30 , Step:  200 , Reward:  [-1123.32498584]\n",
      "Worker name: worker1 , Episode:  31 , Step:  200 , Reward:  [-1265.7111155]\n",
      "Worker name: worker3 , Episode:  32 , Step:  200 , Reward:  [-1321.29828333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker name: worker2 , Episode:  33 , Step:  200 , Reward:  [-1082.32920994]\n",
      "Worker name: worker0 , Episode:  34 , Step:  200 , Reward:  [-1100.92773176]\n",
      "Worker name: worker1 , Episode:  35 , Step:  200 , Reward:  [-1222.12587953]\n",
      "Worker name: worker3 , Episode:  36 , Step:  200 , Reward:  [-1137.87574534]\n",
      "Worker name: worker2 , Episode:  37 , Step:  200 , Reward:  [-1239.54764892]\n",
      "Worker name: worker0 , Episode:  38 , Step:  200 , Reward:  [-1015.52391559]\n",
      "Worker name: worker1 , Episode:  39 , Step:  200 , Reward:  [-1193.21393883]\n",
      "Worker name: worker3 , Episode:  40 , Step:  200 , Reward:  [-1136.90006867]\n",
      "Worker name: worker2 , Episode:  41 , Step:  200 , Reward:  [-989.25127792]\n",
      "Worker name: worker0 , Episode:  42 , Step:  200 , Reward:  [-1227.89576087]\n",
      "Worker name: worker1 , Episode:  43 , Step:  200 , Reward:  [-1156.11884423]\n",
      "Worker name: worker3 , Episode:  44 , Step:  200 , Reward:  [-1034.91138264]\n",
      "Worker name: worker2 , Episode:  45 , Step:  200 , Reward:  [-1240.77737245]\n",
      "Worker name: worker0 , Episode:  46 , Step:  200 , Reward:  [-1127.90617842]\n",
      "Worker name: worker1 , Episode:  47 , Step:  200 , Reward:  [-1126.24513209]\n",
      "Worker name: worker3 , Episode:  48 , Step:  200 , Reward:  [-1031.54781578]\n",
      "Worker name: worker2 , Episode:  49 , Step:  200 , Reward:  [-1176.50962289]\n",
      "Worker name: worker0 , Episode:  50 , Step:  200 , Reward:  [-1007.41850921]\n",
      "Worker name: worker1 , Episode:  51 , Step:  200 , Reward:  [-1114.20338222]\n",
      "Worker name: worker3 , Episode:  52 , Step:  200 , Reward:  [-1122.85648695]\n",
      "Worker name: worker2 , Episode:  53 , Step:  200 , Reward:  [-1033.56325283]\n",
      "Worker name: worker0 , Episode:  54 , Step:  200 , Reward:  [-1032.28493288]\n",
      "Worker name: worker3 , Episode:  55 , Step:  200 , Reward:  [-896.20130829]\n",
      "Worker name: worker1 , Episode:  56 , Step:  200 , Reward:  [-1094.83421216]\n",
      "Worker name: worker2 , Episode:  57 , Step:  200 , Reward:  [-1026.40609032]\n",
      "Worker name: worker0 , Episode:  58 , Step:  200 , Reward:  [-1094.85065854]\n",
      "Worker name: worker3 , Episode:  59 , Step:  200 , Reward:  [-1009.6818216]\n",
      "Worker name: worker1 , Episode:  60 , Step:  200 , Reward:  [-1235.21096829]\n",
      "Worker name: worker2 , Episode:  61 , Step:  200 , Reward:  [-1078.28333885]\n",
      "Worker name: worker0 , Episode:  62 , Step:  200 , Reward:  [-1165.4147555]\n",
      "Worker name: worker3 , Episode:  63 , Step:  200 , Reward:  [-1144.97353085]\n",
      "Worker name: worker1 , Episode:  64 , Step:  200 , Reward:  [-1151.28222265]\n",
      "Worker name: worker2 , Episode:  65 , Step:  200 , Reward:  [-1065.54287493]\n",
      "Worker name: worker0 , Episode:  66 , Step:  200 , Reward:  [-912.28941663]\n",
      "Worker name: worker3 , Episode:  67 , Step:  200 , Reward:  [-1364.48574784]\n",
      "Worker name: worker1 , Episode:  68 , Step:  200 , Reward:  [-919.31708976]\n",
      "Worker name: worker2 , Episode:  69 , Step:  200 , Reward:  [-1088.47364101]\n",
      "Worker name: worker0 , Episode:  70 , Step:  200 , Reward:  [-996.39743414]\n",
      "Worker name: worker1 , Episode:  71 , Step:  200 , Reward:  [-967.65147553]\n",
      "Worker name: worker3 , Episode:  72 , Step:  200 , Reward:  [-1030.4590166]\n",
      "Worker name: worker2 , Episode:  73 , Step:  200 , Reward:  [-1000.92451462]\n",
      "Worker name: worker0 , Episode:  74 , Step:  200 , Reward:  [-1167.25763391]\n",
      "Worker name: worker1 , Episode:  75 , Step:  200 , Reward:  [-1590.50201647]\n",
      "Worker name: worker3 , Episode:  76 , Step:  200 , Reward:  [-979.93948207]\n",
      "Worker name: worker2 , Episode:  77 , Step:  200 , Reward:  [-1005.44659915]\n",
      "Worker name: worker0 , Episode:  78 , Step:  200 , Reward:  [-1039.07226664]\n",
      "Worker name: worker1 , Episode:  79 , Step:  200 , Reward:  [-1131.0731005]\n",
      "Worker name: worker3 , Episode:  80 , Step:  200 , Reward:  [-907.91548964]\n",
      "Worker name: worker2 , Episode:  81 , Step:  200 , Reward:  [-1081.75677342]\n",
      "Worker name: worker0 , Episode:  82 , Step:  200 , Reward:  [-1037.56441177]\n",
      "Worker name: worker1 , Episode:  83 , Step:  200 , Reward:  [-1029.64447231]\n",
      "Worker name: worker3 , Episode:  84 , Step:  200 , Reward:  [-1121.40855391]\n",
      "Worker name: worker2 , Episode:  85 , Step:  200 , Reward:  [-1113.12320029]\n",
      "Worker name: worker0 , Episode:  86 , Step:  200 , Reward:  [-1029.56648207]\n",
      "Worker name: worker1 , Episode:  87 , Step:  200 , Reward:  [-1150.15124613]\n",
      "Worker name: worker3 , Episode:  88 , Step:  200 , Reward:  [-1135.49121522]\n",
      "Worker name: worker2 , Episode:  89 , Step:  200 , Reward:  [-1049.66115571]\n",
      "Worker name: worker0 , Episode:  90 , Step:  200 , Reward:  [-1180.80204764]\n",
      "Worker name: worker1 , Episode:  91 , Step:  200 , Reward:  [-1118.07256865]\n",
      "Worker name: worker3 , Episode:  92 , Step:  200 , Reward:  [-1082.82277368]\n",
      "Worker name: worker2 , Episode:  93 , Step:  200 , Reward:  [-1142.59900132]\n",
      "Worker name: worker0 , Episode:  94 , Step:  200 , Reward:  [-1296.52488429]\n",
      "Worker name: worker1 , Episode:  95 , Step:  200 , Reward:  [-1372.90438693]\n",
      "Worker name: worker3 , Episode:  96 , Step:  200 , Reward:  [-1183.29950384]\n",
      "Worker name: worker0 , Episode:  97 , Step:  200 , Reward:  [-1232.55819225]\n",
      "Worker name: worker2 , Episode:  98 , Step:  200 , Reward:  [-1024.3190646]\n",
      "Worker name: worker1 , Episode:  99 , Step:  200 , Reward:  [-1146.70818001]\n",
      "Worker name: worker3 , Episode:  100 , Step:  200 , Reward:  [-1215.06509243]\n",
      "Worker name: worker0 , Episode:  101 , Step:  200 , Reward:  [-1065.27868329]\n",
      "Worker name: worker2 , Episode:  102 , Step:  200 , Reward:  [-997.09199055]\n",
      "Worker name: worker1 , Episode:  103 , Step:  200 , Reward:  [-1096.32753083]\n",
      "Worker name: worker3 , Episode:  104 , Step:  200 , Reward:  [-993.34543198]\n",
      "Worker name: worker0 , Episode:  105 , Step:  200 , Reward:  [-760.08669941]\n",
      "Worker name: worker2 , Episode:  106 , Step:  200 , Reward:  [-1044.04695377]\n",
      "Worker name: worker3 , Episode:  107 , Step:  200 , Reward:  [-1004.19004732]\n",
      "Worker name: worker1 , Episode:  108 , Step:  200 , Reward:  [-976.57618687]\n",
      "Worker name: worker2 , Episode:  109 , Step:  200 , Reward:  [-984.75250034]\n",
      "Worker name: worker0 , Episode:  110 , Step:  200 , Reward:  [-1109.76120028]\n",
      "Worker name: worker1 , Episode:  111 , Step:  200 , Reward:  [-1027.60729436]\n",
      "Worker name: worker3 , Episode:  112 , Step:  200 , Reward:  [-879.72855639]\n",
      "Worker name: worker0 , Episode:  113 , Step:  200 , Reward:  [-948.34441125]\n",
      "Worker name: worker2 , Episode:  114 , Step:  200 , Reward:  [-999.69311673]\n",
      "Worker name: worker1 , Episode:  115 , Step:  200 , Reward:  [-1141.31507827]\n",
      "Worker name: worker3 , Episode:  116 , Step:  200 , Reward:  [-899.48864743]\n",
      "Worker name: worker0 , Episode:  117 , Step:  200 , Reward:  [-905.18248421]\n",
      "Worker name: worker2 , Episode:  118 , Step:  200 , Reward:  [-887.29746832]\n",
      "Worker name: worker1 , Episode:  119 , Step:  200 , Reward:  [-887.7525307]\n",
      "Worker name: worker3 , Episode:  120 , Step:  200 , Reward:  [-871.67146444]\n",
      "Worker name: worker0 , Episode:  121 , Step:  200 , Reward:  [-1036.56542005]\n",
      "Worker name: worker2 , Episode:  122 , Step:  200 , Reward:  [-969.66835805]\n",
      "Worker name: worker1 , Episode:  123 , Step:  200 , Reward:  [-893.3529834]\n",
      "Worker name: worker3 , Episode:  124 , Step:  200 , Reward:  [-783.36707766]\n",
      "Worker name: worker0 , Episode:  125 , Step:  200 , Reward:  [-1038.56072638]\n",
      "Worker name: worker2 , Episode:  126 , Step:  200 , Reward:  [-908.78179218]\n",
      "Worker name: worker1 , Episode:  127 , Step:  200 , Reward:  [-880.82523747]\n",
      "Worker name: worker3 , Episode:  128 , Step:  200 , Reward:  [-875.60102637]\n",
      "Worker name: worker0 , Episode:  129 , Step:  200 , Reward:  [-980.66552555]\n",
      "Worker name: worker2 , Episode:  130 , Step:  200 , Reward:  [-871.06841179]\n",
      "Worker name: worker1 , Episode:  131 , Step:  200 , Reward:  [-860.38852438]\n",
      "Worker name: worker3 , Episode:  132 , Step:  200 , Reward:  [-989.73485763]\n",
      "Worker name: worker0 , Episode:  133 , Step:  200 , Reward:  [-1030.71457143]\n",
      "Worker name: worker2 , Episode:  134 , Step:  200 , Reward:  [-937.90465139]\n",
      "Worker name: worker1 , Episode:  135 , Step:  200 , Reward:  [-927.89336312]\n",
      "Worker name: worker3 , Episode:  136 , Step:  200 , Reward:  [-994.34449738]\n",
      "Worker name: worker0 , Episode:  137 , Step:  200 , Reward:  [-634.43278705]\n",
      "Worker name: worker2 , Episode:  138 , Step:  200 , Reward:  [-899.90221122]\n",
      "Worker name: worker1 , Episode:  139 , Step:  200 , Reward:  [-765.13636133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker name: worker3 , Episode:  140 , Step:  200 , Reward:  [-636.74917679]\n",
      "Worker name: worker0 , Episode:  141 , Step:  200 , Reward:  [-1164.12407522]\n",
      "Worker name: worker2 , Episode:  142 , Step:  200 , Reward:  [-1003.22126836]\n",
      "Worker name: worker1 , Episode:  143 , Step:  200 , Reward:  [-859.12711028]\n",
      "Worker name: worker3 , Episode:  144 , Step:  200 , Reward:  [-990.38045243]\n",
      "Worker name: worker0 , Episode:  145 , Step:  200 , Reward:  [-805.88600949]\n",
      "Worker name: worker2 , Episode:  146 , Step:  200 , Reward:  [-876.89654527]\n",
      "Worker name: worker3 , Episode:  147 , Step:  200 , Reward:  [-981.26953896]\n",
      "Worker name: worker1 , Episode:  148 , Step:  200 , Reward:  [-825.13568581]\n",
      "Worker name: worker0 , Episode:  149 , Step:  200 , Reward:  [-1284.51644296]\n",
      "Worker name: worker2 , Episode:  150 , Step:  200 , Reward:  [-759.83113923]\n",
      "Worker name: worker3 , Episode:  151 , Step:  200 , Reward:  [-1002.41341761]\n",
      "Worker name: worker1 , Episode:  152 , Step:  200 , Reward:  [-1015.37774246]\n",
      "Worker name: worker0 , Episode:  153 , Step:  200 , Reward:  [-994.3344768]\n",
      "Worker name: worker2 , Episode:  154 , Step:  200 , Reward:  [-875.60979893]\n",
      "Worker name: worker3 , Episode:  155 , Step:  200 , Reward:  [-852.80693501]\n",
      "Worker name: worker1 , Episode:  156 , Step:  200 , Reward:  [-927.2312764]\n",
      "Worker name: worker0 , Episode:  157 , Step:  200 , Reward:  [-982.47522335]\n",
      "Worker name: worker2 , Episode:  158 , Step:  200 , Reward:  [-951.84683627]\n",
      "Worker name: worker3 , Episode:  159 , Step:  200 , Reward:  [-988.50282923]\n",
      "Worker name: worker1 , Episode:  160 , Step:  200 , Reward:  [-975.4662565]\n",
      "Worker name: worker0 , Episode:  161 , Step:  200 , Reward:  [-953.89165977]\n",
      "Worker name: worker2 , Episode:  162 , Step:  200 , Reward:  [-905.12361481]\n",
      "Worker name: worker1 , Episode:  163 , Step:  Worker name: worker3200 , Episode:  164 , Step:  200 , Reward:  [-1022.59327166]\n",
      " , Reward:  [-865.78429537]\n",
      "Worker name: worker0 , Episode:  165 , Step:  200 , Reward:  [-1005.84307754]\n",
      "Worker name: worker2 , Episode:  166 , Step:  200 , Reward:  [-1016.75358955]\n",
      "Worker name: worker1 , Episode:  167 , Step:  200 , Reward:  [-979.09793507]\n",
      "Worker name: worker3 , Episode:  168 , Step:  200 , Reward:  [-1167.66842139]\n",
      "Worker name: worker0 , Episode:  169 , Step:  200 , Reward:  [-994.71411863]\n",
      "Worker name: worker2 , Episode:  170 , Step:  200 , Reward:  [-1146.67886333]\n",
      "Worker name: worker1 , Episode:  171 , Step:  200 , Reward:  [-1206.50331855]\n",
      "Worker name: worker3 , Episode:  172 , Step:  200 , Reward:  [-1027.62796896]\n",
      "Worker name: worker0 , Episode:  173 , Step:  200 , Reward:  [-1267.05686512]\n",
      "Worker name: worker2 , Episode:  174 , Step:  200 , Reward:  [-885.80319377]\n",
      "Worker name: worker3 , Episode:  175 , Step:  200 , Reward:  [-901.53982768]\n",
      "Worker name: worker1 , Episode:  176 , Step:  200 , Reward:  [-735.8814051]\n",
      "Worker name: worker0 , Episode:  177 , Step:  200 , Reward:  [-1030.88417274]\n",
      "Worker name: worker2 , Episode:  178 , Step:  200 , Reward:  [-984.85882749]\n",
      "Worker name: worker3 , Episode:  179 , Step:  200 , Reward:  [-1036.72158497]\n",
      "Worker name: worker1 , Episode:  180 , Step:  200 , Reward:  [-915.51415628]\n",
      "Worker name: worker0 , Episode:  181 , Step:  200 , Reward:  [-1116.98363139]\n",
      "Worker name: worker3 , Episode:  182 , Step:  200 , Reward:  [-1093.78484016]\n",
      "Worker name: worker2 , Episode:  183 , Step:  200 , Reward:  [-1026.72873539]\n",
      "Worker name: worker1 , Episode:  184 , Step:  200 , Reward:  [-1150.99672741]\n",
      "Worker name: worker0 , Episode:  185 , Step:  200 , Reward:  [-640.16934555]\n",
      "Worker name: worker3 , Episode:  186 , Step:  200 , Reward:  [-765.00446645]\n",
      "Worker name: worker2 , Episode:  187 , Step:  200 , Reward:  [-643.61939192]\n",
      "Worker name: worker1 , Episode:  188 , Step:  200 , Reward:  [-879.61863846]\n",
      "Worker name: worker0 , Episode:  189 , Step:  200 , Reward:  [-799.54620369]\n",
      "Worker name: worker3 , Episode:  190 , Step:  200 , Reward:  [-1023.28290154]\n",
      "Worker name: worker2 , Episode:  191 , Step:  200 , Reward:  [-896.26887883]\n",
      "Worker name: worker1 , Episode:  192 , Step:  200 , Reward:  [-893.76047227]\n",
      "Worker name: worker0 , Episode:  193 , Step:  200 , Reward:  [-773.32568564]\n",
      "Worker name: worker3 , Episode:  194 , Step:  200 , Reward:  [-870.54436264]\n",
      "Worker name: worker2 , Episode:  195 , Step:  200 , Reward:  [-750.46230288]\n",
      "Worker name: worker1 , Episode:  196 , Step:  200 , Reward:  [-814.16064894]\n",
      "Worker name: worker0 , Episode:  197 , Step:  200 , Reward:  [-1114.77084125]\n",
      "Worker name: worker3 , Episode:  198 , Step:  200 , Reward:  [-1017.46042351]\n",
      "Worker name: worker2 , Episode:  199 , Step:  200 , Reward:  [-1024.94078562]\n",
      "Worker name: worker1 , Episode:  200 , Step:  200 , Reward:  [-900.94017668]\n",
      "Worker name: worker0 , Episode:  201 , Step:  200 , Reward:  [-638.11743993]\n",
      "Worker name: worker2 , Episode:  202 , Step:  200 , Reward:  [-1011.98004875]\n",
      "Worker name: worker3 , Episode:  203 , Step:  200 , Reward:  [-721.2596747]\n",
      "Worker name: worker1 , Episode:  204 , Step:  200 , Reward:  [-1067.81453578]\n",
      "Worker name: worker0 , Episode:  205 , Step:  200 , Reward:  [-1297.64486513]\n",
      "Worker name: worker3 , Episode:  206 , Step:  200 , Reward:  [-1036.19302464]\n",
      "Worker name: worker2 , Episode:  207 , Step:  200 , Reward:  [-1093.90052506]\n",
      "Worker name: worker1 , Episode:  208 , Step:  200 , Reward:  [-1058.85142232]\n",
      "Worker name: worker0 , Episode:  209 , Step:  200 , Reward:  [-745.98776395]\n",
      "Worker name: worker3 , Episode:  210 , Step:  200 , Reward:  [-1028.96059509]\n",
      "Worker name: worker2 , Episode:  211 , Step:  200 , Reward:  [-803.71711898]\n",
      "Worker name: worker1 , Episode:  212 , Step:  200 , Reward:  [-765.5042025]\n",
      "Worker name: worker0 , Episode:  213 , Step:  200 , Reward:  [-901.42246085]\n",
      "Worker name: worker3 , Episode:  214 , Step:  200 , Reward:  [-880.3244607]\n",
      "Worker name: worker2 , Episode:  215 , Step:  200 , Reward:  [-1111.88106703]\n",
      "Worker name: worker1 , Episode:  216 , Step:  200 , Reward:  [-1085.68503531]\n",
      "Worker name: worker0 , Episode:  217 , Step:  200 , Reward:  [-891.59732739]\n",
      "Worker name: worker3 , Episode:  218 , Step:  200 , Reward:  [-513.75511695]\n",
      "Worker name: worker2 , Episode:  219 , Step:  200 , Reward:  [-1006.52363051]\n",
      "Worker name: worker1 , Episode:  220 , Step:  200 , Reward:  [-1052.07761477]\n",
      "Worker name: worker0 , Episode:  221 , Step:  200 , Reward:  [-842.10131882]\n",
      "Worker name: worker3 , Episode:  222 , Step:  200 , Reward:  [-1029.87551888]\n",
      "Worker name: worker2 , Episode:  223 , Step:  200 , Reward:  [-771.23464321]\n",
      "Worker name: worker1 , Episode:  224 , Step:  200 , Reward:  [-883.18249107]\n",
      "Worker name: worker0 , Episode:  225 , Step:  200 , Reward:  [-974.648593]\n",
      "Worker name: worker3 , Episode:  226 , Step:  200 , Reward:  [-1024.1321979]\n",
      "Worker name: worker2 , Episode:  227 , Step:  200 , Reward:  [-1179.48307924]\n",
      "Worker name: worker1 , Episode:  228 , Step:  200 , Reward:  [-943.09303614]\n",
      "Worker name: worker0 , Episode:  229 , Step:  200 , Reward:  [-810.80217037]\n",
      "Worker name: worker3 , Episode:  230 , Step:  200 , Reward:  [-757.09190069]\n",
      "Worker name: worker2 , Episode:  231 , Step:  200 , Reward:  [-761.61261951]\n",
      "Worker name: worker1 , Episode:  232 , Step:  200 , Reward:  [-760.51767333]\n",
      "Worker name: worker0 , Episode:  233 , Step:  200 , Reward:  [-885.35362018]\n",
      "Worker name: worker3 , Episode:  234 , Step:  200 , Reward:  [-785.38210542]\n",
      "Worker name: worker2 , Episode:  235 , Step:  200 , Reward:  [-877.18286014]\n",
      "Worker name: worker1 , Episode:  236 , Step:  200 , Reward:  [-754.30506265]\n",
      "Worker name: worker0 , Episode:  237 , Step:  200 , Reward:  [-862.68064186]\n",
      "Worker name: worker3 , Episode:  238 , Step:  200 , Reward:  [-905.6060744]\n",
      "Worker name: worker2 , Episode:  239 , Step:  200 , Reward:  [-901.80693663]\n",
      "Worker name: worker1 , Episode:  240 , Step:  200 , Reward:  [-639.30955442]\n",
      "Worker name: worker0 , Episode:  241 , Step:  200 , Reward:  [-996.69063251]\n",
      "Worker name: worker3 , Episode:  242 , Step:  200 , Reward:  [-958.57073065]\n",
      "Worker name: worker2 , Episode:  243 , Step:  200 , Reward:  [-769.30685948]\n",
      "Worker name: worker1 , Episode:  244 , Step:  200 , Reward:  [-677.70755527]\n",
      "Worker name: worker0 , Episode:  245 , Step:  200 , Reward:  [-839.54774666]\n",
      "Worker name: worker3 , Episode:  246 , Step:  200 , Reward:  [-775.62341783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker name: worker2 , Episode:  247 , Step:  200 , Reward:  [-768.85174466]\n",
      "Worker name: worker1 , Episode:  248 , Step:  200 , Reward:  [-743.54035515]\n",
      "Worker name: worker0 , Episode:  249 , Step:  200 , Reward:  [-969.34068723]\n",
      "Worker name: worker3 , Episode:  250 , Step:  200 , Reward:  [-678.08232711]\n",
      "Worker name: worker2 , Episode:  251 , Step:  200 , Reward:  [-885.64272387]\n",
      "Worker name: worker1 , Episode:  252 , Step:  200 , Reward:  [-765.8843438]\n",
      "Worker name: worker0 , Episode:  253 , Step:  200 , Reward:  [-739.64767758]\n",
      "Worker name: worker3 , Episode:  254 , Step:  200 , Reward:  [-743.6435837]\n",
      "Worker name: worker2 , Episode:  255 , Step:  200 , Reward:  [-880.95014546]\n",
      "Worker name: worker1 , Episode:  256 , Step:  200 , Reward:  [-740.92571042]\n",
      "Worker name: worker0 , Episode:  257 , Step:  200 , Reward:  [-988.02749406]\n",
      "Worker name: worker3 , Episode:  258 , Step:  200 , Reward:  [-783.06387183]\n",
      "Worker name: worker2 , Episode:  259 , Step:  200 , Reward:  [-1048.24422334]\n",
      "Worker name: worker1 , Episode:  260 , Step:  200 , Reward:  [-1044.6658369]\n",
      "Worker name: worker0 , Episode:  261 , Step:  200 , Reward:  [-612.00183001]\n",
      "Worker name: worker3 , Episode:  262 , Step:  200 , Reward:  [-1044.92895994]\n",
      "Worker name: worker2 , Episode:  263 , Step:  200 , Reward:  [-917.77545328]\n",
      "Worker name: worker1 , Episode:  264 , Step:  200 , Reward:  [-125.79506383]\n",
      "Worker name: worker0 , Episode:  265 , Step:  200 , Reward:  [-914.17521784]\n",
      "Worker name: worker3 , Episode:  266 , Step:  200 , Reward:  [-783.75572719]\n",
      "Worker name: worker2 , Episode:  267 , Step:  200 , Reward:  [-739.63220789]\n",
      "Worker name: worker1 , Episode:  268 , Step:  200 , Reward:  [-898.88076597]\n",
      "Worker name: worker0 , Episode:  269 , Step:  200 , Reward:  [-745.26226364]\n",
      "Worker name: worker3 , Episode:  270 , Step:  200 , Reward:  [-769.37673403]\n",
      "Worker name: worker2 , Episode:  271 , Step:  200 , Reward:  [-751.84910656]\n",
      "Worker name: worker1 , Episode:  272 , Step:  200 , Reward:  [-857.2790211]\n",
      "Worker name: worker0 , Episode:  273 , Step:  200 , Reward:  [-639.97692418]\n",
      "Worker name: worker3 , Episode:  274 , Step:  200 , Reward:  [-1032.74231325]\n",
      "Worker name: worker2 , Episode:  275 , Step:  200 , Reward:  [-1021.00854137]\n",
      "Worker name: worker1 , Episode:  276 , Step:  200 , Reward:  [-1038.22810659]\n",
      "Worker name: worker0 , Episode:  277 , Step:  200 , Reward:  [-787.30634152]\n",
      "Worker name: worker3 , Episode:  278 , Step:  200 , Reward:  [-757.09457415]\n",
      "Worker name: worker2 , Episode:  279 , Step:  200 , Reward:  [-636.40778346]\n",
      "Worker name: worker1 , Episode:  280 , Step:  200 , Reward:  [-663.82451396]\n",
      "Worker name: worker0 , Episode:  281 , Step:  200 , Reward:  [-513.70089053]\n",
      "Worker name: worker3 , Episode:  282 , Step:  200 , Reward:  [-493.92358682]\n",
      "Worker name: worker2 , Episode:  283 , Step:  200 , Reward:  [-605.77188152]\n",
      "Worker name: worker1 , Episode:  284 , Step:  200 , Reward:  [-834.9605172]\n",
      "Worker name: worker0 , Episode:  285 , Step:  200 , Reward:  [-636.42589826]\n",
      "Worker name: worker3 , Episode:  286 , Step:  200 , Reward:  [-621.99541034]\n",
      "Worker name: worker2 , Episode:  287 , Step:  200 , Reward:  [-653.15110642]\n",
      "Worker name: worker1 , Episode:  288 , Step:  200 , Reward:  [-791.90122584]\n",
      "Worker name: worker0 , Episode:  289 , Step:  200 , Reward:  [-1055.67256342]\n",
      "Worker name: worker3 , Episode:  290 , Step:  200 , Reward:  [-875.33823107]\n",
      "Worker name: worker2 , Episode:  291 , Step:  200 , Reward:  [-783.59526149]\n",
      "Worker name: worker1 , Episode:  292 , Step:  200 , Reward:  [-1016.29967146]\n",
      "Worker name: worker0 , Episode:  293 , Step:  200 , Reward:  [-510.54455648]\n",
      "Worker name: worker3 , Episode:  294 , Step:  200 , Reward:  [-383.25217136]\n",
      "Worker name: worker2 , Episode:  295 , Step:  200 , Reward:  [-893.16321286]\n",
      "Worker name: worker1 , Episode:  296 , Step:  200 , Reward:  [-542.68131998]\n",
      "Worker name: worker0 , Episode:  297 , Step:  200 , Reward:  [-904.97798034]\n",
      "Worker name: worker3 , Episode:  298 , Step:  200 , Reward:  [-546.64075411]\n",
      "Worker name: worker2 , Episode:  299 , Step:  200 , Reward:  [-899.15428132]\n",
      "Worker name: worker1 , Episode:  300 , Step:  200 , Reward:  [-790.46503214]\n",
      "Worker name: worker0 , Episode:  301 , Step:  200 , Reward:  [-747.46752484]\n",
      "Worker name: worker3 , Episode:  302 , Step:  200 , Reward:  [-810.99180157]\n",
      "Worker name: worker2 , Episode:  303 , Step:  200 , Reward:  [-645.82271266]\n",
      "Worker name: worker1 , Episode:  304 , Step:  200 , Reward:  [-819.88126723]\n",
      "Worker name: worker0 , Episode:  305 , Step:  200 , Reward:  [-637.04387229]\n",
      "Worker name: worker3 , Episode:  306 , Step:  200 , Reward:  [-635.97372752]\n",
      "Worker name: worker2 , Episode:  307 , Step:  200 , Reward:  [-811.18235592]\n",
      "Worker name: worker1 , Episode:  308 , Step:  200 , Reward:  [-515.21571024]\n",
      "Worker name: worker0 , Episode:  309 , Step:  200 , Reward:  [-1097.44488443]\n",
      "Worker name: worker3 , Episode:  310 , Step:  200 , Reward:  [-534.4266648]\n",
      "Worker name: worker2 , Episode:  311 , Step:  200 , Reward:  [-506.50441933]\n",
      "Worker name: worker1 , Episode:  312 , Step:  200 , Reward:  [-256.36632732]\n",
      "Worker name: worker0 , Episode:  313 , Step:  200 , Reward:  [-128.67863107]\n",
      "Worker name: worker3 , Episode:  314 , Step:  200 , Reward:  [-261.60107282]\n",
      "Worker name: worker2 , Episode:  315 , Step:  200 , Reward:  [-639.8612128]\n",
      "Worker name: worker1 , Episode:  316 , Step:  200 , Reward:  [-1010.94733101]\n",
      "Worker name: worker0 , Episode:  317 , Step:  200 , Reward:  [-502.04830803]\n",
      "Worker name: worker3 , Episode:  318 , Step:  200 , Reward:  [-1108.41101681]\n",
      "Worker name: worker2 , Episode:  319 , Step:  200 , Reward:  [-481.29777439]\n",
      "Worker name: worker1 , Episode:  320 , Step:  200 , Reward:  [-1065.36948727]\n",
      "Worker name: worker0 , Episode:  321 , Step:  200 , Reward:  [-787.32892498]\n",
      "Worker name: worker3 , Episode:  322 , Step:  200 , Reward:  [-1034.23338332]\n",
      "Worker name: worker2 , Episode:  323 , Step:  200 , Reward:  [-128.81303472]\n",
      "Worker name: worker1 , Episode:  324 , Step:  200 , Reward:  [-519.95645649]\n",
      "Worker name: worker0 , Episode:  325 , Step:  200 , Reward:  [-627.31382692]\n",
      "Worker name: worker3 , Episode:  326 , Step:  200 , Reward:  [-662.27067431]\n",
      "Worker name: worker1 , Episode:  327 , Step:  200 , Reward:  [-915.48059462]Worker name:\n",
      " worker2 , Episode:  328 , Step:  200 , Reward:  [-639.86590655]\n",
      "Worker name: worker0 , Episode:  329 , Step:  200 , Reward:  [-267.72771237]\n",
      "Worker name: worker3 , Episode:  330 , Step:  200 , Reward:  [-734.49151073]\n",
      "Worker name: worker1 , Episode:  331 , Step:  200 , Reward:  [-1182.45824429]\n",
      "Worker name: worker2 , Episode:  332 , Step:  200 , Reward:  [-253.72836971]\n",
      "Worker name: worker0 , Episode:  333 , Step:  200 , Reward:  [-257.22438467]\n",
      "Worker name: worker3 , Episode:  334 , Step:  200 , Reward:  [-1193.69447146]\n",
      "Worker name: worker1 , Episode:  335 , Step:  200 , Reward:  [-1061.66077232]\n",
      "Worker name: worker2 , Episode:  336 , Step:  200 , Reward:  [-1044.90296134]\n",
      "Worker name: worker0 , Episode:  337 , Step:  200 , Reward:  [-798.90866921]\n",
      "Worker name: worker3 , Episode:  338 , Step:  200 , Reward:  [-501.14716931]\n",
      "Worker name: worker1 , Episode:  339 , Step:  200 , Reward:  [-129.10885792]\n",
      "Worker name: worker2 , Episode:  340 , Step:  200 , Reward:  [-915.93679733]\n",
      "Worker name: worker0 , Episode:  341 , Step:  200 , Reward:  [-535.22426788]\n",
      "Worker name: worker3 , Episode:  342 , Step:  200 , Reward:  [-505.88621622]\n",
      "Worker name: worker1 , Episode:  343 , Step:  200 , Reward:  [-131.46576719]\n",
      "Worker name: worker2 , Episode:  344 , Step:  200 , Reward:  [-496.7917061]\n",
      "Worker name: worker0 , Episode:  345 , Step:  200 , Reward:  [-552.82575006]\n",
      "Worker name: worker3 , Episode:  346 , Step:  200 , Reward:  [-900.0825293]\n",
      "Worker name: worker1 , Episode:  347 , Step:  200 , Reward:  [-538.29193475]\n",
      "Worker name: worker2 , Episode:  348 , Step:  200 , Reward:  [-132.37123188]\n",
      "Worker name: worker0 , Episode:  349 , Step:  200 , Reward:  [-613.02972894]\n",
      "Worker name: worker3 , Episode:  350 , Step:  200 , Reward:  [-129.45419176]\n",
      "Worker name: worker1 , Episode:  351 , Step:  200 , Reward:  [-520.41237258]\n",
      "Worker name: worker2 , Episode:  352 , Step:  200 , Reward:  [-1081.13966584]\n",
      "Worker name: worker0 , Episode:  353 , Step:  200 , Reward:  [-1063.94093992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker name: worker3 , Episode:  354 , Step:  200 , Reward:  [-647.65350145]\n",
      "Worker name: worker1 , Episode:  355 , Step:  200 , Reward:  [-258.20092543]\n",
      "Worker name: worker2 , Episode:  356 , Step:  200 , Reward:  [-645.77199929]\n",
      "Worker name: worker0 , Episode:  357 , Step:  200 , Reward:  [-876.33852675]\n",
      "Worker name: worker3 , Episode:  358 , Step:  200 , Reward:  [-509.02994911]\n",
      "Worker name: worker1 , Episode:  359 , Step:  200 , Reward:  [-255.90082977]\n",
      "Worker name: worker2 , Episode:  360 , Step:  200 , Reward:  [-487.28876345]\n",
      "Worker name: worker0 , Episode:  361 , Step:  200 , Reward:  [-369.31878329]\n",
      "Worker name: worker3 , Episode:  362 , Step:  200 , Reward:  [-253.69100974]\n",
      "Worker name: worker1 , Episode:  363 , Step:  200 , Reward:  [-389.36347919]\n",
      "Worker name: worker2 , Episode:  364 , Step:  200 , Reward:  [-412.98451207]\n",
      "Worker name: worker0 , Episode:  365 , Step:  200 , Reward:  [-398.84584708]\n",
      "Worker name: worker3 , Episode:  366 , Step:  200 , Reward:  [-892.83162985]\n",
      "Worker name: worker1 , Episode:  367 , Step:  200 , Reward:  [-4.62708476]\n",
      "Worker name: worker2 , Episode:  368 , Step:  200 , Reward:  [-255.31924954]\n",
      "Worker name: worker0 , Episode:  369 , Step:  200 , Reward:  [-251.14880974]\n",
      "Worker name: worker3 , Episode:  370 , Step:  200 , Reward:  [-1161.42454376]\n",
      "Worker name: worker1 , Episode:  371 , Step:  200 , Reward:  [-451.99826404]\n",
      "Worker name: worker2 , Episode:  372 , Step:  200 , Reward:  [-129.02180357]\n",
      "Worker name: worker0 , Episode:  373 , Step:  200 , Reward:  [-1130.18658673]\n",
      "Worker name: worker3 , Episode:  374 , Step:  200 , Reward:  [-791.88256966]\n",
      "Worker name: worker1 , Episode:  375 , Step:  200 , Reward:  [-255.35917531]\n",
      "Worker name: worker2 , Episode:  376 , Step:  200 , Reward:  [-377.14877158]\n",
      "Worker name: worker0 , Episode:  377 , Step:  200 , Reward:  [-768.37283234]\n",
      "Worker name: worker3 , Episode:  378 , Step:  200 , Reward:  [-1022.86030051]\n",
      "Worker name: worker1 , Episode:  379 , Step:  200 , Reward:  [-1042.4837531]\n",
      "Worker name: worker2 , Episode:  380 , Step:  200 , Reward:  [-1040.68819129]\n",
      "Worker name: worker0 , Episode:  381 , Step:  200 , Reward:  [-128.03240298]\n",
      "Worker name: worker3 , Episode:  382 , Step:  200 , Reward:  [-362.71143606]\n",
      "Worker name: worker1 , Episode:  383 , Step:  200 , Reward:  [-861.99245706]\n",
      "Worker name: worker2 , Episode:  384 , Step:  200 , Reward:  [-2.41490998]\n",
      "Worker name: worker0 , Episode:  385 , Step:  200 , Reward:  [-127.41423536]\n",
      "Worker name: worker3 , Episode:  386 , Step:  200 , Reward:  [-131.48396202]\n",
      "Worker name: worker1 , Episode:  387 , Step:  200 , Reward:  [-254.06440351]\n",
      "Worker name: worker2 , Episode:  388 , Step:  200 , Reward:  [-245.18166072]\n",
      "Worker name: worker0 , Episode:  389 , Step:  200 , Reward:  [-130.30275734]\n",
      "Worker name: worker3 , Episode:  390 , Step:  200 , Reward:  [-359.19142535]\n",
      "Worker name: worker1 , Episode:  391 , Step:  200 , Reward:  [-459.35367395]\n",
      "Worker name: worker2 , Episode:  392 , Step:  200 , Reward:  [-1.2311081]\n",
      "Worker name: worker0 , Episode:  393 , Step:  200 , Reward:  [-515.96661142]\n",
      "Worker name: worker3 , Episode:  394 , Step:  200 , Reward:  [-261.78647841]\n"
     ]
    }
   ],
   "source": [
    "max_episode_num = 1000\n",
    "env_name = 'Pendulum-v0'\n",
    "agent = A3Cagent(env_name)\n",
    "\n",
    "agent.train(max_episode_num)\n",
    "\n",
    "agent.plot_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
