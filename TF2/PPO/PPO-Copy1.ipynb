{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    \"\"\"\n",
    "        Actor Network for PPO\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, action_bound, learning_rate, ratio_clipping):\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.learning_rate = learning_rate\n",
    "        self.ratio_clipping = ratio_clipping\n",
    "\n",
    "        self.std_bound = [1e-2, 1.0] # std bound\n",
    "\n",
    "        self.model = self.build_network()\n",
    "\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
    "\n",
    "    ## actor network\n",
    "    def build_network(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        h1 = Dense(64, activation='relu')(state_input)\n",
    "        h2 = Dense(32, activation='relu')(h1)\n",
    "        h3 = Dense(16, activation='relu')(h2)\n",
    "        out_mu = Dense(self.action_dim, activation='tanh')(h3)\n",
    "        std_output = Dense(self.action_dim, activation='softplus')(h3)\n",
    "\n",
    "        # Scale output to [-action_bound, action_bound]\n",
    "        mu_output = Lambda(lambda x: x*self.action_bound)(out_mu)\n",
    "        model = Model(state_input, [mu_output, std_output])\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "\n",
    "    ## log policy pdf\n",
    "    def log_pdf(self, mu, std, action):\n",
    "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
    "        var = std**2\n",
    "        log_policy_pdf = -0.5 * (action - mu) ** 2 / var - 0.5 * tf.math.log(var * 2 * np.pi)\n",
    "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
    "\n",
    "\n",
    "    ## actor policy\n",
    "    def get_policy_action(self, state):\n",
    "        # type of action in env is numpy array\n",
    "        # np.reshape(state, [1, self.state_dim]) : shape (state_dim,) -> shape (1, state_dim)\n",
    "        # why [0]?  shape (1, action_dim) -> (action_dim,)\n",
    "        mu_a, std_a = self.model.predict(np.reshape(state, [1, self.state_dim]))\n",
    "        mu_a = mu_a[0]\n",
    "        std_a = std_a[0]\n",
    "        std_a = np.clip(std_a, self.std_bound[0], self.std_bound[1])\n",
    "        action = np.random.normal(mu_a, std_a, size=self.action_dim)\n",
    "        return mu_a, std_a, action\n",
    "\n",
    "    ## actor prediction\n",
    "    def predict(self, state):\n",
    "        mu_a, _= self.model.predict(np.reshape(state, [1, self.state_dim]))\n",
    "        return mu_a[0]\n",
    "\n",
    "\n",
    "    ## train the actor network\n",
    "    def train(self, log_old_policy_pdf, states, actions, advantages):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # current policy pdf\n",
    "            mu_a, std_a = self.model(states)\n",
    "            log_policy_pdf = self.log_pdf(mu_a, std_a, actions)\n",
    "\n",
    "            # ratio of current and old policies\n",
    "            ratio = tf.exp(log_policy_pdf - log_old_policy_pdf)\n",
    "            clipped_ratio = tf.clip_by_value(ratio, 1.0-self.ratio_clipping, 1.0+self.ratio_clipping)\n",
    "            surrogate = -tf.minimum(ratio * advantages, clipped_ratio * advantages)\n",
    "            loss = tf.reduce_mean(surrogate)\n",
    "        dj_dtheta = tape.gradient(loss, self.model.trainable_variables)\n",
    "        grads = zip(dj_dtheta, self.model.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(grads)\n",
    "    def load_weights(self, path):\n",
    "        self.model.load_weights(path + 'pendulum_actor.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    \"\"\"\n",
    "        Critic Network for PPO: V function approximator\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, learning_rate):\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # create and compile critic network\n",
    "        self.model, self.states = self.build_network()\n",
    "\n",
    "        self.model.compile(optimizer=Adam(self.learning_rate), loss='mse')\n",
    "\n",
    "    ## critic network\n",
    "    def build_network(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        h1 = Dense(64, activation='relu')(state_input)\n",
    "        h2 = Dense(32, activation='relu')(h1)\n",
    "        h3 = Dense(16, activation='relu')(h2)\n",
    "        v_output = Dense(1, activation='linear')(h3)\n",
    "        model = Model(state_input, v_output)\n",
    "        model.summary()\n",
    "        return model, state_input\n",
    "\n",
    "\n",
    "    ## single gradient update on a single batch data\n",
    "    def train_on_batch(self, states, td_targets):\n",
    "        return self.model.train_on_batch(states, td_targets)\n",
    "    def load_weights(self, path):\n",
    "        self.model.load_weights(path + 'pendulum_critic.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOagent(object):\n",
    "\n",
    "    def __init__(self, env):\n",
    "\n",
    "        # hyperparameters\n",
    "        self.GAMMA = 0.95\n",
    "        self.GAE_LAMBDA = 0.9 #0.8\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.ACTOR_LEARNING_RATE = 0.0001\n",
    "        self.CRITIC_LEARNING_RATE = 0.001\n",
    "        self.RATIO_CLIPPING = 0.2\n",
    "        self.EPOCHS = 10\n",
    "\n",
    "        self.env = env\n",
    "        # get state dimension\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        # get action dimension\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        # get action bound\n",
    "        self.action_bound = env.action_space.high[0]\n",
    "\n",
    "        # create actor and critic networks\n",
    "        self.actor = Actor(self.state_dim, self.action_dim, self.action_bound,\n",
    "                           self.ACTOR_LEARNING_RATE, self.RATIO_CLIPPING)\n",
    "        self.critic = Critic(self.state_dim, self.action_dim, self.CRITIC_LEARNING_RATE)\n",
    "\n",
    "        # save the results\n",
    "        self.save_epi_reward = []\n",
    "\n",
    "\n",
    "    ## computing Advantages and targets: y_k = r_k + gamma*V(s_k+1), A(s_k, a_k)= y_k - V(s_k)\n",
    "    def gae_target(self, rewards, v_values, next_v_value, done):\n",
    "        n_step_targets = np.zeros_like(rewards)\n",
    "        gae = np.zeros_like(rewards)\n",
    "        gae_cumulative = 0\n",
    "        forward_val = 0\n",
    "\n",
    "        if not done:\n",
    "            forward_val = next_v_value\n",
    "\n",
    "        for k in reversed(range(0, len(rewards))):\n",
    "            delta = rewards[k] + self.GAMMA * forward_val - v_values[k]\n",
    "            gae_cumulative = self.GAMMA * self.GAE_LAMBDA * gae_cumulative + delta\n",
    "            gae[k] = gae_cumulative\n",
    "            forward_val = v_values[k]\n",
    "            n_step_targets[k] = gae[k] + v_values[k]\n",
    "        return gae, n_step_targets\n",
    "\n",
    "\n",
    "    ## convert (list of np.array) to np.array\n",
    "    def unpack_batch(self, batch):\n",
    "        unpack = batch[0]\n",
    "        for idx in range(len(batch)-1):\n",
    "            unpack = np.append(unpack, batch[idx+1], axis=0)\n",
    "\n",
    "        return unpack\n",
    "\n",
    "\n",
    "    ## train the agent\n",
    "    def train(self, max_episode_num):\n",
    "\n",
    "        # initialize batch\n",
    "        batch_state, batch_action, batch_reward = [], [], []\n",
    "        batch_log_old_policy_pdf = []\n",
    "\n",
    "        for ep in range(int(max_episode_num)):\n",
    "\n",
    "            # reset episode\n",
    "            time, episode_reward, done = 0, 0, False\n",
    "            # reset the environment and observe the first state\n",
    "            state = self.env.reset() # shape of state from gym (3,)\n",
    "\n",
    "            while not done:\n",
    "\n",
    "                # visualize the environment\n",
    "                #self.env.render()\n",
    "                # compute mu and std of old policy and pick an action (shape of gym action = (action_dim,) )\n",
    "                mu_old, std_old, action = self.actor.get_policy_action(state)\n",
    "                # clip continuous action to be within action_bound\n",
    "                action = np.clip(action, -self.action_bound, self.action_bound)\n",
    "                # compute log old policy pdf\n",
    "                var_old = std_old ** 2\n",
    "                log_old_policy_pdf = -0.5 * (action - mu_old) ** 2 / var_old - 0.5 * np.log(var_old * 2 * np.pi)\n",
    "                log_old_policy_pdf = np.sum(log_old_policy_pdf)\n",
    "                # observe reward, new_state, shape of output of gym (state_dim,)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                # change shape (state_dim,) -> (1, state_dim), same to mu, std, action\n",
    "                state = np.reshape(state, [1, self.state_dim])\n",
    "                action = np.reshape(action, [1, self.action_dim])\n",
    "                reward = np.reshape(reward, [1, 1])\n",
    "                log_old_policy_pdf = np.reshape(log_old_policy_pdf, [1, 1])\n",
    "\n",
    "                # append to the batch\n",
    "                batch_state.append(state)\n",
    "                batch_action.append(action)\n",
    "                batch_reward.append((reward+8)/8) # <-- normalization\n",
    "                #batch_reward.append(reward)\n",
    "                batch_log_old_policy_pdf.append(log_old_policy_pdf)\n",
    "\n",
    "                # continue until batch becomes full\n",
    "                if len(batch_state) < self.BATCH_SIZE:\n",
    "                    # update current state\n",
    "                    state = next_state\n",
    "                    episode_reward += reward[0]\n",
    "                    time += 1\n",
    "                    continue\n",
    "\n",
    "                # extract batched states, actions, td_targets, advantages\n",
    "                states = self.unpack_batch(batch_state)\n",
    "                actions = self.unpack_batch(batch_action)\n",
    "                rewards = self.unpack_batch(batch_reward)\n",
    "                log_old_policy_pdfs = self.unpack_batch(batch_log_old_policy_pdf)\n",
    "\n",
    "                # clear the batch\n",
    "                batch_state, batch_action, batch_reward = [], [], []\n",
    "                batch_log_old_policy_pdf = []\n",
    "\n",
    "                # compute gae and TD targets\n",
    "                next_state = np.reshape(next_state, [1, self.state_dim])\n",
    "                next_v_value = self.critic.model.predict(next_state)\n",
    "                v_values = self.critic.model.predict(states)\n",
    "                gaes, y_i = self.gae_target(rewards, v_values, next_v_value, done)\n",
    "\n",
    "                # update the networks\n",
    "                for _ in range(self.EPOCHS):\n",
    "\n",
    "                    # train\n",
    "                    self.actor.train(log_old_policy_pdfs, states, actions, gaes)\n",
    "                    self.critic.train_on_batch(states, y_i)\n",
    "\n",
    "                # update current state\n",
    "                state = next_state\n",
    "                episode_reward += reward[0]\n",
    "                time += 1\n",
    "\n",
    "            ## display rewards every episode\n",
    "            print('Episode: ', ep+1, 'Time: ', time, 'Reward: ', episode_reward)\n",
    "\n",
    "            self.save_epi_reward.append(episode_reward)\n",
    "        print(self.save_epi_reward)\n",
    "\n",
    "\n",
    "    ## save them to file if done\n",
    "    def plot_result(self):\n",
    "        plt.plot(self.save_epi_reward)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7bbae099bcd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'CartPole-v0'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPPOagent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./save_weights/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-36c7d6275fad>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, env)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# get action dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;31m# get action bound\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_bound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "env_name = 'Pendulum-v0'\n",
    "env = gym.make(env_name)\n",
    "agent = PPOagent(env)\n",
    "\n",
    "agent.actor.load_weights('./save_weights/')\n",
    "agent.critic.load_weights('./save_weights/')\n",
    "\n",
    "time = 0\n",
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    action = agent.actor.predict(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    time += 1\n",
    "\n",
    "    print('Time: ', time, 'Reward: ', reward)\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
